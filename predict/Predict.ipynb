{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('PredictionData.csv')\n",
    "data_invert = pd.read_csv('PredictionDataInvert.csv')\n",
    "data['Conference_team_one'] = data['Conference_team_one'].astype('category')\n",
    "data['Conference_team_two'] = data['Conference_team_two'].astype('category')\n",
    "\n",
    "data_invert['Conference_team_one'] = data_invert['Conference_team_one'].astype('category')\n",
    "data_invert['Conference_team_two'] = data_invert['Conference_team_two'].astype('category')\n",
    "\n",
    "cat_columns = data.select_dtypes(['category']).columns\n",
    "data[cat_columns] = data[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "cat_columns = data_invert.select_dtypes(['category']).columns\n",
    "data_invert[cat_columns] = data_invert[cat_columns].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seed_team_one</th>\n",
       "      <th>Team_team_one</th>\n",
       "      <th>Conference_team_one</th>\n",
       "      <th>AdjEM_team_one</th>\n",
       "      <th>AdjO_team_one</th>\n",
       "      <th>AdjD_team_one</th>\n",
       "      <th>AdjT_team_one</th>\n",
       "      <th>Luck_team_one</th>\n",
       "      <th>Opp AdjEM_team_one</th>\n",
       "      <th>OppO_team_one</th>\n",
       "      <th>OppD_team_one</th>\n",
       "      <th>NCSOS AdjEM_team_one</th>\n",
       "      <th>W_team_one</th>\n",
       "      <th>L_team_one</th>\n",
       "      <th>ast_pct_team_one</th>\n",
       "      <th>blk_pct_team_one</th>\n",
       "      <th>efg_pct_team_one</th>\n",
       "      <th>fg3a_per_fga_pct_team_one</th>\n",
       "      <th>ft_rate_team_one</th>\n",
       "      <th>fta_per_fga_pct_team_one</th>\n",
       "      <th>losses_conf_team_one</th>\n",
       "      <th>losses_home_team_one</th>\n",
       "      <th>losses_visitor_team_one</th>\n",
       "      <th>opp_pts_team_one</th>\n",
       "      <th>pts_team_one</th>\n",
       "      <th>sos_team_one</th>\n",
       "      <th>srs_team_one</th>\n",
       "      <th>tov_pct_team_one</th>\n",
       "      <th>trb_pct_team_one</th>\n",
       "      <th>ts_pct_team_one</th>\n",
       "      <th>wins_conf_team_one</th>\n",
       "      <th>wins_home_team_one</th>\n",
       "      <th>wins_visitor_team_one</th>\n",
       "      <th>def_rtg_team_one</th>\n",
       "      <th>opp_fta_per_fga_pct_team_one</th>\n",
       "      <th>opp_fg3a_per_fga_pct_team_one</th>\n",
       "      <th>opp_ts_pct_team_one</th>\n",
       "      <th>opp_trb_pct_team_one</th>\n",
       "      <th>opp_ast_pct_team_one</th>\n",
       "      <th>opp_stl_pct_team_one</th>\n",
       "      <th>opp_blk_pct_team_one</th>\n",
       "      <th>opp_efg_pct_team_one</th>\n",
       "      <th>opp_tov_pct_team_one</th>\n",
       "      <th>opp_orb_pct_team_one</th>\n",
       "      <th>opp_ft_rate_team_one</th>\n",
       "      <th>num_JR_team_one</th>\n",
       "      <th>num_FR_team_one</th>\n",
       "      <th>num_SR_team_one</th>\n",
       "      <th>num_SO_team_one</th>\n",
       "      <th>num_G_team_one</th>\n",
       "      <th>num_F_team_one</th>\n",
       "      <th>ast_per_g_1_team_one</th>\n",
       "      <th>ast_per_g_2_team_one</th>\n",
       "      <th>ast_per_g_3_team_one</th>\n",
       "      <th>ast_per_g_4_team_one</th>\n",
       "      <th>ast_per_g_5_team_one</th>\n",
       "      <th>ast_per_g_6_team_one</th>\n",
       "      <th>ast_per_g_7_team_one</th>\n",
       "      <th>ast_per_g_8_team_one</th>\n",
       "      <th>ast_per_g_9_team_one</th>\n",
       "      <th>ast_per_g_10_team_one</th>\n",
       "      <th>ast_per_g_11_team_one</th>\n",
       "      <th>ast_per_g_12_team_one</th>\n",
       "      <th>ast_per_g_13_team_one</th>\n",
       "      <th>blk_per_g_1_team_one</th>\n",
       "      <th>blk_per_g_2_team_one</th>\n",
       "      <th>blk_per_g_3_team_one</th>\n",
       "      <th>blk_per_g_4_team_one</th>\n",
       "      <th>blk_per_g_5_team_one</th>\n",
       "      <th>blk_per_g_6_team_one</th>\n",
       "      <th>blk_per_g_7_team_one</th>\n",
       "      <th>blk_per_g_8_team_one</th>\n",
       "      <th>blk_per_g_9_team_one</th>\n",
       "      <th>blk_per_g_10_team_one</th>\n",
       "      <th>blk_per_g_11_team_one</th>\n",
       "      <th>blk_per_g_12_team_one</th>\n",
       "      <th>blk_per_g_13_team_one</th>\n",
       "      <th>drb_per_g_1_team_one</th>\n",
       "      <th>drb_per_g_2_team_one</th>\n",
       "      <th>drb_per_g_3_team_one</th>\n",
       "      <th>drb_per_g_4_team_one</th>\n",
       "      <th>drb_per_g_5_team_one</th>\n",
       "      <th>drb_per_g_6_team_one</th>\n",
       "      <th>drb_per_g_7_team_one</th>\n",
       "      <th>drb_per_g_8_team_one</th>\n",
       "      <th>drb_per_g_9_team_one</th>\n",
       "      <th>drb_per_g_10_team_one</th>\n",
       "      <th>drb_per_g_11_team_one</th>\n",
       "      <th>drb_per_g_12_team_one</th>\n",
       "      <th>drb_per_g_13_team_one</th>\n",
       "      <th>fg2_pct_1_team_one</th>\n",
       "      <th>fg2_pct_2_team_one</th>\n",
       "      <th>fg2_pct_3_team_one</th>\n",
       "      <th>fg2_pct_4_team_one</th>\n",
       "      <th>fg2_pct_5_team_one</th>\n",
       "      <th>fg2_pct_6_team_one</th>\n",
       "      <th>fg2_pct_7_team_one</th>\n",
       "      <th>fg2_pct_8_team_one</th>\n",
       "      <th>fg2_pct_9_team_one</th>\n",
       "      <th>fg2_pct_10_team_one</th>\n",
       "      <th>fg2_pct_11_team_one</th>\n",
       "      <th>fg2_pct_12_team_one</th>\n",
       "      <th>fg2_pct_13_team_one</th>\n",
       "      <th>fg2_per_g_1_team_one</th>\n",
       "      <th>fg2_per_g_2_team_one</th>\n",
       "      <th>fg2_per_g_3_team_one</th>\n",
       "      <th>fg2_per_g_4_team_one</th>\n",
       "      <th>fg2_per_g_5_team_one</th>\n",
       "      <th>fg2_per_g_6_team_one</th>\n",
       "      <th>fg2_per_g_7_team_one</th>\n",
       "      <th>fg2_per_g_8_team_one</th>\n",
       "      <th>fg2_per_g_9_team_one</th>\n",
       "      <th>fg2_per_g_10_team_one</th>\n",
       "      <th>fg2_per_g_11_team_one</th>\n",
       "      <th>fg2_per_g_12_team_one</th>\n",
       "      <th>fg2_per_g_13_team_one</th>\n",
       "      <th>fg2a_per_g_1_team_one</th>\n",
       "      <th>fg2a_per_g_2_team_one</th>\n",
       "      <th>fg2a_per_g_3_team_one</th>\n",
       "      <th>fg2a_per_g_4_team_one</th>\n",
       "      <th>fg2a_per_g_5_team_one</th>\n",
       "      <th>fg2a_per_g_6_team_one</th>\n",
       "      <th>fg2a_per_g_7_team_one</th>\n",
       "      <th>fg2a_per_g_8_team_one</th>\n",
       "      <th>fg2a_per_g_9_team_one</th>\n",
       "      <th>fg2a_per_g_10_team_one</th>\n",
       "      <th>fg2a_per_g_11_team_one</th>\n",
       "      <th>fg2a_per_g_12_team_one</th>\n",
       "      <th>fg2a_per_g_13_team_one</th>\n",
       "      <th>fg3_pct_1_team_one</th>\n",
       "      <th>fg3_pct_2_team_one</th>\n",
       "      <th>fg3_pct_3_team_one</th>\n",
       "      <th>fg3_pct_4_team_one</th>\n",
       "      <th>fg3_pct_5_team_one</th>\n",
       "      <th>fg3_pct_6_team_one</th>\n",
       "      <th>fg3_pct_7_team_one</th>\n",
       "      <th>fg3_pct_8_team_one</th>\n",
       "      <th>fg3_pct_9_team_one</th>\n",
       "      <th>fg3_pct_10_team_one</th>\n",
       "      <th>fg3_pct_11_team_one</th>\n",
       "      <th>fg3_pct_12_team_one</th>\n",
       "      <th>fg3_pct_13_team_one</th>\n",
       "      <th>fg3_per_g_1_team_one</th>\n",
       "      <th>fg3_per_g_2_team_one</th>\n",
       "      <th>fg3_per_g_3_team_one</th>\n",
       "      <th>fg3_per_g_4_team_one</th>\n",
       "      <th>fg3_per_g_5_team_one</th>\n",
       "      <th>fg3_per_g_6_team_one</th>\n",
       "      <th>fg3_per_g_7_team_one</th>\n",
       "      <th>fg3_per_g_8_team_one</th>\n",
       "      <th>fg3_per_g_9_team_one</th>\n",
       "      <th>fg3_per_g_10_team_one</th>\n",
       "      <th>fg3_per_g_11_team_one</th>\n",
       "      <th>fg3_per_g_12_team_one</th>\n",
       "      <th>fg3_per_g_13_team_one</th>\n",
       "      <th>fg3a_per_g_1_team_one</th>\n",
       "      <th>fg3a_per_g_2_team_one</th>\n",
       "      <th>fg3a_per_g_3_team_one</th>\n",
       "      <th>fg3a_per_g_4_team_one</th>\n",
       "      <th>fg3a_per_g_5_team_one</th>\n",
       "      <th>fg3a_per_g_6_team_one</th>\n",
       "      <th>fg3a_per_g_7_team_one</th>\n",
       "      <th>fg3a_per_g_8_team_one</th>\n",
       "      <th>fg3a_per_g_9_team_one</th>\n",
       "      <th>fg3a_per_g_10_team_one</th>\n",
       "      <th>fg3a_per_g_11_team_one</th>\n",
       "      <th>fg3a_per_g_12_team_one</th>\n",
       "      <th>fg3a_per_g_13_team_one</th>\n",
       "      <th>fg_pct_1_team_one</th>\n",
       "      <th>fg_pct_2_team_one</th>\n",
       "      <th>fg_pct_3_team_one</th>\n",
       "      <th>fg_pct_4_team_one</th>\n",
       "      <th>fg_pct_5_team_one</th>\n",
       "      <th>fg_pct_6_team_one</th>\n",
       "      <th>fg_pct_7_team_one</th>\n",
       "      <th>fg_pct_8_team_one</th>\n",
       "      <th>fg_pct_9_team_one</th>\n",
       "      <th>fg_pct_10_team_one</th>\n",
       "      <th>fg_pct_11_team_one</th>\n",
       "      <th>fg_pct_12_team_one</th>\n",
       "      <th>fg_pct_13_team_one</th>\n",
       "      <th>fg_per_g_1_team_one</th>\n",
       "      <th>fg_per_g_2_team_one</th>\n",
       "      <th>fg_per_g_3_team_one</th>\n",
       "      <th>fg_per_g_4_team_one</th>\n",
       "      <th>fg_per_g_5_team_one</th>\n",
       "      <th>fg_per_g_6_team_one</th>\n",
       "      <th>fg_per_g_7_team_one</th>\n",
       "      <th>fg_per_g_8_team_one</th>\n",
       "      <th>fg_per_g_9_team_one</th>\n",
       "      <th>fg_per_g_10_team_one</th>\n",
       "      <th>fg_per_g_11_team_one</th>\n",
       "      <th>fg_per_g_12_team_one</th>\n",
       "      <th>fg_per_g_13_team_one</th>\n",
       "      <th>fga_per_g_1_team_one</th>\n",
       "      <th>fga_per_g_2_team_one</th>\n",
       "      <th>fga_per_g_3_team_one</th>\n",
       "      <th>fga_per_g_4_team_one</th>\n",
       "      <th>fga_per_g_5_team_one</th>\n",
       "      <th>fga_per_g_6_team_one</th>\n",
       "      <th>fga_per_g_7_team_one</th>\n",
       "      <th>fga_per_g_8_team_one</th>\n",
       "      <th>fga_per_g_9_team_one</th>\n",
       "      <th>fga_per_g_10_team_one</th>\n",
       "      <th>fga_per_g_11_team_one</th>\n",
       "      <th>fga_per_g_12_team_one</th>\n",
       "      <th>fga_per_g_13_team_one</th>\n",
       "      <th>ft_pct_1_team_one</th>\n",
       "      <th>ft_pct_2_team_one</th>\n",
       "      <th>ft_pct_3_team_one</th>\n",
       "      <th>ft_pct_4_team_one</th>\n",
       "      <th>ft_pct_5_team_one</th>\n",
       "      <th>ft_pct_6_team_one</th>\n",
       "      <th>ft_pct_7_team_one</th>\n",
       "      <th>ft_pct_8_team_one</th>\n",
       "      <th>ft_pct_9_team_one</th>\n",
       "      <th>ft_pct_10_team_one</th>\n",
       "      <th>ft_pct_11_team_one</th>\n",
       "      <th>ft_pct_12_team_one</th>\n",
       "      <th>ft_pct_13_team_one</th>\n",
       "      <th>ft_per_g_1_team_one</th>\n",
       "      <th>ft_per_g_2_team_one</th>\n",
       "      <th>ft_per_g_3_team_one</th>\n",
       "      <th>ft_per_g_4_team_one</th>\n",
       "      <th>ft_per_g_5_team_one</th>\n",
       "      <th>ft_per_g_6_team_one</th>\n",
       "      <th>ft_per_g_7_team_one</th>\n",
       "      <th>ft_per_g_8_team_one</th>\n",
       "      <th>ft_per_g_9_team_one</th>\n",
       "      <th>ft_per_g_10_team_one</th>\n",
       "      <th>ft_per_g_11_team_one</th>\n",
       "      <th>ft_per_g_12_team_one</th>\n",
       "      <th>ft_per_g_13_team_one</th>\n",
       "      <th>fta_per_g_1_team_one</th>\n",
       "      <th>fta_per_g_2_team_one</th>\n",
       "      <th>fta_per_g_3_team_one</th>\n",
       "      <th>fta_per_g_4_team_one</th>\n",
       "      <th>fta_per_g_5_team_one</th>\n",
       "      <th>fta_per_g_6_team_one</th>\n",
       "      <th>fta_per_g_7_team_one</th>\n",
       "      <th>fta_per_g_8_team_one</th>\n",
       "      <th>fta_per_g_9_team_one</th>\n",
       "      <th>fta_per_g_10_team_one</th>\n",
       "      <th>fta_per_g_11_team_one</th>\n",
       "      <th>fta_per_g_12_team_one</th>\n",
       "      <th>fta_per_g_13_team_one</th>\n",
       "      <th>height_1_team_one</th>\n",
       "      <th>height_2_team_one</th>\n",
       "      <th>height_3_team_one</th>\n",
       "      <th>height_4_team_one</th>\n",
       "      <th>...</th>\n",
       "      <th>ft_pct_4_team_two</th>\n",
       "      <th>ft_pct_5_team_two</th>\n",
       "      <th>ft_pct_6_team_two</th>\n",
       "      <th>ft_pct_7_team_two</th>\n",
       "      <th>ft_pct_8_team_two</th>\n",
       "      <th>ft_pct_9_team_two</th>\n",
       "      <th>ft_pct_10_team_two</th>\n",
       "      <th>ft_pct_11_team_two</th>\n",
       "      <th>ft_pct_12_team_two</th>\n",
       "      <th>ft_pct_13_team_two</th>\n",
       "      <th>ft_per_g_1_team_two</th>\n",
       "      <th>ft_per_g_2_team_two</th>\n",
       "      <th>ft_per_g_3_team_two</th>\n",
       "      <th>ft_per_g_4_team_two</th>\n",
       "      <th>ft_per_g_5_team_two</th>\n",
       "      <th>ft_per_g_6_team_two</th>\n",
       "      <th>ft_per_g_7_team_two</th>\n",
       "      <th>ft_per_g_8_team_two</th>\n",
       "      <th>ft_per_g_9_team_two</th>\n",
       "      <th>ft_per_g_10_team_two</th>\n",
       "      <th>ft_per_g_11_team_two</th>\n",
       "      <th>ft_per_g_12_team_two</th>\n",
       "      <th>ft_per_g_13_team_two</th>\n",
       "      <th>fta_per_g_1_team_two</th>\n",
       "      <th>fta_per_g_2_team_two</th>\n",
       "      <th>fta_per_g_3_team_two</th>\n",
       "      <th>fta_per_g_4_team_two</th>\n",
       "      <th>fta_per_g_5_team_two</th>\n",
       "      <th>fta_per_g_6_team_two</th>\n",
       "      <th>fta_per_g_7_team_two</th>\n",
       "      <th>fta_per_g_8_team_two</th>\n",
       "      <th>fta_per_g_9_team_two</th>\n",
       "      <th>fta_per_g_10_team_two</th>\n",
       "      <th>fta_per_g_11_team_two</th>\n",
       "      <th>fta_per_g_12_team_two</th>\n",
       "      <th>fta_per_g_13_team_two</th>\n",
       "      <th>height_1_team_two</th>\n",
       "      <th>height_2_team_two</th>\n",
       "      <th>height_3_team_two</th>\n",
       "      <th>height_4_team_two</th>\n",
       "      <th>height_5_team_two</th>\n",
       "      <th>height_6_team_two</th>\n",
       "      <th>height_7_team_two</th>\n",
       "      <th>height_8_team_two</th>\n",
       "      <th>height_9_team_two</th>\n",
       "      <th>height_10_team_two</th>\n",
       "      <th>height_11_team_two</th>\n",
       "      <th>height_12_team_two</th>\n",
       "      <th>height_13_team_two</th>\n",
       "      <th>mp_per_g_1_team_two</th>\n",
       "      <th>mp_per_g_2_team_two</th>\n",
       "      <th>mp_per_g_3_team_two</th>\n",
       "      <th>mp_per_g_4_team_two</th>\n",
       "      <th>mp_per_g_5_team_two</th>\n",
       "      <th>mp_per_g_6_team_two</th>\n",
       "      <th>mp_per_g_7_team_two</th>\n",
       "      <th>mp_per_g_8_team_two</th>\n",
       "      <th>mp_per_g_9_team_two</th>\n",
       "      <th>mp_per_g_10_team_two</th>\n",
       "      <th>mp_per_g_11_team_two</th>\n",
       "      <th>mp_per_g_12_team_two</th>\n",
       "      <th>mp_per_g_13_team_two</th>\n",
       "      <th>orb_per_g_1_team_two</th>\n",
       "      <th>orb_per_g_2_team_two</th>\n",
       "      <th>orb_per_g_3_team_two</th>\n",
       "      <th>orb_per_g_4_team_two</th>\n",
       "      <th>orb_per_g_5_team_two</th>\n",
       "      <th>orb_per_g_6_team_two</th>\n",
       "      <th>orb_per_g_7_team_two</th>\n",
       "      <th>orb_per_g_8_team_two</th>\n",
       "      <th>orb_per_g_9_team_two</th>\n",
       "      <th>orb_per_g_10_team_two</th>\n",
       "      <th>orb_per_g_11_team_two</th>\n",
       "      <th>orb_per_g_12_team_two</th>\n",
       "      <th>orb_per_g_13_team_two</th>\n",
       "      <th>pf_per_g_1_team_two</th>\n",
       "      <th>pf_per_g_2_team_two</th>\n",
       "      <th>pf_per_g_3_team_two</th>\n",
       "      <th>pf_per_g_4_team_two</th>\n",
       "      <th>pf_per_g_5_team_two</th>\n",
       "      <th>pf_per_g_6_team_two</th>\n",
       "      <th>pf_per_g_7_team_two</th>\n",
       "      <th>pf_per_g_8_team_two</th>\n",
       "      <th>pf_per_g_9_team_two</th>\n",
       "      <th>pf_per_g_10_team_two</th>\n",
       "      <th>pf_per_g_11_team_two</th>\n",
       "      <th>pf_per_g_12_team_two</th>\n",
       "      <th>pf_per_g_13_team_two</th>\n",
       "      <th>pts_per_g_1_team_two</th>\n",
       "      <th>pts_per_g_2_team_two</th>\n",
       "      <th>pts_per_g_3_team_two</th>\n",
       "      <th>pts_per_g_4_team_two</th>\n",
       "      <th>pts_per_g_5_team_two</th>\n",
       "      <th>pts_per_g_6_team_two</th>\n",
       "      <th>pts_per_g_7_team_two</th>\n",
       "      <th>pts_per_g_8_team_two</th>\n",
       "      <th>pts_per_g_9_team_two</th>\n",
       "      <th>pts_per_g_10_team_two</th>\n",
       "      <th>pts_per_g_11_team_two</th>\n",
       "      <th>pts_per_g_12_team_two</th>\n",
       "      <th>pts_per_g_13_team_two</th>\n",
       "      <th>stl_per_g_1_team_two</th>\n",
       "      <th>stl_per_g_2_team_two</th>\n",
       "      <th>stl_per_g_3_team_two</th>\n",
       "      <th>stl_per_g_4_team_two</th>\n",
       "      <th>stl_per_g_5_team_two</th>\n",
       "      <th>stl_per_g_6_team_two</th>\n",
       "      <th>stl_per_g_7_team_two</th>\n",
       "      <th>stl_per_g_8_team_two</th>\n",
       "      <th>stl_per_g_9_team_two</th>\n",
       "      <th>stl_per_g_10_team_two</th>\n",
       "      <th>stl_per_g_11_team_two</th>\n",
       "      <th>stl_per_g_12_team_two</th>\n",
       "      <th>stl_per_g_13_team_two</th>\n",
       "      <th>tov_per_g_1_team_two</th>\n",
       "      <th>tov_per_g_2_team_two</th>\n",
       "      <th>tov_per_g_3_team_two</th>\n",
       "      <th>tov_per_g_4_team_two</th>\n",
       "      <th>tov_per_g_5_team_two</th>\n",
       "      <th>tov_per_g_6_team_two</th>\n",
       "      <th>tov_per_g_7_team_two</th>\n",
       "      <th>tov_per_g_8_team_two</th>\n",
       "      <th>tov_per_g_9_team_two</th>\n",
       "      <th>tov_per_g_10_team_two</th>\n",
       "      <th>tov_per_g_11_team_two</th>\n",
       "      <th>tov_per_g_12_team_two</th>\n",
       "      <th>tov_per_g_13_team_two</th>\n",
       "      <th>trb_per_g_1_team_two</th>\n",
       "      <th>trb_per_g_2_team_two</th>\n",
       "      <th>trb_per_g_3_team_two</th>\n",
       "      <th>trb_per_g_4_team_two</th>\n",
       "      <th>trb_per_g_5_team_two</th>\n",
       "      <th>trb_per_g_6_team_two</th>\n",
       "      <th>trb_per_g_7_team_two</th>\n",
       "      <th>trb_per_g_8_team_two</th>\n",
       "      <th>trb_per_g_9_team_two</th>\n",
       "      <th>trb_per_g_10_team_two</th>\n",
       "      <th>trb_per_g_11_team_two</th>\n",
       "      <th>trb_per_g_12_team_two</th>\n",
       "      <th>trb_per_g_13_team_two</th>\n",
       "      <th>weight_1_team_two</th>\n",
       "      <th>weight_2_team_two</th>\n",
       "      <th>weight_3_team_two</th>\n",
       "      <th>weight_4_team_two</th>\n",
       "      <th>weight_5_team_two</th>\n",
       "      <th>weight_6_team_two</th>\n",
       "      <th>weight_7_team_two</th>\n",
       "      <th>weight_8_team_two</th>\n",
       "      <th>weight_9_team_two</th>\n",
       "      <th>weight_10_team_two</th>\n",
       "      <th>weight_11_team_two</th>\n",
       "      <th>weight_12_team_two</th>\n",
       "      <th>weight_13_team_two</th>\n",
       "      <th>num_C_team_two</th>\n",
       "      <th>ast_per_g_14_team_two</th>\n",
       "      <th>blk_per_g_14_team_two</th>\n",
       "      <th>drb_per_g_14_team_two</th>\n",
       "      <th>fg2_pct_14_team_two</th>\n",
       "      <th>fg2_per_g_14_team_two</th>\n",
       "      <th>fg2a_per_g_14_team_two</th>\n",
       "      <th>fg3_pct_14_team_two</th>\n",
       "      <th>fg3_per_g_14_team_two</th>\n",
       "      <th>fg3a_per_g_14_team_two</th>\n",
       "      <th>fg_pct_14_team_two</th>\n",
       "      <th>fg_per_g_14_team_two</th>\n",
       "      <th>fga_per_g_14_team_two</th>\n",
       "      <th>ft_pct_14_team_two</th>\n",
       "      <th>ft_per_g_14_team_two</th>\n",
       "      <th>fta_per_g_14_team_two</th>\n",
       "      <th>height_14_team_two</th>\n",
       "      <th>mp_per_g_14_team_two</th>\n",
       "      <th>orb_per_g_14_team_two</th>\n",
       "      <th>pf_per_g_14_team_two</th>\n",
       "      <th>pts_per_g_14_team_two</th>\n",
       "      <th>stl_per_g_14_team_two</th>\n",
       "      <th>tov_per_g_14_team_two</th>\n",
       "      <th>trb_per_g_14_team_two</th>\n",
       "      <th>weight_14_team_two</th>\n",
       "      <th>ast_per_g_15_team_two</th>\n",
       "      <th>ast_per_g_16_team_two</th>\n",
       "      <th>blk_per_g_15_team_two</th>\n",
       "      <th>blk_per_g_16_team_two</th>\n",
       "      <th>drb_per_g_15_team_two</th>\n",
       "      <th>drb_per_g_16_team_two</th>\n",
       "      <th>fg2_pct_15_team_two</th>\n",
       "      <th>fg2_pct_16_team_two</th>\n",
       "      <th>fg2_per_g_15_team_two</th>\n",
       "      <th>fg2_per_g_16_team_two</th>\n",
       "      <th>fg2a_per_g_15_team_two</th>\n",
       "      <th>fg2a_per_g_16_team_two</th>\n",
       "      <th>fg3_pct_15_team_two</th>\n",
       "      <th>fg3_pct_16_team_two</th>\n",
       "      <th>fg3_per_g_15_team_two</th>\n",
       "      <th>fg3_per_g_16_team_two</th>\n",
       "      <th>fg3a_per_g_15_team_two</th>\n",
       "      <th>fg3a_per_g_16_team_two</th>\n",
       "      <th>fg_pct_15_team_two</th>\n",
       "      <th>fg_pct_16_team_two</th>\n",
       "      <th>fg_per_g_15_team_two</th>\n",
       "      <th>fg_per_g_16_team_two</th>\n",
       "      <th>fga_per_g_15_team_two</th>\n",
       "      <th>fga_per_g_16_team_two</th>\n",
       "      <th>ft_pct_15_team_two</th>\n",
       "      <th>ft_pct_16_team_two</th>\n",
       "      <th>ft_per_g_15_team_two</th>\n",
       "      <th>ft_per_g_16_team_two</th>\n",
       "      <th>fta_per_g_15_team_two</th>\n",
       "      <th>fta_per_g_16_team_two</th>\n",
       "      <th>height_15_team_two</th>\n",
       "      <th>height_16_team_two</th>\n",
       "      <th>mp_per_g_15_team_two</th>\n",
       "      <th>mp_per_g_16_team_two</th>\n",
       "      <th>orb_per_g_15_team_two</th>\n",
       "      <th>orb_per_g_16_team_two</th>\n",
       "      <th>pf_per_g_15_team_two</th>\n",
       "      <th>pf_per_g_16_team_two</th>\n",
       "      <th>pts_per_g_15_team_two</th>\n",
       "      <th>pts_per_g_16_team_two</th>\n",
       "      <th>stl_per_g_15_team_two</th>\n",
       "      <th>stl_per_g_16_team_two</th>\n",
       "      <th>tov_per_g_15_team_two</th>\n",
       "      <th>tov_per_g_16_team_two</th>\n",
       "      <th>trb_per_g_15_team_two</th>\n",
       "      <th>trb_per_g_16_team_two</th>\n",
       "      <th>weight_15_team_two</th>\n",
       "      <th>weight_16_team_two</th>\n",
       "      <th>year_team_two</th>\n",
       "      <th>week1_team_two</th>\n",
       "      <th>week2_team_two</th>\n",
       "      <th>week3_team_two</th>\n",
       "      <th>week4_team_two</th>\n",
       "      <th>week5_team_two</th>\n",
       "      <th>week6_team_two</th>\n",
       "      <th>week7_team_two</th>\n",
       "      <th>week8_team_two</th>\n",
       "      <th>week9_team_two</th>\n",
       "      <th>week10_team_two</th>\n",
       "      <th>week11_team_two</th>\n",
       "      <th>week12_team_two</th>\n",
       "      <th>week13_team_two</th>\n",
       "      <th>week14_team_two</th>\n",
       "      <th>week15_team_two</th>\n",
       "      <th>week16_team_two</th>\n",
       "      <th>week17_team_two</th>\n",
       "      <th>week18_team_two</th>\n",
       "      <th>max_team_two</th>\n",
       "      <th>final_team_two</th>\n",
       "      <th>result</th>\n",
       "      <th>score_one</th>\n",
       "      <th>score_two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>1</td>\n",
       "      <td>29.25</td>\n",
       "      <td>119.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>73.7</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9.88</td>\n",
       "      <td>109.1</td>\n",
       "      <td>99.3</td>\n",
       "      <td>1.62</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>65.9</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.415</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2552</td>\n",
       "      <td>3060</td>\n",
       "      <td>9.38</td>\n",
       "      <td>23.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.569</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>35.1</td>\n",
       "      <td>27.1</td>\n",
       "      <td>26.9</td>\n",
       "      <td>23.7</td>\n",
       "      <td>21.1</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>9.7</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>19.9</td>\n",
       "      <td>11.1</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6.9</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>275.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>1</td>\n",
       "      <td>29.25</td>\n",
       "      <td>119.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>73.7</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9.88</td>\n",
       "      <td>109.1</td>\n",
       "      <td>99.3</td>\n",
       "      <td>1.62</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>65.9</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.415</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2552</td>\n",
       "      <td>3060</td>\n",
       "      <td>9.38</td>\n",
       "      <td>23.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.569</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>34.2</td>\n",
       "      <td>32.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>30.1</td>\n",
       "      <td>28.2</td>\n",
       "      <td>23.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>12.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>8.1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>1</td>\n",
       "      <td>29.25</td>\n",
       "      <td>119.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>73.7</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9.88</td>\n",
       "      <td>109.1</td>\n",
       "      <td>99.3</td>\n",
       "      <td>1.62</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>65.9</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.415</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2552</td>\n",
       "      <td>3060</td>\n",
       "      <td>9.38</td>\n",
       "      <td>23.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.569</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.548</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>83.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>27.4</td>\n",
       "      <td>27.1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>17.2</td>\n",
       "      <td>16.8</td>\n",
       "      <td>16.7</td>\n",
       "      <td>14.1</td>\n",
       "      <td>13.9</td>\n",
       "      <td>13.1</td>\n",
       "      <td>12.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>17.5</td>\n",
       "      <td>11.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>8.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>185.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>1</td>\n",
       "      <td>29.25</td>\n",
       "      <td>119.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>73.7</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9.88</td>\n",
       "      <td>109.1</td>\n",
       "      <td>99.3</td>\n",
       "      <td>1.62</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>65.9</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.415</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2552</td>\n",
       "      <td>3060</td>\n",
       "      <td>9.38</td>\n",
       "      <td>23.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.569</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>28.7</td>\n",
       "      <td>27.7</td>\n",
       "      <td>24.8</td>\n",
       "      <td>8.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>11.3</td>\n",
       "      <td>9.2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>252.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>1</td>\n",
       "      <td>29.25</td>\n",
       "      <td>119.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>73.7</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9.88</td>\n",
       "      <td>109.1</td>\n",
       "      <td>99.3</td>\n",
       "      <td>1.62</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>65.9</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.415</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2552</td>\n",
       "      <td>3060</td>\n",
       "      <td>9.38</td>\n",
       "      <td>23.5</td>\n",
       "      <td>15.5</td>\n",
       "      <td>52.4</td>\n",
       "      <td>0.569</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.500</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>30.9</td>\n",
       "      <td>30.2</td>\n",
       "      <td>27.5</td>\n",
       "      <td>26.8</td>\n",
       "      <td>20.9</td>\n",
       "      <td>15.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>19.8</td>\n",
       "      <td>15.6</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.4</td>\n",
       "      <td>8.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>11.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  917 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Seed_team_one Team_team_one  Conference_team_one  AdjEM_team_one  \\\n",
       "0              1      Maryland                    1           29.25   \n",
       "1              1      Maryland                    1           29.25   \n",
       "2              1      Maryland                    1           29.25   \n",
       "3              1      Maryland                    1           29.25   \n",
       "4              1      Maryland                    1           29.25   \n",
       "\n",
       "   AdjO_team_one  AdjD_team_one  AdjT_team_one  Luck_team_one  \\\n",
       "0          119.2           89.9           73.7          0.025   \n",
       "1          119.2           89.9           73.7          0.025   \n",
       "2          119.2           89.9           73.7          0.025   \n",
       "3          119.2           89.9           73.7          0.025   \n",
       "4          119.2           89.9           73.7          0.025   \n",
       "\n",
       "   Opp AdjEM_team_one  OppO_team_one  OppD_team_one  NCSOS AdjEM_team_one  \\\n",
       "0                9.88          109.1           99.3                  1.62   \n",
       "1                9.88          109.1           99.3                  1.62   \n",
       "2                9.88          109.1           99.3                  1.62   \n",
       "3                9.88          109.1           99.3                  1.62   \n",
       "4                9.88          109.1           99.3                  1.62   \n",
       "\n",
       "   W_team_one  L_team_one  ast_pct_team_one  blk_pct_team_one  \\\n",
       "0          32           4              65.9               9.4   \n",
       "1          32           4              65.9               9.4   \n",
       "2          32           4              65.9               9.4   \n",
       "3          32           4              65.9               9.4   \n",
       "4          32           4              65.9               9.4   \n",
       "\n",
       "   efg_pct_team_one  fg3a_per_fga_pct_team_one  ft_rate_team_one  \\\n",
       "0              0.53                      0.258             0.301   \n",
       "1              0.53                      0.258             0.301   \n",
       "2              0.53                      0.258             0.301   \n",
       "3              0.53                      0.258             0.301   \n",
       "4              0.53                      0.258             0.301   \n",
       "\n",
       "   fta_per_fga_pct_team_one  losses_conf_team_one  losses_home_team_one  \\\n",
       "0                     0.415                     1                     0   \n",
       "1                     0.415                     1                     0   \n",
       "2                     0.415                     1                     0   \n",
       "3                     0.415                     1                     0   \n",
       "4                     0.415                     1                     0   \n",
       "\n",
       "   losses_visitor_team_one  opp_pts_team_one  pts_team_one  sos_team_one  \\\n",
       "0                        2              2552          3060          9.38   \n",
       "1                        2              2552          3060          9.38   \n",
       "2                        2              2552          3060          9.38   \n",
       "3                        2              2552          3060          9.38   \n",
       "4                        2              2552          3060          9.38   \n",
       "\n",
       "   srs_team_one  tov_pct_team_one  trb_pct_team_one  ts_pct_team_one  \\\n",
       "0          23.5              15.5              52.4            0.569   \n",
       "1          23.5              15.5              52.4            0.569   \n",
       "2          23.5              15.5              52.4            0.569   \n",
       "3          23.5              15.5              52.4            0.569   \n",
       "4          23.5              15.5              52.4            0.569   \n",
       "\n",
       "   wins_conf_team_one  wins_home_team_one  wins_visitor_team_one  \\\n",
       "0                  15                  15                      7   \n",
       "1                  15                  15                      7   \n",
       "2                  15                  15                      7   \n",
       "3                  15                  15                      7   \n",
       "4                  15                  15                      7   \n",
       "\n",
       "   def_rtg_team_one  opp_fta_per_fga_pct_team_one  \\\n",
       "0               0.0                           0.0   \n",
       "1               0.0                           0.0   \n",
       "2               0.0                           0.0   \n",
       "3               0.0                           0.0   \n",
       "4               0.0                           0.0   \n",
       "\n",
       "   opp_fg3a_per_fga_pct_team_one  opp_ts_pct_team_one  opp_trb_pct_team_one  \\\n",
       "0                            0.0                  0.0                  47.6   \n",
       "1                            0.0                  0.0                  47.6   \n",
       "2                            0.0                  0.0                  47.6   \n",
       "3                            0.0                  0.0                  47.6   \n",
       "4                            0.0                  0.0                  47.6   \n",
       "\n",
       "   opp_ast_pct_team_one  opp_stl_pct_team_one  opp_blk_pct_team_one  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   opp_efg_pct_team_one  opp_tov_pct_team_one  opp_orb_pct_team_one  \\\n",
       "0                 0.399                   0.0                   0.0   \n",
       "1                 0.399                   0.0                   0.0   \n",
       "2                 0.399                   0.0                   0.0   \n",
       "3                 0.399                   0.0                   0.0   \n",
       "4                 0.399                   0.0                   0.0   \n",
       "\n",
       "   opp_ft_rate_team_one  num_JR_team_one  num_FR_team_one  num_SR_team_one  \\\n",
       "0                   0.0              5.0              2.0              4.0   \n",
       "1                   0.0              5.0              2.0              4.0   \n",
       "2                   0.0              5.0              2.0              4.0   \n",
       "3                   0.0              5.0              2.0              4.0   \n",
       "4                   0.0              5.0              2.0              4.0   \n",
       "\n",
       "   num_SO_team_one  num_G_team_one  num_F_team_one  ast_per_g_1_team_one  \\\n",
       "0              1.0               7               2                   7.9   \n",
       "1              1.0               7               2                   7.9   \n",
       "2              1.0               7               2                   7.9   \n",
       "3              1.0               7               2                   7.9   \n",
       "4              1.0               7               2                   7.9   \n",
       "\n",
       "   ast_per_g_2_team_one  ast_per_g_3_team_one  ast_per_g_4_team_one  \\\n",
       "0                   2.9                   2.4                   2.1   \n",
       "1                   2.9                   2.4                   2.1   \n",
       "2                   2.9                   2.4                   2.1   \n",
       "3                   2.9                   2.4                   2.1   \n",
       "4                   2.9                   2.4                   2.1   \n",
       "\n",
       "   ast_per_g_5_team_one  ast_per_g_6_team_one  ast_per_g_7_team_one  \\\n",
       "0                   1.5                   1.2                   0.9   \n",
       "1                   1.5                   1.2                   0.9   \n",
       "2                   1.5                   1.2                   0.9   \n",
       "3                   1.5                   1.2                   0.9   \n",
       "4                   1.5                   1.2                   0.9   \n",
       "\n",
       "   ast_per_g_8_team_one  ast_per_g_9_team_one  ast_per_g_10_team_one  \\\n",
       "0                   0.8                   0.8                    0.3   \n",
       "1                   0.8                   0.8                    0.3   \n",
       "2                   0.8                   0.8                    0.3   \n",
       "3                   0.8                   0.8                    0.3   \n",
       "4                   0.8                   0.8                    0.3   \n",
       "\n",
       "   ast_per_g_11_team_one  ast_per_g_12_team_one  ast_per_g_13_team_one  \\\n",
       "0                    0.1                    0.1                    0.0   \n",
       "1                    0.1                    0.1                    0.0   \n",
       "2                    0.1                    0.1                    0.0   \n",
       "3                    0.1                    0.1                    0.0   \n",
       "4                    0.1                    0.1                    0.0   \n",
       "\n",
       "   blk_per_g_1_team_one  blk_per_g_2_team_one  blk_per_g_3_team_one  \\\n",
       "0                   2.0                   1.5                   0.9   \n",
       "1                   2.0                   1.5                   0.9   \n",
       "2                   2.0                   1.5                   0.9   \n",
       "3                   2.0                   1.5                   0.9   \n",
       "4                   2.0                   1.5                   0.9   \n",
       "\n",
       "   blk_per_g_4_team_one  blk_per_g_5_team_one  blk_per_g_6_team_one  \\\n",
       "0                   0.5                   0.4                   0.3   \n",
       "1                   0.5                   0.4                   0.3   \n",
       "2                   0.5                   0.4                   0.3   \n",
       "3                   0.5                   0.4                   0.3   \n",
       "4                   0.5                   0.4                   0.3   \n",
       "\n",
       "   blk_per_g_7_team_one  blk_per_g_8_team_one  blk_per_g_9_team_one  \\\n",
       "0                   0.2                   0.2                   0.1   \n",
       "1                   0.2                   0.2                   0.1   \n",
       "2                   0.2                   0.2                   0.1   \n",
       "3                   0.2                   0.2                   0.1   \n",
       "4                   0.2                   0.2                   0.1   \n",
       "\n",
       "   blk_per_g_10_team_one  blk_per_g_11_team_one  blk_per_g_12_team_one  \\\n",
       "0                    0.0                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.0                    0.0                    0.0   \n",
       "3                    0.0                    0.0                    0.0   \n",
       "4                    0.0                    0.0                    0.0   \n",
       "\n",
       "   blk_per_g_13_team_one  drb_per_g_1_team_one  drb_per_g_2_team_one  \\\n",
       "0                    0.0                   5.9                   4.4   \n",
       "1                    0.0                   5.9                   4.4   \n",
       "2                    0.0                   5.9                   4.4   \n",
       "3                    0.0                   5.9                   4.4   \n",
       "4                    0.0                   5.9                   4.4   \n",
       "\n",
       "   drb_per_g_3_team_one  drb_per_g_4_team_one  drb_per_g_5_team_one  \\\n",
       "0                   3.3                   3.3                   2.5   \n",
       "1                   3.3                   3.3                   2.5   \n",
       "2                   3.3                   3.3                   2.5   \n",
       "3                   3.3                   3.3                   2.5   \n",
       "4                   3.3                   3.3                   2.5   \n",
       "\n",
       "   drb_per_g_6_team_one  drb_per_g_7_team_one  drb_per_g_8_team_one  \\\n",
       "0                   2.1                   2.0                   1.8   \n",
       "1                   2.1                   2.0                   1.8   \n",
       "2                   2.1                   2.0                   1.8   \n",
       "3                   2.1                   2.0                   1.8   \n",
       "4                   2.1                   2.0                   1.8   \n",
       "\n",
       "   drb_per_g_9_team_one  drb_per_g_10_team_one  drb_per_g_11_team_one  \\\n",
       "0                   0.7                    0.5                    0.3   \n",
       "1                   0.7                    0.5                    0.3   \n",
       "2                   0.7                    0.5                    0.3   \n",
       "3                   0.7                    0.5                    0.3   \n",
       "4                   0.7                    0.5                    0.3   \n",
       "\n",
       "   drb_per_g_12_team_one  drb_per_g_13_team_one  fg2_pct_1_team_one  \\\n",
       "0                    0.1                    0.0               0.765   \n",
       "1                    0.1                    0.0               0.765   \n",
       "2                    0.1                    0.0               0.765   \n",
       "3                    0.1                    0.0               0.765   \n",
       "4                    0.1                    0.0               0.765   \n",
       "\n",
       "   fg2_pct_2_team_one  fg2_pct_3_team_one  fg2_pct_4_team_one  \\\n",
       "0               0.667               0.579               0.556   \n",
       "1               0.667               0.579               0.556   \n",
       "2               0.667               0.579               0.556   \n",
       "3               0.667               0.579               0.556   \n",
       "4               0.667               0.579               0.556   \n",
       "\n",
       "   fg2_pct_5_team_one  fg2_pct_6_team_one  fg2_pct_7_team_one  \\\n",
       "0               0.547               0.525               0.519   \n",
       "1               0.547               0.525               0.519   \n",
       "2               0.547               0.525               0.519   \n",
       "3               0.547               0.525               0.519   \n",
       "4               0.547               0.525               0.519   \n",
       "\n",
       "   fg2_pct_8_team_one  fg2_pct_9_team_one  fg2_pct_10_team_one  \\\n",
       "0               0.516               0.507                0.464   \n",
       "1               0.516               0.507                0.464   \n",
       "2               0.516               0.507                0.464   \n",
       "3               0.516               0.507                0.464   \n",
       "4               0.516               0.507                0.464   \n",
       "\n",
       "   fg2_pct_11_team_one  fg2_pct_12_team_one  fg2_pct_13_team_one  \\\n",
       "0                0.427                0.333                  0.0   \n",
       "1                0.427                0.333                  0.0   \n",
       "2                0.427                0.333                  0.0   \n",
       "3                0.427                0.333                  0.0   \n",
       "4                0.427                0.333                  0.0   \n",
       "\n",
       "   fg2_per_g_1_team_one  fg2_per_g_2_team_one  fg2_per_g_3_team_one  \\\n",
       "0                   5.5                   4.8                   4.4   \n",
       "1                   5.5                   4.8                   4.4   \n",
       "2                   5.5                   4.8                   4.4   \n",
       "3                   5.5                   4.8                   4.4   \n",
       "4                   5.5                   4.8                   4.4   \n",
       "\n",
       "   fg2_per_g_4_team_one  fg2_per_g_5_team_one  fg2_per_g_6_team_one  \\\n",
       "0                   3.6                   1.6                   1.3   \n",
       "1                   3.6                   1.6                   1.3   \n",
       "2                   3.6                   1.6                   1.3   \n",
       "3                   3.6                   1.6                   1.3   \n",
       "4                   3.6                   1.6                   1.3   \n",
       "\n",
       "   fg2_per_g_7_team_one  fg2_per_g_8_team_one  fg2_per_g_9_team_one  \\\n",
       "0                   1.3                   1.2                   0.6   \n",
       "1                   1.3                   1.2                   0.6   \n",
       "2                   1.3                   1.2                   0.6   \n",
       "3                   1.3                   1.2                   0.6   \n",
       "4                   1.3                   1.2                   0.6   \n",
       "\n",
       "   fg2_per_g_10_team_one  fg2_per_g_11_team_one  fg2_per_g_12_team_one  \\\n",
       "0                    0.3                    0.2                    0.1   \n",
       "1                    0.3                    0.2                    0.1   \n",
       "2                    0.3                    0.2                    0.1   \n",
       "3                    0.3                    0.2                    0.1   \n",
       "4                    0.3                    0.2                    0.1   \n",
       "\n",
       "   fg2_per_g_13_team_one  fg2a_per_g_1_team_one  fg2a_per_g_2_team_one  \\\n",
       "0                    0.0                   10.1                    9.5   \n",
       "1                    0.0                   10.1                    9.5   \n",
       "2                    0.0                   10.1                    9.5   \n",
       "3                    0.0                   10.1                    9.5   \n",
       "4                    0.0                   10.1                    9.5   \n",
       "\n",
       "   fg2a_per_g_3_team_one  fg2a_per_g_4_team_one  fg2a_per_g_5_team_one  \\\n",
       "0                    8.4                    7.0                    3.1   \n",
       "1                    8.4                    7.0                    3.1   \n",
       "2                    8.4                    7.0                    3.1   \n",
       "3                    8.4                    7.0                    3.1   \n",
       "4                    8.4                    7.0                    3.1   \n",
       "\n",
       "   fg2a_per_g_6_team_one  fg2a_per_g_7_team_one  fg2a_per_g_8_team_one  \\\n",
       "0                    3.1                    2.7                    2.1   \n",
       "1                    3.1                    2.7                    2.1   \n",
       "2                    3.1                    2.7                    2.1   \n",
       "3                    3.1                    2.7                    2.1   \n",
       "4                    3.1                    2.7                    2.1   \n",
       "\n",
       "   fg2a_per_g_9_team_one  fg2a_per_g_10_team_one  fg2a_per_g_11_team_one  \\\n",
       "0                    0.8                     0.5                     0.3   \n",
       "1                    0.8                     0.5                     0.3   \n",
       "2                    0.8                     0.5                     0.3   \n",
       "3                    0.8                     0.5                     0.3   \n",
       "4                    0.8                     0.5                     0.3   \n",
       "\n",
       "   fg2a_per_g_12_team_one  fg2a_per_g_13_team_one  fg3_pct_1_team_one  \\\n",
       "0                     0.2                     0.0                 1.0   \n",
       "1                     0.2                     0.0                 1.0   \n",
       "2                     0.2                     0.0                 1.0   \n",
       "3                     0.2                     0.0                 1.0   \n",
       "4                     0.2                     0.0                 1.0   \n",
       "\n",
       "   fg3_pct_2_team_one  fg3_pct_3_team_one  fg3_pct_4_team_one  \\\n",
       "0                 0.5                 0.5               0.425   \n",
       "1                 0.5                 0.5               0.425   \n",
       "2                 0.5                 0.5               0.425   \n",
       "3                 0.5                 0.5               0.425   \n",
       "4                 0.5                 0.5               0.425   \n",
       "\n",
       "   fg3_pct_5_team_one  fg3_pct_6_team_one  fg3_pct_7_team_one  \\\n",
       "0               0.397               0.396               0.344   \n",
       "1               0.397               0.396               0.344   \n",
       "2               0.397               0.396               0.344   \n",
       "3               0.397               0.396               0.344   \n",
       "4               0.397               0.396               0.344   \n",
       "\n",
       "   fg3_pct_8_team_one  fg3_pct_9_team_one  fg3_pct_10_team_one  \\\n",
       "0               0.255                0.25                  0.0   \n",
       "1               0.255                0.25                  0.0   \n",
       "2               0.255                0.25                  0.0   \n",
       "3               0.255                0.25                  0.0   \n",
       "4               0.255                0.25                  0.0   \n",
       "\n",
       "   fg3_pct_11_team_one  fg3_pct_12_team_one  fg3_pct_13_team_one  \\\n",
       "0                  0.0                  0.0                  0.0   \n",
       "1                  0.0                  0.0                  0.0   \n",
       "2                  0.0                  0.0                  0.0   \n",
       "3                  0.0                  0.0                  0.0   \n",
       "4                  0.0                  0.0                  0.0   \n",
       "\n",
       "   fg3_per_g_1_team_one  fg3_per_g_2_team_one  fg3_per_g_3_team_one  \\\n",
       "0                   2.6                   1.2                   1.1   \n",
       "1                   2.6                   1.2                   1.1   \n",
       "2                   2.6                   1.2                   1.1   \n",
       "3                   2.6                   1.2                   1.1   \n",
       "4                   2.6                   1.2                   1.1   \n",
       "\n",
       "   fg3_per_g_4_team_one  fg3_per_g_5_team_one  fg3_per_g_6_team_one  \\\n",
       "0                   0.5                   0.4                   0.3   \n",
       "1                   0.5                   0.4                   0.3   \n",
       "2                   0.5                   0.4                   0.3   \n",
       "3                   0.5                   0.4                   0.3   \n",
       "4                   0.5                   0.4                   0.3   \n",
       "\n",
       "   fg3_per_g_7_team_one  fg3_per_g_8_team_one  fg3_per_g_9_team_one  \\\n",
       "0                   0.2                   0.1                   0.0   \n",
       "1                   0.2                   0.1                   0.0   \n",
       "2                   0.2                   0.1                   0.0   \n",
       "3                   0.2                   0.1                   0.0   \n",
       "4                   0.2                   0.1                   0.0   \n",
       "\n",
       "   fg3_per_g_10_team_one  fg3_per_g_11_team_one  fg3_per_g_12_team_one  \\\n",
       "0                    0.0                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.0                    0.0                    0.0   \n",
       "3                    0.0                    0.0                    0.0   \n",
       "4                    0.0                    0.0                    0.0   \n",
       "\n",
       "   fg3_per_g_13_team_one  fg3a_per_g_1_team_one  fg3a_per_g_2_team_one  \\\n",
       "0                    0.0                    6.4                    3.6   \n",
       "1                    0.0                    6.4                    3.6   \n",
       "2                    0.0                    6.4                    3.6   \n",
       "3                    0.0                    6.4                    3.6   \n",
       "4                    0.0                    6.4                    3.6   \n",
       "\n",
       "   fg3a_per_g_3_team_one  fg3a_per_g_4_team_one  fg3a_per_g_5_team_one  \\\n",
       "0                    2.7                    1.5                    1.1   \n",
       "1                    2.7                    1.5                    1.1   \n",
       "2                    2.7                    1.5                    1.1   \n",
       "3                    2.7                    1.5                    1.1   \n",
       "4                    2.7                    1.5                    1.1   \n",
       "\n",
       "   fg3a_per_g_6_team_one  fg3a_per_g_7_team_one  fg3a_per_g_8_team_one  \\\n",
       "0                    0.5                    0.5                    0.3   \n",
       "1                    0.5                    0.5                    0.3   \n",
       "2                    0.5                    0.5                    0.3   \n",
       "3                    0.5                    0.5                    0.3   \n",
       "4                    0.5                    0.5                    0.3   \n",
       "\n",
       "   fg3a_per_g_9_team_one  fg3a_per_g_10_team_one  fg3a_per_g_11_team_one  \\\n",
       "0                    0.1                     0.1                     0.0   \n",
       "1                    0.1                     0.1                     0.0   \n",
       "2                    0.1                     0.1                     0.0   \n",
       "3                    0.1                     0.1                     0.0   \n",
       "4                    0.1                     0.1                     0.0   \n",
       "\n",
       "   fg3a_per_g_12_team_one  fg3a_per_g_13_team_one  fg_pct_1_team_one  \\\n",
       "0                     0.0                     0.0              0.667   \n",
       "1                     0.0                     0.0              0.667   \n",
       "2                     0.0                     0.0              0.667   \n",
       "3                     0.0                     0.0              0.667   \n",
       "4                     0.0                     0.0              0.667   \n",
       "\n",
       "   fg_pct_2_team_one  fg_pct_3_team_one  fg_pct_4_team_one  fg_pct_5_team_one  \\\n",
       "0              0.545              0.526              0.524              0.504   \n",
       "1              0.545              0.526              0.524              0.504   \n",
       "2              0.545              0.526              0.524              0.504   \n",
       "3              0.545              0.526              0.524              0.504   \n",
       "4              0.545              0.526              0.524              0.504   \n",
       "\n",
       "   fg_pct_6_team_one  fg_pct_7_team_one  fg_pct_8_team_one  fg_pct_9_team_one  \\\n",
       "0                0.5              0.477              0.469              0.469   \n",
       "1                0.5              0.477              0.469              0.469   \n",
       "2                0.5              0.477              0.469              0.469   \n",
       "3                0.5              0.477              0.469              0.469   \n",
       "4                0.5              0.477              0.469              0.469   \n",
       "\n",
       "   fg_pct_10_team_one  fg_pct_11_team_one  fg_pct_12_team_one  \\\n",
       "0               0.453               0.382               0.286   \n",
       "1               0.453               0.382               0.286   \n",
       "2               0.453               0.382               0.286   \n",
       "3               0.453               0.382               0.286   \n",
       "4               0.453               0.382               0.286   \n",
       "\n",
       "   fg_pct_13_team_one  fg_per_g_1_team_one  fg_per_g_2_team_one  \\\n",
       "0                 0.0                  7.0                  5.5   \n",
       "1                 0.0                  7.0                  5.5   \n",
       "2                 0.0                  7.0                  5.5   \n",
       "3                 0.0                  7.0                  5.5   \n",
       "4                 0.0                  7.0                  5.5   \n",
       "\n",
       "   fg_per_g_3_team_one  fg_per_g_4_team_one  fg_per_g_5_team_one  \\\n",
       "0                  4.8                  4.0                  2.5   \n",
       "1                  4.8                  4.0                  2.5   \n",
       "2                  4.8                  4.0                  2.5   \n",
       "3                  4.8                  4.0                  2.5   \n",
       "4                  4.8                  4.0                  2.5   \n",
       "\n",
       "   fg_per_g_6_team_one  fg_per_g_7_team_one  fg_per_g_8_team_one  \\\n",
       "0                  2.3                  1.7                  1.6   \n",
       "1                  2.3                  1.7                  1.6   \n",
       "2                  2.3                  1.7                  1.6   \n",
       "3                  2.3                  1.7                  1.6   \n",
       "4                  2.3                  1.7                  1.6   \n",
       "\n",
       "   fg_per_g_9_team_one  fg_per_g_10_team_one  fg_per_g_11_team_one  \\\n",
       "0                  0.8                   0.5                   0.2   \n",
       "1                  0.8                   0.5                   0.2   \n",
       "2                  0.8                   0.5                   0.2   \n",
       "3                  0.8                   0.5                   0.2   \n",
       "4                  0.8                   0.5                   0.2   \n",
       "\n",
       "   fg_per_g_12_team_one  fg_per_g_13_team_one  fga_per_g_1_team_one  \\\n",
       "0                   0.1                   0.0                  14.9   \n",
       "1                   0.1                   0.0                  14.9   \n",
       "2                   0.1                   0.0                  14.9   \n",
       "3                   0.1                   0.0                  14.9   \n",
       "4                   0.1                   0.0                  14.9   \n",
       "\n",
       "   fga_per_g_2_team_one  fga_per_g_3_team_one  fga_per_g_4_team_one  \\\n",
       "0                  10.1                   9.5                   8.5   \n",
       "1                  10.1                   9.5                   8.5   \n",
       "2                  10.1                   9.5                   8.5   \n",
       "3                  10.1                   9.5                   8.5   \n",
       "4                  10.1                   9.5                   8.5   \n",
       "\n",
       "   fga_per_g_5_team_one  fga_per_g_6_team_one  fga_per_g_7_team_one  \\\n",
       "0                   6.6                   4.8                   3.8   \n",
       "1                   6.6                   4.8                   3.8   \n",
       "2                   6.6                   4.8                   3.8   \n",
       "3                   6.6                   4.8                   3.8   \n",
       "4                   6.6                   4.8                   3.8   \n",
       "\n",
       "   fga_per_g_8_team_one  fga_per_g_9_team_one  fga_per_g_10_team_one  \\\n",
       "0                   3.1                   1.2                    1.0   \n",
       "1                   3.1                   1.2                    1.0   \n",
       "2                   3.1                   1.2                    1.0   \n",
       "3                   3.1                   1.2                    1.0   \n",
       "4                   3.1                   1.2                    1.0   \n",
       "\n",
       "   fga_per_g_11_team_one  fga_per_g_12_team_one  fga_per_g_13_team_one  \\\n",
       "0                    0.4                    0.3                    0.0   \n",
       "1                    0.4                    0.3                    0.0   \n",
       "2                    0.4                    0.3                    0.0   \n",
       "3                    0.4                    0.3                    0.0   \n",
       "4                    0.4                    0.3                    0.0   \n",
       "\n",
       "   ft_pct_1_team_one  ft_pct_2_team_one  ft_pct_3_team_one  ft_pct_4_team_one  \\\n",
       "0                1.0              0.898              0.836              0.824   \n",
       "1                1.0              0.898              0.836              0.824   \n",
       "2                1.0              0.898              0.836              0.824   \n",
       "3                1.0              0.898              0.836              0.824   \n",
       "4                1.0              0.898              0.836              0.824   \n",
       "\n",
       "   ft_pct_5_team_one  ft_pct_6_team_one  ft_pct_7_team_one  ft_pct_8_team_one  \\\n",
       "0              0.803              0.778              0.767              0.623   \n",
       "1              0.803              0.778              0.767              0.623   \n",
       "2              0.803              0.778              0.767              0.623   \n",
       "3              0.803              0.778              0.767              0.623   \n",
       "4              0.803              0.778              0.767              0.623   \n",
       "\n",
       "   ft_pct_9_team_one  ft_pct_10_team_one  ft_pct_11_team_one  \\\n",
       "0              0.585               0.563                 0.5   \n",
       "1              0.585               0.563                 0.5   \n",
       "2              0.585               0.563                 0.5   \n",
       "3              0.585               0.563                 0.5   \n",
       "4              0.585               0.563                 0.5   \n",
       "\n",
       "   ft_pct_12_team_one  ft_pct_13_team_one  ft_per_g_1_team_one  \\\n",
       "0                 0.0                 0.0                  4.2   \n",
       "1                 0.0                 0.0                  4.2   \n",
       "2                 0.0                 0.0                  4.2   \n",
       "3                 0.0                 0.0                  4.2   \n",
       "4                 0.0                 0.0                  4.2   \n",
       "\n",
       "   ft_per_g_2_team_one  ft_per_g_3_team_one  ft_per_g_4_team_one  \\\n",
       "0                  3.9                  2.8                  2.4   \n",
       "1                  3.9                  2.8                  2.4   \n",
       "2                  3.9                  2.8                  2.4   \n",
       "3                  3.9                  2.8                  2.4   \n",
       "4                  3.9                  2.8                  2.4   \n",
       "\n",
       "   ft_per_g_5_team_one  ft_per_g_6_team_one  ft_per_g_7_team_one  \\\n",
       "0                  1.7                  1.7                  1.5   \n",
       "1                  1.7                  1.7                  1.5   \n",
       "2                  1.7                  1.7                  1.5   \n",
       "3                  1.7                  1.7                  1.5   \n",
       "4                  1.7                  1.7                  1.5   \n",
       "\n",
       "   ft_per_g_8_team_one  ft_per_g_9_team_one  ft_per_g_10_team_one  \\\n",
       "0                  0.5                  0.3                   0.1   \n",
       "1                  0.5                  0.3                   0.1   \n",
       "2                  0.5                  0.3                   0.1   \n",
       "3                  0.5                  0.3                   0.1   \n",
       "4                  0.5                  0.3                   0.1   \n",
       "\n",
       "   ft_per_g_11_team_one  ft_per_g_12_team_one  ft_per_g_13_team_one  \\\n",
       "0                   0.1                   0.0                   0.0   \n",
       "1                   0.1                   0.0                   0.0   \n",
       "2                   0.1                   0.0                   0.0   \n",
       "3                   0.1                   0.0                   0.0   \n",
       "4                   0.1                   0.0                   0.0   \n",
       "\n",
       "   fta_per_g_1_team_one  fta_per_g_2_team_one  fta_per_g_3_team_one  \\\n",
       "0                   6.7                   4.4                   4.1   \n",
       "1                   6.7                   4.4                   4.1   \n",
       "2                   6.7                   4.4                   4.1   \n",
       "3                   6.7                   4.4                   4.1   \n",
       "4                   6.7                   4.4                   4.1   \n",
       "\n",
       "   fta_per_g_4_team_one  fta_per_g_5_team_one  fta_per_g_6_team_one  \\\n",
       "0                   3.6                   2.1                   2.0   \n",
       "1                   3.6                   2.1                   2.0   \n",
       "2                   3.6                   2.1                   2.0   \n",
       "3                   3.6                   2.1                   2.0   \n",
       "4                   3.6                   2.1                   2.0   \n",
       "\n",
       "   fta_per_g_7_team_one  fta_per_g_8_team_one  fta_per_g_9_team_one  \\\n",
       "0                   1.8                   0.9                   0.4   \n",
       "1                   1.8                   0.9                   0.4   \n",
       "2                   1.8                   0.9                   0.4   \n",
       "3                   1.8                   0.9                   0.4   \n",
       "4                   1.8                   0.9                   0.4   \n",
       "\n",
       "   fta_per_g_10_team_one  fta_per_g_11_team_one  fta_per_g_12_team_one  \\\n",
       "0                    0.3                    0.2                    0.1   \n",
       "1                    0.3                    0.2                    0.1   \n",
       "2                    0.3                    0.2                    0.1   \n",
       "3                    0.3                    0.2                    0.1   \n",
       "4                    0.3                    0.2                    0.1   \n",
       "\n",
       "   fta_per_g_13_team_one  height_1_team_one  height_2_team_one  \\\n",
       "0                    0.0               82.0               82.0   \n",
       "1                    0.0               82.0               82.0   \n",
       "2                    0.0               82.0               82.0   \n",
       "3                    0.0               82.0               82.0   \n",
       "4                    0.0               82.0               82.0   \n",
       "\n",
       "   height_3_team_one  height_4_team_one  ...  ft_pct_4_team_two  \\\n",
       "0               81.0               80.0  ...              0.786   \n",
       "1               81.0               80.0  ...              0.756   \n",
       "2               81.0               80.0  ...              0.738   \n",
       "3               81.0               80.0  ...              0.779   \n",
       "4               81.0               80.0  ...              0.787   \n",
       "\n",
       "   ft_pct_5_team_two  ft_pct_6_team_two  ft_pct_7_team_two  ft_pct_8_team_two  \\\n",
       "0              0.780              0.754              0.746              0.629   \n",
       "1              0.714              0.670              0.667              0.660   \n",
       "2              0.706              0.703              0.698              0.696   \n",
       "3              0.750              0.726              0.667              0.650   \n",
       "4              0.755              0.738              0.699              0.680   \n",
       "\n",
       "   ft_pct_9_team_two  ft_pct_10_team_two  ft_pct_11_team_two  \\\n",
       "0              0.605               0.565               0.525   \n",
       "1              0.600               0.500               0.000   \n",
       "2              0.670               0.667               0.605   \n",
       "3              0.620               0.579               0.533   \n",
       "4              0.667               0.667               0.579   \n",
       "\n",
       "   ft_pct_12_team_two  ft_pct_13_team_two  ft_per_g_1_team_two  \\\n",
       "0               0.000               0.000                  4.3   \n",
       "1               0.000               0.000                  3.2   \n",
       "2               0.556               0.548                  2.6   \n",
       "3               0.000               0.000                  4.6   \n",
       "4               0.575               0.500                  4.2   \n",
       "\n",
       "   ft_per_g_2_team_two  ft_per_g_3_team_two  ft_per_g_4_team_two  \\\n",
       "0                  2.7                  2.5                  1.7   \n",
       "1                  2.9                  2.7                  2.3   \n",
       "2                  1.9                  1.9                  1.7   \n",
       "3                  2.5                  2.2                  1.9   \n",
       "4                  2.3                  2.3                  2.1   \n",
       "\n",
       "   ft_per_g_5_team_two  ft_per_g_6_team_two  ft_per_g_7_team_two  \\\n",
       "0                  1.7                  1.3                  0.7   \n",
       "1                  2.2                  0.8                  0.5   \n",
       "2                  1.5                  1.5                  1.1   \n",
       "3                  1.8                  1.7                  0.5   \n",
       "4                  1.9                  1.9                  1.3   \n",
       "\n",
       "   ft_per_g_8_team_two  ft_per_g_9_team_two  ft_per_g_10_team_two  \\\n",
       "0                  0.6                  0.5                   0.5   \n",
       "1                  0.4                  0.2                   0.1   \n",
       "2                  0.8                  0.6                   0.5   \n",
       "3                  0.5                  0.5                   0.3   \n",
       "4                  0.5                  0.4                   0.4   \n",
       "\n",
       "   ft_per_g_11_team_two  ft_per_g_12_team_two  ft_per_g_13_team_two  \\\n",
       "0                   0.4                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.4                   0.3                   0.3   \n",
       "3                   0.3                   0.0                   0.0   \n",
       "4                   0.1                   0.1                   0.1   \n",
       "\n",
       "   fta_per_g_1_team_two  fta_per_g_2_team_two  fta_per_g_3_team_two  \\\n",
       "0                   5.2                   3.3                   3.2   \n",
       "1                   4.1                   3.8                   3.5   \n",
       "2                   3.7                   2.8                   2.7   \n",
       "3                   5.9                   3.8                   3.4   \n",
       "4                   5.5                   3.9                   3.1   \n",
       "\n",
       "   fta_per_g_4_team_two  fta_per_g_5_team_two  fta_per_g_6_team_two  \\\n",
       "0                   2.7                   2.3                   1.7   \n",
       "1                   3.5                   3.3                   1.3   \n",
       "2                   2.5                   2.5                   2.0   \n",
       "3                   2.9                   2.7                   2.3   \n",
       "4                   2.8                   2.6                   2.4   \n",
       "\n",
       "   fta_per_g_7_team_two  fta_per_g_8_team_two  fta_per_g_9_team_two  \\\n",
       "0                   1.2                   1.2                   0.9   \n",
       "1                   1.0                   0.7                   0.3   \n",
       "2                   2.0                   1.1                   1.0   \n",
       "3                   0.8                   0.6                   0.6   \n",
       "4                   1.6                   0.7                   0.6   \n",
       "\n",
       "   fta_per_g_10_team_two  fta_per_g_11_team_two  fta_per_g_12_team_two  \\\n",
       "0                    0.6                    0.5                    0.0   \n",
       "1                    0.1                    0.0                    0.0   \n",
       "2                    0.7                    0.5                    0.4   \n",
       "3                    0.5                    0.4                    0.2   \n",
       "4                    0.6                    0.2                    0.1   \n",
       "\n",
       "   fta_per_g_13_team_two  height_1_team_two  height_2_team_two  \\\n",
       "0                    0.0               82.0               82.0   \n",
       "1                    0.0               83.0               80.0   \n",
       "2                    0.4               83.0               82.0   \n",
       "3                    0.0               84.0               82.0   \n",
       "4                    0.1               83.0               82.0   \n",
       "\n",
       "   height_3_team_two  height_4_team_two  height_5_team_two  height_6_team_two  \\\n",
       "0               81.0               81.0               81.0               81.0   \n",
       "1               80.0               78.0               77.0               77.0   \n",
       "2               81.0               81.0               80.0               78.0   \n",
       "3               80.0               80.0               79.0               79.0   \n",
       "4               81.0               81.0               81.0               78.0   \n",
       "\n",
       "   height_7_team_two  height_8_team_two  height_9_team_two  \\\n",
       "0               80.0               79.0               78.0   \n",
       "1               77.0               77.0               77.0   \n",
       "2               78.0               77.0               77.0   \n",
       "3               77.0               77.0               77.0   \n",
       "4               77.0               76.0               75.0   \n",
       "\n",
       "   height_10_team_two  height_11_team_two  height_12_team_two  \\\n",
       "0                78.0                76.0                75.0   \n",
       "1                76.0                75.0                75.0   \n",
       "2                76.0                75.0                75.0   \n",
       "3                76.0                75.0                74.0   \n",
       "4                75.0                75.0                73.0   \n",
       "\n",
       "   height_13_team_two  mp_per_g_1_team_two  mp_per_g_2_team_two  \\\n",
       "0                75.0                 35.1                 27.1   \n",
       "1                74.0                 34.2                 32.5   \n",
       "2                74.0                 33.3                 27.4   \n",
       "3                73.0                 36.0                 31.4   \n",
       "4                73.0                 31.4                 30.9   \n",
       "\n",
       "   mp_per_g_3_team_two  mp_per_g_4_team_two  mp_per_g_5_team_two  \\\n",
       "0                 26.9                 23.7                 21.1   \n",
       "1                 32.3                 30.1                 28.2   \n",
       "2                 27.1                 25.9                 17.2   \n",
       "3                 30.0                 28.7                 27.7   \n",
       "4                 30.2                 27.5                 26.8   \n",
       "\n",
       "   mp_per_g_6_team_two  mp_per_g_7_team_two  mp_per_g_8_team_two  \\\n",
       "0                 19.9                 19.4                 19.0   \n",
       "1                 23.8                 14.6                  6.4   \n",
       "2                 16.8                 16.7                 14.1   \n",
       "3                 24.8                  8.5                  8.0   \n",
       "4                 20.9                 15.3                  7.0   \n",
       "\n",
       "   mp_per_g_9_team_two  mp_per_g_10_team_two  mp_per_g_11_team_two  \\\n",
       "0                 11.4                  11.2                   9.7   \n",
       "1                  2.6                   0.8                   0.5   \n",
       "2                 13.9                  13.1                  12.7   \n",
       "3                  7.2                   4.4                   2.8   \n",
       "4                  4.2                   3.9                   3.3   \n",
       "\n",
       "   mp_per_g_12_team_two  mp_per_g_13_team_two  orb_per_g_1_team_two  \\\n",
       "0                   1.9                   1.3                   1.8   \n",
       "1                   0.5                   0.5                   2.3   \n",
       "2                   5.3                   4.3                   2.1   \n",
       "3                   2.0                   1.5                   3.2   \n",
       "4                   2.3                   2.2                   3.4   \n",
       "\n",
       "   orb_per_g_2_team_two  orb_per_g_3_team_two  orb_per_g_4_team_two  \\\n",
       "0                   1.6                   1.4                   1.0   \n",
       "1                   1.4                   1.3                   1.0   \n",
       "2                   2.1                   2.0                   1.9   \n",
       "3                   2.6                   2.3                   0.7   \n",
       "4                   3.4                   2.0                   1.4   \n",
       "\n",
       "   orb_per_g_5_team_two  orb_per_g_6_team_two  orb_per_g_7_team_two  \\\n",
       "0                   1.0                   0.9                   0.7   \n",
       "1                   0.8                   0.6                   0.6   \n",
       "2                   1.7                   1.4                   1.3   \n",
       "3                   0.7                   0.6                   0.6   \n",
       "4                   1.2                   0.7                   0.7   \n",
       "\n",
       "   orb_per_g_8_team_two  orb_per_g_9_team_two  orb_per_g_10_team_two  \\\n",
       "0                   0.6                   0.6                    0.5   \n",
       "1                   0.4                   0.1                    0.0   \n",
       "2                   1.1                   0.9                    0.3   \n",
       "3                   0.5                   0.4                    0.3   \n",
       "4                   0.6                   0.5                    0.3   \n",
       "\n",
       "   orb_per_g_11_team_two  orb_per_g_12_team_two  orb_per_g_13_team_two  \\\n",
       "0                    0.5                    0.2                    0.1   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.2                    0.2                    0.1   \n",
       "3                    0.2                    0.2                    0.1   \n",
       "4                    0.3                    0.1                    0.0   \n",
       "\n",
       "   pf_per_g_1_team_two  pf_per_g_2_team_two  pf_per_g_3_team_two  \\\n",
       "0                  3.1                  3.0                  2.6   \n",
       "1                  2.8                  2.4                  2.4   \n",
       "2                  2.8                  2.3                  2.2   \n",
       "3                  2.8                  2.6                  2.6   \n",
       "4                  2.9                  2.6                  2.5   \n",
       "\n",
       "   pf_per_g_4_team_two  pf_per_g_5_team_two  pf_per_g_6_team_two  \\\n",
       "0                  2.5                  2.5                  1.9   \n",
       "1                  2.3                  2.1                  2.1   \n",
       "2                  2.1                  1.9                  1.9   \n",
       "3                  2.4                  2.3                  2.2   \n",
       "4                  2.1                  2.0                  1.8   \n",
       "\n",
       "   pf_per_g_7_team_two  pf_per_g_8_team_two  pf_per_g_9_team_two  \\\n",
       "0                  1.8                  1.6                  1.6   \n",
       "1                  2.1                  0.5                  0.5   \n",
       "2                  1.8                  1.6                  1.5   \n",
       "3                  1.0                  1.0                  1.0   \n",
       "4                  1.8                  1.0                  0.6   \n",
       "\n",
       "   pf_per_g_10_team_two  pf_per_g_11_team_two  pf_per_g_12_team_two  \\\n",
       "0                   1.5                   1.0                   0.5   \n",
       "1                   0.2                   0.0                   0.0   \n",
       "2                   0.9                   0.9                   0.5   \n",
       "3                   0.8                   0.6                   0.3   \n",
       "4                   0.6                   0.5                   0.3   \n",
       "\n",
       "   pf_per_g_13_team_two  pts_per_g_1_team_two  pts_per_g_2_team_two  \\\n",
       "0                   0.4                  19.9                  11.1   \n",
       "1                   0.0                  15.1                  12.3   \n",
       "2                   0.4                  17.5                  11.6   \n",
       "3                   0.3                  20.3                  12.6   \n",
       "4                   0.2                  19.8                  15.6   \n",
       "\n",
       "   pts_per_g_3_team_two  pts_per_g_4_team_two  pts_per_g_5_team_two  \\\n",
       "0                   8.8                   6.9                   6.4   \n",
       "1                  10.2                   9.3                   8.1   \n",
       "2                   8.9                   8.9                   7.1   \n",
       "3                  11.4                  11.3                   9.2   \n",
       "4                  14.8                  13.4                   8.1   \n",
       "\n",
       "   pts_per_g_6_team_two  pts_per_g_7_team_two  pts_per_g_8_team_two  \\\n",
       "0                   6.3                   5.3                   4.6   \n",
       "1                   7.8                   3.0                   1.0   \n",
       "2                   6.1                   5.9                   5.5   \n",
       "3                   7.9                   2.4                   1.6   \n",
       "4                   7.9                   7.1                   1.9   \n",
       "\n",
       "   pts_per_g_9_team_two  pts_per_g_10_team_two  pts_per_g_11_team_two  \\\n",
       "0                   2.2                    2.0                    1.6   \n",
       "1                   0.9                    0.9                    0.5   \n",
       "2                   5.3                    3.7                    2.1   \n",
       "3                   1.2                    1.2                    1.0   \n",
       "4                   1.2                    1.0                    1.0   \n",
       "\n",
       "   pts_per_g_12_team_two  pts_per_g_13_team_two  stl_per_g_1_team_two  \\\n",
       "0                    0.6                    0.3                   1.2   \n",
       "1                    0.0                    0.0                   1.3   \n",
       "2                    1.1                    0.8                   1.6   \n",
       "3                    0.9                    0.3                   2.1   \n",
       "4                    0.8                    0.7                   1.8   \n",
       "\n",
       "   stl_per_g_2_team_two  stl_per_g_3_team_two  stl_per_g_4_team_two  \\\n",
       "0                   0.9                   0.6                   0.6   \n",
       "1                   1.3                   0.9                   0.7   \n",
       "2                   1.5                   1.1                   1.0   \n",
       "3                   1.3                   1.2                   1.0   \n",
       "4                   1.6                   1.6                   1.3   \n",
       "\n",
       "   stl_per_g_5_team_two  stl_per_g_6_team_two  stl_per_g_7_team_two  \\\n",
       "0                   0.5                   0.5                   0.5   \n",
       "1                   0.6                   0.6                   0.3   \n",
       "2                   0.8                   0.6                   0.6   \n",
       "3                   0.8                   0.6                   0.2   \n",
       "4                   1.1                   0.9                   0.6   \n",
       "\n",
       "   stl_per_g_8_team_two  stl_per_g_9_team_two  stl_per_g_10_team_two  \\\n",
       "0                   0.5                   0.4                    0.2   \n",
       "1                   0.2                   0.1                    0.0   \n",
       "2                   0.5                   0.4                    0.3   \n",
       "3                   0.2                   0.2                    0.1   \n",
       "4                   0.3                   0.3                    0.2   \n",
       "\n",
       "   stl_per_g_11_team_two  stl_per_g_12_team_two  stl_per_g_13_team_two  \\\n",
       "0                    0.1                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.3                    0.2                    0.1   \n",
       "3                    0.1                    0.1                    0.0   \n",
       "4                    0.1                    0.1                    0.1   \n",
       "\n",
       "   tov_per_g_1_team_two  tov_per_g_2_team_two  tov_per_g_3_team_two  \\\n",
       "0                   3.1                   2.7                   2.5   \n",
       "1                   2.5                   2.1                   1.9   \n",
       "2                   3.0                   2.0                   1.8   \n",
       "3                   3.3                   2.9                   2.4   \n",
       "4                   3.0                   2.9                   2.7   \n",
       "\n",
       "   tov_per_g_4_team_two  tov_per_g_5_team_two  tov_per_g_6_team_two  \\\n",
       "0                   1.5                   1.4                   1.3   \n",
       "1                   1.7                   1.6                   1.4   \n",
       "2                   1.6                   1.5                   1.3   \n",
       "3                   2.4                   1.7                   1.4   \n",
       "4                   2.5                   1.6                   1.1   \n",
       "\n",
       "   tov_per_g_7_team_two  tov_per_g_8_team_two  tov_per_g_9_team_two  \\\n",
       "0                   1.1                   0.8                   0.7   \n",
       "1                   1.2                   0.4                   0.3   \n",
       "2                   1.1                   0.9                   0.8   \n",
       "3                   0.6                   0.6                   0.4   \n",
       "4                   1.0                   0.4                   0.4   \n",
       "\n",
       "   tov_per_g_10_team_two  tov_per_g_11_team_two  tov_per_g_12_team_two  \\\n",
       "0                    0.7                    0.7                    0.2   \n",
       "1                    0.2                    0.0                    0.0   \n",
       "2                    0.7                    0.6                    0.6   \n",
       "3                    0.3                    0.2                    0.2   \n",
       "4                    0.4                    0.3                    0.2   \n",
       "\n",
       "   tov_per_g_13_team_two  trb_per_g_1_team_two  trb_per_g_2_team_two  \\\n",
       "0                    0.2                   7.2                   5.0   \n",
       "1                    0.0                   5.3                   5.2   \n",
       "2                    0.3                   6.3                   5.4   \n",
       "3                    0.0                   9.0                   7.5   \n",
       "4                    0.2                  11.4                   8.3   \n",
       "\n",
       "   trb_per_g_3_team_two  trb_per_g_4_team_two  trb_per_g_5_team_two  \\\n",
       "0                   5.0                   3.7                   3.2   \n",
       "1                   4.9                   4.3                   3.3   \n",
       "2                   4.5                   4.3                   4.3   \n",
       "3                   6.0                   3.3                   2.7   \n",
       "4                   5.3                   4.8                   3.3   \n",
       "\n",
       "   trb_per_g_6_team_two  trb_per_g_7_team_two  trb_per_g_8_team_two  \\\n",
       "0                   2.7                   2.3                   2.2   \n",
       "1                   2.7                   2.6                   0.9   \n",
       "2                   4.0                   4.0                   2.9   \n",
       "3                   2.3                   1.9                   1.8   \n",
       "4                   2.7                   2.6                   1.6   \n",
       "\n",
       "   trb_per_g_9_team_two  trb_per_g_10_team_two  trb_per_g_11_team_two  \\\n",
       "0                   2.2                    1.9                    1.5   \n",
       "1                   0.3                    0.0                    0.0   \n",
       "2                   2.6                    1.1                    1.0   \n",
       "3                   1.5                    0.8                    0.5   \n",
       "4                   0.9                    0.7                    0.6   \n",
       "\n",
       "   trb_per_g_12_team_two  trb_per_g_13_team_two  weight_1_team_two  \\\n",
       "0                    0.4                    0.3              275.0   \n",
       "1                    0.0                    0.0              265.0   \n",
       "2                    0.7                    0.6              240.0   \n",
       "3                    0.4                    0.4              252.0   \n",
       "4                    0.5                    0.4              255.0   \n",
       "\n",
       "   weight_2_team_two  weight_3_team_two  weight_4_team_two  weight_5_team_two  \\\n",
       "0              235.0              230.0              215.0              210.0   \n",
       "1              240.0              230.0              220.0              208.0   \n",
       "2              240.0              236.0              215.0              215.0   \n",
       "3              245.0              235.0              230.0              225.0   \n",
       "4              255.0              250.0              235.0              230.0   \n",
       "\n",
       "   weight_6_team_two  weight_7_team_two  weight_8_team_two  weight_9_team_two  \\\n",
       "0              205.0              205.0              200.0              200.0   \n",
       "1              205.0              200.0              195.0              195.0   \n",
       "2              214.0              211.0              205.0              195.0   \n",
       "3              220.0              220.0              217.0              215.0   \n",
       "4              230.0              215.0              215.0              205.0   \n",
       "\n",
       "   weight_10_team_two  weight_11_team_two  weight_12_team_two  \\\n",
       "0               190.0               190.0               185.0   \n",
       "1               190.0               190.0               190.0   \n",
       "2               194.0               193.0               188.0   \n",
       "3               215.0               200.0               200.0   \n",
       "4               190.0               185.0               175.0   \n",
       "\n",
       "   weight_13_team_two  num_C_team_two  ast_per_g_14_team_two  \\\n",
       "0               185.0             3.0                    0.0   \n",
       "1               185.0             1.0                    0.0   \n",
       "2               187.0             2.0                    0.2   \n",
       "3               195.0             2.0                    0.0   \n",
       "4               175.0             0.0                    0.1   \n",
       "\n",
       "   blk_per_g_14_team_two  drb_per_g_14_team_two  fg2_pct_14_team_two  \\\n",
       "0                    0.0                    0.0                0.273   \n",
       "1                    0.0                    0.0                0.000   \n",
       "2                    0.0                    0.3                0.300   \n",
       "3                    0.0                    0.2                0.000   \n",
       "4                    0.0                    0.0                0.286   \n",
       "\n",
       "   fg2_per_g_14_team_two  fg2a_per_g_14_team_two  fg3_pct_14_team_two  \\\n",
       "0                    0.1                     0.3                  0.0   \n",
       "1                    0.0                     0.0                  0.0   \n",
       "2                    0.1                     0.4                  0.0   \n",
       "3                    0.0                     0.3                  0.0   \n",
       "4                    0.1                     0.3                  0.0   \n",
       "\n",
       "   fg3_per_g_14_team_two  fg3a_per_g_14_team_two  fg_pct_14_team_two  \\\n",
       "0                    0.0                     0.0               0.250   \n",
       "1                    0.0                     0.0               0.000   \n",
       "2                    0.0                     0.2               0.238   \n",
       "3                    0.0                     0.0               0.000   \n",
       "4                    0.0                     0.0               0.200   \n",
       "\n",
       "   fg_per_g_14_team_two  fga_per_g_14_team_two  ft_pct_14_team_two  \\\n",
       "0                   0.1                    0.3                 0.0   \n",
       "1                   0.0                    0.0                 0.0   \n",
       "2                   0.3                    0.8                 0.5   \n",
       "3                   0.0                    0.4                 0.0   \n",
       "4                   0.1                    0.5                 0.0   \n",
       "\n",
       "   ft_per_g_14_team_two  fta_per_g_14_team_two  height_14_team_two  \\\n",
       "0                   0.0                    0.0                73.0   \n",
       "1                   0.0                    0.0                70.0   \n",
       "2                   0.3                    0.3                73.0   \n",
       "3                   0.0                    0.0                72.0   \n",
       "4                   0.0                    0.0                72.0   \n",
       "\n",
       "   mp_per_g_14_team_two  orb_per_g_14_team_two  pf_per_g_14_team_two  \\\n",
       "0                   0.6                    0.1                   0.2   \n",
       "1                   0.0                    0.0                   0.0   \n",
       "2                   2.3                    0.1                   0.1   \n",
       "3                   1.4                    0.0                   0.0   \n",
       "4                   2.2                    0.0                   0.1   \n",
       "\n",
       "   pts_per_g_14_team_two  stl_per_g_14_team_two  tov_per_g_14_team_two  \\\n",
       "0                    0.3                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.8                    0.0                    0.3   \n",
       "3                    0.0                    0.0                    0.0   \n",
       "4                    0.2                    0.0                    0.1   \n",
       "\n",
       "   trb_per_g_14_team_two  weight_14_team_two  ast_per_g_15_team_two  \\\n",
       "0                    0.1               180.0                    0.0   \n",
       "1                    0.0               180.0                    0.0   \n",
       "2                    0.5               185.0                    0.1   \n",
       "3                    0.3               190.0                    0.0   \n",
       "4                    0.2               165.0                    0.0   \n",
       "\n",
       "   ast_per_g_16_team_two  blk_per_g_15_team_two  blk_per_g_16_team_two  \\\n",
       "0                    0.0                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.0                    0.0                    0.0   \n",
       "3                    0.0                    0.0                    0.0   \n",
       "4                    0.0                    0.0                    0.0   \n",
       "\n",
       "   drb_per_g_15_team_two  drb_per_g_16_team_two  fg2_pct_15_team_two  \\\n",
       "0                    0.0                    0.0                0.000   \n",
       "1                    0.0                    0.0                0.000   \n",
       "2                    0.1                    0.0                0.167   \n",
       "3                    0.2                    0.0                0.000   \n",
       "4                    0.0                    0.0                0.000   \n",
       "\n",
       "   fg2_pct_16_team_two  fg2_per_g_15_team_two  fg2_per_g_16_team_two  \\\n",
       "0                  0.0                    0.0                    0.0   \n",
       "1                  0.0                    0.0                    0.0   \n",
       "2                  0.0                    0.1                    0.0   \n",
       "3                  0.0                    0.0                    0.0   \n",
       "4                  0.0                    0.0                    0.0   \n",
       "\n",
       "   fg2a_per_g_15_team_two  fg2a_per_g_16_team_two  fg3_pct_15_team_two  \\\n",
       "0                     0.0                     0.0                  0.0   \n",
       "1                     0.0                     0.0                  0.0   \n",
       "2                     0.1                     0.0                  0.0   \n",
       "3                     0.3                     0.0                  0.0   \n",
       "4                     0.0                     0.0                  0.0   \n",
       "\n",
       "   fg3_pct_16_team_two  fg3_per_g_15_team_two  fg3_per_g_16_team_two  \\\n",
       "0                  0.0                    0.0                    0.0   \n",
       "1                  0.0                    0.0                    0.0   \n",
       "2                  0.0                    0.0                    0.0   \n",
       "3                  0.0                    0.0                    0.0   \n",
       "4                  0.0                    0.0                    0.0   \n",
       "\n",
       "   fg3a_per_g_15_team_two  fg3a_per_g_16_team_two  fg_pct_15_team_two  \\\n",
       "0                     0.0                     0.0               0.000   \n",
       "1                     0.0                     0.0               0.000   \n",
       "2                     0.0                     0.0               0.211   \n",
       "3                     0.0                     0.0               0.000   \n",
       "4                     0.0                     0.0               0.000   \n",
       "\n",
       "   fg_pct_16_team_two  fg_per_g_15_team_two  fg_per_g_16_team_two  \\\n",
       "0                 0.0                   0.0                   0.0   \n",
       "1                 0.0                   0.0                   0.0   \n",
       "2                 0.0                   0.2                   0.0   \n",
       "3                 0.0                   0.0                   0.0   \n",
       "4                 0.0                   0.0                   0.0   \n",
       "\n",
       "   fga_per_g_15_team_two  fga_per_g_16_team_two  ft_pct_15_team_two  \\\n",
       "0                    0.0                    0.0                 0.0   \n",
       "1                    0.0                    0.0                 0.0   \n",
       "2                    0.8                    0.0                 0.4   \n",
       "3                    0.3                    0.3                 0.0   \n",
       "4                    0.0                    0.0                 0.0   \n",
       "\n",
       "   ft_pct_16_team_two  ft_per_g_15_team_two  ft_per_g_16_team_two  \\\n",
       "0                 0.0                   0.0                   0.0   \n",
       "1                 0.0                   0.0                   0.0   \n",
       "2                 0.0                   0.1                   0.0   \n",
       "3                 0.0                   0.0                   0.0   \n",
       "4                 0.0                   0.0                   0.0   \n",
       "\n",
       "   fta_per_g_15_team_two  fta_per_g_16_team_two height_15_team_two  \\\n",
       "0                    0.0                    0.0                0.0   \n",
       "1                    0.0                    0.0                0.0   \n",
       "2                    0.3                    0.0               72.0   \n",
       "3                    0.0                    0.0                0.0   \n",
       "4                    0.0                    0.0                0.0   \n",
       "\n",
       "   height_16_team_two  mp_per_g_15_team_two  mp_per_g_16_team_two  \\\n",
       "0                 0.0                   0.0                   0.0   \n",
       "1                 0.0                   0.0                   0.0   \n",
       "2                 0.0                   2.1                   0.0   \n",
       "3                 0.0                   1.3                   1.3   \n",
       "4                 0.0                   0.0                   0.0   \n",
       "\n",
       "   orb_per_g_15_team_two  orb_per_g_16_team_two  pf_per_g_15_team_two  \\\n",
       "0                    0.0                    0.0                   0.0   \n",
       "1                    0.0                    0.0                   0.0   \n",
       "2                    0.1                    0.0                   0.1   \n",
       "3                    0.0                    0.0                   0.0   \n",
       "4                    0.0                    0.0                   0.0   \n",
       "\n",
       "   pf_per_g_16_team_two  pts_per_g_15_team_two  pts_per_g_16_team_two  \\\n",
       "0                   0.0                    0.0                    0.0   \n",
       "1                   0.0                    0.0                    0.0   \n",
       "2                   0.0                    0.7                    0.0   \n",
       "3                   0.0                    0.0                    0.0   \n",
       "4                   0.0                    0.0                    0.0   \n",
       "\n",
       "   stl_per_g_15_team_two  stl_per_g_16_team_two  tov_per_g_15_team_two  \\\n",
       "0                    0.0                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.0                    0.0                    0.3   \n",
       "3                    0.0                    0.0                    0.0   \n",
       "4                    0.0                    0.0                    0.0   \n",
       "\n",
       "   tov_per_g_16_team_two  trb_per_g_15_team_two  trb_per_g_16_team_two  \\\n",
       "0                    0.0                    0.0                    0.0   \n",
       "1                    0.0                    0.0                    0.0   \n",
       "2                    0.0                    0.3                    0.0   \n",
       "3                    0.0                    0.3                    0.0   \n",
       "4                    0.0                    0.0                    0.0   \n",
       "\n",
       "   weight_15_team_two  weight_16_team_two  year_team_two  week1_team_two  \\\n",
       "0                 0.0                 0.0           2002             0.0   \n",
       "1                 0.0                 0.0           2002             0.0   \n",
       "2               180.0                 0.0           2002             4.0   \n",
       "3               175.0               150.0           2002             0.0   \n",
       "4                 0.0                 0.0           2002             7.0   \n",
       "\n",
       "   week2_team_two  week3_team_two  week4_team_two  week5_team_two  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2            10.0            13.0            11.0             9.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             4.0             8.0             4.0             4.0   \n",
       "\n",
       "   week6_team_two  week7_team_two  week8_team_two  week9_team_two  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             7.0             6.0             6.0             8.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             3.0             2.0             2.0             1.0   \n",
       "\n",
       "   week10_team_two  week11_team_two  week12_team_two  week13_team_two  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2             12.0              8.0             10.0              7.0   \n",
       "3              0.0             25.0             17.0              0.0   \n",
       "4              4.0              2.0              2.0              2.0   \n",
       "\n",
       "   week14_team_two  week15_team_two  week16_team_two  week17_team_two  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2             10.0             12.0             11.0             12.0   \n",
       "3              0.0              0.0             23.0             19.0   \n",
       "4              2.0              1.0              1.0              1.0   \n",
       "\n",
       "   week18_team_two  max_team_two  final_team_two  result  score_one  score_two  \n",
       "0              0.0           0.0             0.0       1         85         70  \n",
       "1              0.0           0.0             0.0       1         87         57  \n",
       "2             16.0           4.0            16.0       1         78         68  \n",
       "3             10.0          10.0            10.0       1         90         82  \n",
       "4              2.0           1.0             2.0       1         97         88  \n",
       "\n",
       "[5 rows x 917 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(data['result'])\n",
    "labels = np.reshape(labels, newshape=(labels.shape[0], 1))\n",
    "scores = data[['score_one', 'score_two']].to_numpy()\n",
    "labels_invert = np.array(data_invert['result'])\n",
    "labels_invert = np.reshape(labels_invert, newshape=(labels_invert.shape[0], 1))\n",
    "scores_invert = data_invert[['score_one', 'score_two']].to_numpy()\n",
    "\n",
    "data = data.drop(columns=['result', 'score_one', 'score_two'])\n",
    "data_invert = data_invert.drop(columns=['result', 'score_one', 'score_two'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Team_team_one', 'Team_team_two', 'year_team_one', 'year_team_two']\n",
    "\n",
    "cols_not = [col for col in data.columns if col not in cols_to_drop]\n",
    "\n",
    "data = data[cols_to_drop + cols_not]\n",
    "data_invert = data_invert[cols_to_drop + cols_not]\n",
    "\n",
    "data_np = data.drop(columns=cols_to_drop).to_numpy()\n",
    "\n",
    "data_invert_np = data_invert.drop(columns=cols_to_drop).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = data_np.shape[1]\n",
    "num_neurons = num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=(None, num_features))\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "\n",
    "b_hidden = tf.Variable(tf.zeros([num_neurons]))\n",
    "W_hidden = tf.Variable(tf.random_normal([num_features, num_neurons]))\n",
    "\n",
    "z_hidden = tf.add(tf.matmul(x, W_hidden), b_hidden)\n",
    "a_hidden = tf.nn.relu(z_hidden) # shape is (910)\n",
    "\n",
    "b_out = tf.Variable(tf.zeros([1]))\n",
    "W_out = tf.Variable(tf.random_normal([num_features, 1]))\n",
    "\n",
    "z_out = tf.add(tf.matmul(a_hidden, W_out), b_out)\n",
    "# a_out = tf.sigmoid(z_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 3000\n",
    "learning_rate = 0.01\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=z_out)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "predicted = tf.nn.sigmoid(z_out)\n",
    "correct_pred = tf.equal(tf.round(predicted), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:     0\tLoss: 84470.352\tAcc: 29.59%\n",
      "Step:   500\tLoss: 96.448\tAcc: 89.84%\n",
      "Step:  1000\tLoss: 19.164\tAcc: 95.95%\n",
      "Step:  1500\tLoss: 8.098\tAcc: 97.57%\n",
      "Step:  2000\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  2500\tLoss: 0.000\tAcc: 100.00%\n",
      "Step:  3000\tLoss: 0.000\tAcc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(training_epochs + 1):\n",
    "        sess.run(optimizer, feed_dict={x: data_np, y: labels})\n",
    "        loss, _, acc = sess.run([cost, optimizer, accuracy], feed_dict={x: data_np, y: labels})\n",
    "        if step % 500 == 0:\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tensorflow Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_features, num_hidden_layers=3, learning_rate=0.001):\n",
    "    model = tf.keras.Sequential()\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(layers.Dense(units=num_features, activation='relu'))\n",
    "    \n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate),\n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['binary_crossentropy', 'accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_callbacks(scaler=True, split=True):\n",
    "    return [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss' if split else 'loss', patience=20 if scaler else 100)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def split(self, split=False):\n",
    "        if split:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(self.data, self.labels)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = self.data, None, self.labels, None\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def clone(self):\n",
    "        data_new = Data(self.data, self.labels)\n",
    "        if hasattr(self, 'X_train'):\n",
    "            data_new.X_train = self.X_train\n",
    "            data_new.X_test = self.X_test\n",
    "            data_new.y_train = self.y_train\n",
    "            data_new.y_test = self.y_test\n",
    "        if hasattr(self, 'scaler'):\n",
    "            data_new.scaler = self.scaler\n",
    "        if hasattr(self, 'model'):\n",
    "            data_new.model = self.model\n",
    "        return data_new\n",
    "    \n",
    "    def add_scaler(self, scaler):\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def fit_scaler(self):\n",
    "        if not self.scaler:\n",
    "            return\n",
    "        self.scaler = self.scaler.fit(self.X_train)\n",
    "        self.X_train = self.scaler.transform(self.X_train)\n",
    "        if self.X_test is not None:\n",
    "            self.X_test = self.scaler.transform(self.X_test)\n",
    "        self.data = self.scaler.transform(self.data)\n",
    "    \n",
    "    def add_model(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def fit_model(self, verbose=3):\n",
    "        if verbose == 0:\n",
    "            print (f'Fitting model {self.label}')\n",
    "        if self.scaler:\n",
    "            if self.X_test is None and self.y_test is None:\n",
    "                self.history = self.model.fit(x=self.X_train, \n",
    "                                              y=self.y_train, \n",
    "                                              batch_size=100, \n",
    "                                              callbacks=get_callbacks(True, False), \n",
    "                                              epochs=100, \n",
    "                                             verbose=verbose)\n",
    "            else:\n",
    "                self.history = self.model.fit(x=self.X_train, \n",
    "                                              y=self.y_train, \n",
    "                                              batch_size=100,\n",
    "                                              validation_data=(self.X_test, self.y_test), \n",
    "                                              callbacks=get_callbacks(True, True), \n",
    "                                              epochs=2000, \n",
    "                                             verbose=verbose)\n",
    "        else:\n",
    "            if self.X_test is None and self.y_test is None:\n",
    "                self.history = self.model.fit(x=self.X_train,\n",
    "                                             y=self.y_train, \n",
    "                                             batch_size=100,\n",
    "                                             callbacks=get_callbacks(False, False),\n",
    "                                             epochs=100, \n",
    "                                             verbose=verbose)\n",
    "            else:\n",
    "                self.history = self.model.fit(x=self.X_train, \n",
    "                                             y=self.y_train,\n",
    "                                             batch_size=100,\n",
    "                                             validation_data=(self.X_test, self.y_test),\n",
    "                                             callbacks=get_callbacks(False, True),\n",
    "                                             epochs=2000, \n",
    "                                             verbose=verbose)\n",
    "        if verbose == 0:\n",
    "            print ('Done!')\n",
    "        return self.history\n",
    "    \n",
    "    def predict_classes(self):\n",
    "        self.predictions = self.model.predict_classes(x=self.data)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.X_train.shape}, {self.X_test}, {self.y_train.shape}, {self.y_test}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tensorflow Possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obj = Data(data_np, labels)\n",
    "data_invert_obj = Data(data_invert_np, labels_invert)\n",
    "\n",
    "datas = [data_obj, data_invert_obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "possibilities = []\n",
    "for poss in datas:\n",
    "    for val in [True, False]:\n",
    "        possibilities.append(poss.clone().split(val))\n",
    "\n",
    "\n",
    "label_models = ['Normal Split', 'Normal Non-Split', 'Invert Split', 'Invert Non-Split']\n",
    "for poss, label in zip(possibilities, label_models):\n",
    "    poss.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "possibilities_min_max = []\n",
    "possibilities_standard = []\n",
    "\n",
    "for poss in possibilities:\n",
    "    poss.add_scaler(None)\n",
    "\n",
    "for poss in possibilities:\n",
    "    min_max_poss = poss.clone()\n",
    "    min_max_poss.add_scaler(MinMaxScaler())\n",
    "    min_max_poss.fit_scaler()\n",
    "    min_max_poss.label = poss.label + ' Min Max'\n",
    "    possibilities_min_max.append(min_max_poss)\n",
    "    \n",
    "    standard_poss = poss.clone()\n",
    "    standard_poss.add_scaler(StandardScaler())\n",
    "    standard_poss.fit_scaler()\n",
    "    standard_poss.label = poss.label + ' Standard'\n",
    "    possibilities_standard.append(standard_poss)\n",
    "\n",
    "\n",
    "possibilities = possibilities + possibilities_min_max + possibilities_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Models and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /mnt/c/Users/tas12/Documents/Projects/BracketProject/bracketenv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "for poss in possibilities:\n",
    "    poss.add_model(build_model(poss.X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for poss in possibilities:\n",
    "#     poss.fit_model(0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-5e3ce68a9622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_plots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_plots\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "num_plots = len(possibilities)\n",
    "fig = plt.figure(figsize=(10, 20))\n",
    "num_y = num_plots // 2\n",
    "\n",
    "for idx, poss in enumerate(possibilities):\n",
    "    history = poss.history\n",
    "    plt.subplot(num_y, 2, idx+1)\n",
    "    plt.plot(history.history['loss'], label='Loss')\n",
    "    plt.plot(history.history['acc'], label='Accuracy')\n",
    "    \n",
    "    if history.history.get('val_loss') is not None:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
    "    \n",
    "    plt.title(f'Training History ({poss.label})')\n",
    "    plt.legend()\n",
    "    plt.ylim((0, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TF Models by Observing Prediction for UVA-UMBC game and viewing Classification Reports and Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = data[(data['Team_team_one'] == 'Virginia') & (data['year_team_one'] == 2018)]\n",
    "row_unc = data[(data['Team_team_two'] == 'North Carolina') & (data['year_team_one'] == 2017) & (data['Team_team_one'] == 'Gonzaga')]\n",
    "\n",
    "row = row.drop(columns=cols_to_drop).to_numpy()\n",
    "row_unc = row_unc.drop(columns=cols_to_drop).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 910)\n",
      "(1, 910)\n",
      "(1112, 910)\n"
     ]
    }
   ],
   "source": [
    "print (row.shape)\n",
    "print (row_unc.shape)\n",
    "print (data_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'possibilities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-81238c1c46df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mposs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibilities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mposs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mposs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mposs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'possibilities' is not defined"
     ]
    }
   ],
   "source": [
    "for poss in possibilities:\n",
    "    prediction = poss.model.predict_proba(x=row if not poss.scaler else poss.scaler.transform(row))\n",
    "    print (poss.label)\n",
    "    print (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for poss in possibilities:\n",
    "    poss.predict_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Split\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.74      0.80       329\n",
      "           1       0.90      0.96      0.93       783\n",
      "\n",
      "    accuracy                           0.89      1112\n",
      "   macro avg       0.89      0.85      0.86      1112\n",
      "weighted avg       0.89      0.89      0.89      1112\n",
      "\n",
      "Normal Non-Split\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.67      0.71       329\n",
      "           1       0.87      0.91      0.89       783\n",
      "\n",
      "    accuracy                           0.84      1112\n",
      "   macro avg       0.81      0.79      0.80      1112\n",
      "weighted avg       0.83      0.84      0.83      1112\n",
      "\n",
      "Invert Split\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.74      0.83      1112\n",
      "           1       0.79      0.97      0.87      1112\n",
      "\n",
      "    accuracy                           0.85      2224\n",
      "   macro avg       0.87      0.85      0.85      2224\n",
      "weighted avg       0.87      0.85      0.85      2224\n",
      "\n",
      "Invert Non-Split\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.69      0.78      1112\n",
      "           1       0.75      0.94      0.83      1112\n",
      "\n",
      "    accuracy                           0.81      2224\n",
      "   macro avg       0.83      0.81      0.81      2224\n",
      "weighted avg       0.83      0.81      0.81      2224\n",
      "\n",
      "Normal Split Min Max\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.84       329\n",
      "           1       0.94      0.92      0.93       783\n",
      "\n",
      "    accuracy                           0.90      1112\n",
      "   macro avg       0.88      0.89      0.89      1112\n",
      "weighted avg       0.91      0.90      0.90      1112\n",
      "\n",
      "Normal Non-Split Min Max\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       329\n",
      "           1       1.00      1.00      1.00       783\n",
      "\n",
      "    accuracy                           1.00      1112\n",
      "   macro avg       1.00      1.00      1.00      1112\n",
      "weighted avg       1.00      1.00      1.00      1112\n",
      "\n",
      "Invert Split Min Max\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.79      0.87      1112\n",
      "           1       0.82      0.98      0.89      1112\n",
      "\n",
      "    accuracy                           0.88      2224\n",
      "   macro avg       0.89      0.88      0.88      2224\n",
      "weighted avg       0.89      0.88      0.88      2224\n",
      "\n",
      "Invert Non-Split Min Max\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      1112\n",
      "           1       0.99      0.95      0.97      1112\n",
      "\n",
      "    accuracy                           0.97      2224\n",
      "   macro avg       0.97      0.97      0.97      2224\n",
      "weighted avg       0.97      0.97      0.97      2224\n",
      "\n",
      "Normal Split Standard\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88       329\n",
      "           1       0.95      0.95      0.95       783\n",
      "\n",
      "    accuracy                           0.93      1112\n",
      "   macro avg       0.91      0.91      0.91      1112\n",
      "weighted avg       0.93      0.93      0.93      1112\n",
      "\n",
      "Normal Non-Split Standard\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       329\n",
      "           1       1.00      1.00      1.00       783\n",
      "\n",
      "    accuracy                           1.00      1112\n",
      "   macro avg       1.00      1.00      1.00      1112\n",
      "weighted avg       1.00      1.00      1.00      1112\n",
      "\n",
      "Invert Split Standard\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      1112\n",
      "           1       0.92      0.96      0.94      1112\n",
      "\n",
      "    accuracy                           0.94      2224\n",
      "   macro avg       0.94      0.94      0.94      2224\n",
      "weighted avg       0.94      0.94      0.94      2224\n",
      "\n",
      "Invert Non-Split Standard\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1112\n",
      "           1       1.00      1.00      1.00      1112\n",
      "\n",
      "    accuracy                           1.00      2224\n",
      "   macro avg       1.00      1.00      1.00      2224\n",
      "weighted avg       1.00      1.00      1.00      2224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for poss in possibilities:\n",
    "    print (poss.label)\n",
    "    print (classification_report(poss.labels, poss.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Split\n",
      "[[242  87]\n",
      " [ 34 749]]\n",
      "Normal Non-Split\n",
      "[[220 109]\n",
      " [ 73 710]]\n",
      "Invert Split\n",
      "[[ 820  292]\n",
      " [  36 1076]]\n",
      "Invert Non-Split\n",
      "[[ 762  350]\n",
      " [  68 1044]]\n",
      "Normal Split Min Max\n",
      "[[286  43]\n",
      " [ 64 719]]\n",
      "Normal Non-Split Min Max\n",
      "[[329   0]\n",
      " [  0 783]]\n",
      "Invert Split Min Max\n",
      "[[ 873  239]\n",
      " [  27 1085]]\n",
      "Invert Non-Split Min Max\n",
      "[[1101   11]\n",
      " [  55 1057]]\n",
      "Normal Split Standard\n",
      "[[287  42]\n",
      " [ 39 744]]\n",
      "Normal Non-Split Standard\n",
      "[[329   0]\n",
      " [  0 783]]\n",
      "Invert Split Standard\n",
      "[[1023   89]\n",
      " [  48 1064]]\n",
      "Invert Non-Split Standard\n",
      "[[1112    0]\n",
      " [   0 1112]]\n"
     ]
    }
   ],
   "source": [
    "for poss in possibilities:\n",
    "    print (poss.label)\n",
    "    print (confusion_matrix(poss.labels, poss.predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's examine some SKLearn Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn Algorithms\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, ElasticNet, Ridge\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1112,)\n"
     ]
    }
   ],
   "source": [
    "labels_sklearn = labels.flatten()\n",
    "labels_invert_sklearn = labels_invert.flatten()\n",
    "\n",
    "print (labels_sklearn.shape)\n",
    "\n",
    "sklearn_space = [(data_np, labels_sklearn, 'Data'), (data_invert_np, labels_invert_sklearn, 'Data Invert')]\n",
    "\n",
    "sklearn_score_space = [(data_np, scores, 'Data'), (data_invert_np, scores_invert, 'Data Invert')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "1.0000\n",
      "Data Standard\n",
      "1.0000\n",
      "Data Invert Min Max\n",
      "1.0000\n",
      "Data Invert Standard\n",
      "1.0000\n",
      "CPU times: user 3min 11s, sys: 24.5 s, total: 3min 36s\n",
      "Wall time: 10min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Random Forest\n",
    "param_space_random_forest = {\n",
    "    'rf__bootstrap' : [True, False],\n",
    "    'rf__n_estimators' : [100, 1000]\n",
    "}\n",
    "\n",
    "random_forests = []\n",
    "for poss in sklearn_space:\n",
    "    random_forest_min_max = RandomForestClassifier(n_jobs=-1)\n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()), ('rf', random_forest_min_max)])\n",
    "    \n",
    "    grid_search_forest = GridSearchCV(estimator=pipe, param_grid=param_space_random_forest, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    random_forest_standard = RandomForestClassifier(n_jobs=-1)\n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()), ('rf', random_forest_standard)])\n",
    "    \n",
    "    grid_search_forest_standard = GridSearchCV(estimator=pipe_standard, param_grid=param_space_random_forest, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    random_forests.append((poss[0], poss[1], grid_search_forest, poss[2] + ' Min Max'))\n",
    "    random_forests.append((poss[0], poss[1], grid_search_forest_standard, poss[2] + ' Standard'))\n",
    "\n",
    "for X, y, forest, _ in random_forests:\n",
    "    forest.fit(X, y)\n",
    "    \n",
    "for X, y, forest, label in random_forests:\n",
    "    print (label)\n",
    "    print ('{:.4f}'.format(forest.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "{'rf__bootstrap': False, 'rf__n_estimators': 100}\n",
      "Data Standard\n",
      "{'rf__bootstrap': False, 'rf__n_estimators': 100}\n",
      "Data Invert Min Max\n",
      "{'rf__bootstrap': False, 'rf__n_estimators': 1000}\n",
      "Data Invert Standard\n",
      "{'rf__bootstrap': False, 'rf__n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "for _, _, forest, label in random_forests:\n",
    "    print (label)\n",
    "    print (forest.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "1. AdjEM_team_two\n",
      "2. pts_team_two\n",
      "3. W_team_two\n",
      "4. pts_team_one\n",
      "5. AdjEM_team_one\n",
      "6. W_team_one\n",
      "7. AdjD_team_two\n",
      "8. opp_pts_team_one\n",
      "9. srs_team_two\n",
      "10. srs_team_one\n",
      "Data Standard\n",
      "1. srs_team_two\n",
      "2. pts_team_two\n",
      "3. W_team_two\n",
      "4. pts_team_one\n",
      "5. AdjEM_team_two\n",
      "6. AdjEM_team_one\n",
      "7. W_team_one\n",
      "8. OppD_team_two\n",
      "9. opp_pts_team_one\n",
      "10. Opp AdjEM_team_one\n",
      "Data Invert Min Max\n",
      "1. AdjEM_team_two\n",
      "2. AdjEM_team_one\n",
      "3. W_team_one\n",
      "4. W_team_two\n",
      "5. srs_team_one\n",
      "6. pts_team_one\n",
      "7. pts_team_two\n",
      "8. srs_team_two\n",
      "9. Opp AdjEM_team_two\n",
      "10. Opp AdjEM_team_one\n",
      "Data Invert Standard\n",
      "1. AdjEM_team_one\n",
      "2. AdjEM_team_two\n",
      "3. W_team_one\n",
      "4. W_team_two\n",
      "5. srs_team_two\n",
      "6. pts_team_one\n",
      "7. srs_team_one\n",
      "8. pts_team_two\n",
      "9. Opp AdjEM_team_two\n",
      "10. Opp AdjEM_team_one\n"
     ]
    }
   ],
   "source": [
    "for _, _, forest, label in random_forests:\n",
    "    feature_importances = forest.best_estimator_.named_steps['rf'].feature_importances_\n",
    "    feature_importances = [(idx, val) for idx, val in enumerate(feature_importances)]\n",
    "    feature_importances.sort(key=lambda k: k[1], reverse=True)\n",
    "    feature_importances = [idx+len(cols_to_drop) for idx, _ in feature_importances[:10]]\n",
    "    print (label)\n",
    "    for idx, col in enumerate(data.columns[feature_importances]):\n",
    "        print (f'{idx+1}. {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "[[329   0]\n",
      " [  0 783]]\n",
      "Data Standard\n",
      "[[329   0]\n",
      " [  0 783]]\n",
      "Data Invert Min Max\n",
      "[[1112    0]\n",
      " [   0 1112]]\n",
      "Data Invert Standard\n",
      "[[1112    0]\n",
      " [   0 1112]]\n"
     ]
    }
   ],
   "source": [
    "for X, y, forest, label in random_forests:\n",
    "    print (label)\n",
    "    y_pred = forest.predict(X)\n",
    "    print (confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "[-1]\n",
      "[-1]\n",
      "Data Standard\n",
      "[-1]\n",
      "[-1]\n",
      "Data Invert Min Max\n",
      "[-1]\n",
      "[-1]\n",
      "Data Invert Standard\n",
      "[-1]\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "for _, _, forest, label in random_forests:\n",
    "    print (label)\n",
    "    print (forest.predict(row))\n",
    "    print (forest.predict(row_unc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "[-1]\n",
      "Data Standard\n",
      "[-1]\n",
      "Data Invert Min Max\n",
      "[-1]\n",
      "Data Invert Standard\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "for _, _, forest, label in random_forests:\n",
    "    print (label)\n",
    "    scaled_row = forest.best_estimator_.named_steps['scale'].transform(row_unc)\n",
    "    print (forest.best_estimator_.named_steps['rf'].predict(scaled_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/tas12/Documents/Projects/BracketProject/bracketenv/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/mnt/c/Users/tas12/Documents/Projects/BracketProject/bracketenv/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/mnt/c/Users/tas12/Documents/Projects/BracketProject/bracketenv/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "0.9451\n",
      "Data Standard\n",
      "0.9056\n",
      "Data Invert Min Max\n",
      "0.9155\n",
      "Data Invert Standard\n",
      "0.9101\n",
      "CPU times: user 2min 3s, sys: 1min 6s, total: 3min 10s\n",
      "Wall time: 52min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/tas12/Documents/Projects/BracketProject/bracketenv/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Logistic Regrerssion\n",
    "param_space_log_reg = {\n",
    "    'lr__C' : [10**i for i in range(-3, 2)],\n",
    "    'lr__penalty' : ['l1']\n",
    "}\n",
    "\n",
    "log_regs = []\n",
    "for poss in sklearn_space:\n",
    "    logreg_min_max = LogisticRegression(max_iter=int(1e3), solver='saga', n_jobs=-1)\n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()), ('lr', logreg_min_max)])\n",
    "    \n",
    "    curr_labels = poss[1]\n",
    "    curr_labels[curr_labels==0] = -1\n",
    "    \n",
    "    grid_search_logreg = GridSearchCV(estimator=pipe, param_grid=param_space_log_reg, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    logreg_standard = LogisticRegression(max_iter=int(1e3), solver='saga', n_jobs=-1)\n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()), ('lr', logreg_standard)])\n",
    "    \n",
    "    grid_search_logreg_standard = GridSearchCV(estimator=pipe_standard, param_grid=param_space_log_reg, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    log_regs.append((poss[0], curr_labels, grid_search_logreg, poss[2] + ' Min Max'))\n",
    "    log_regs.append((poss[0], curr_labels, grid_search_logreg_standard, poss[2] + ' Standard'))\n",
    "    \n",
    "for X, y, logreg, _ in log_regs:\n",
    "    logreg.fit(X, y)\n",
    "    \n",
    "for X, y, logreg, label in log_regs:\n",
    "    print (label)\n",
    "    print ('{:.4f}'.format(logreg.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "{'lr__C': 1, 'lr__penalty': 'l1'}\n",
      "Data Standard\n",
      "{'lr__C': 0.1, 'lr__penalty': 'l1'}\n",
      "Data Invert Min Max\n",
      "{'lr__C': 1, 'lr__penalty': 'l1'}\n",
      "Data Invert Standard\n",
      "{'lr__C': 0.1, 'lr__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "for _, _, logreg, label in log_regs:\n",
    "    print (label)\n",
    "    print (logreg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "['opp_pts_team_one', 'losses_visitor_team_one', 'NCSOS AdjEM_team_one', 'Conference_team_two', 'Conference_team_one']\n",
      "['Seed_team_two', 'AdjO_team_one', 'losses_conf_team_two', 'losses_home_team_two', 'OppO_team_two']\n",
      "Data Standard\n",
      "['losses_visitor_team_one', 'opp_pts_team_one', 'NCSOS AdjEM_team_one', 'Conference_team_two', 'Conference_team_one']\n",
      "['AdjO_team_one', 'Seed_team_two', 'losses_conf_team_two', 'losses_home_team_two', 'OppO_team_two']\n",
      "Data Invert Min Max\n",
      "['NCSOS AdjEM_team_one', 'opp_pts_team_one', 'losses_visitor_team_one', 'Conference_team_two', 'Conference_team_one']\n",
      "['Seed_team_two', 'AdjO_team_one', 'losses_conf_team_two', 'losses_home_team_two', 'OppO_team_two']\n",
      "Data Invert Standard\n",
      "['losses_visitor_team_one', 'NCSOS AdjEM_team_one', 'opp_pts_team_one', 'Conference_team_two', 'Conference_team_one']\n",
      "['Seed_team_two', 'AdjO_team_one', 'losses_home_team_two', 'OppO_team_two', 'losses_conf_team_two']\n"
     ]
    }
   ],
   "source": [
    "for _, _, logreg, label in log_regs:\n",
    "    print (label)\n",
    "    coefs = logreg.best_estimator_.named_steps['lr'].coef_[0]\n",
    "    coefs = [(idx, coef) for idx, coef in enumerate(coefs)]\n",
    "    coefs.sort(key=lambda k: k[1], reverse=True)\n",
    "    coef_idxs = [idx for idx, _ in coefs]\n",
    "    \n",
    "    top_coefs = coef_idxs[:5]\n",
    "    bottom_coefs = coef_idxs[-5:]\n",
    "    print (list(data.columns[top_coefs]))\n",
    "    print (list(data.columns[bottom_coefs]))\n",
    "\n",
    "# coefs = [(idx, coef) for idx, coef in enumerate(logreg.coef_[0])]\n",
    "# coefs.sort(key=lambda k : k[1], reverse=True)\n",
    "\n",
    "# coef_idxs = [idx for idx, _ in coefs]\n",
    "# for idx, col in enumerate(data.columns[coef_idxs]):\n",
    "#     print (f'{col}: {coefs[idx][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "[[286  43]\n",
      " [ 18 765]]\n",
      "Data Standard\n",
      "[[257  72]\n",
      " [ 33 750]]\n",
      "Data Invert Min Max\n",
      "[[1018   94]\n",
      " [  94 1018]]\n",
      "Data Invert Standard\n",
      "[[1012  100]\n",
      " [ 100 1012]]\n"
     ]
    }
   ],
   "source": [
    "for X, y, logreg, label in log_regs:\n",
    "    print (label)\n",
    "    y_pred = logreg.predict(X)\n",
    "    print (confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "0.4263\n",
      "Data Standard\n",
      "0.3939\n",
      "Data Invert Min Max\n",
      "0.7203\n",
      "Data Invert Standard\n",
      "0.7203\n",
      "CPU times: user 1.59 s, sys: 93.8 ms, total: 1.69 s\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_space_gaussian = {\n",
    "    \n",
    "}\n",
    "\n",
    "gaussians = []\n",
    "for poss in sklearn_space:\n",
    "    gaussian_min_max = GaussianNB()\n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()), ('gauss', gaussian_min_max)])\n",
    "    \n",
    "    grid_search_gaussian = GridSearchCV(estimator=pipe, param_grid=param_space_gaussian, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    gaussian_standard = GaussianNB()\n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()), ('gauss', gaussian_standard)])\n",
    "    \n",
    "    grid_search_gaussian_standard = GridSearchCV(estimator=pipe_standard, param_grid=param_space_gaussian, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    gaussians.append((poss[0], poss[1], grid_search_gaussian, poss[2] + ' Min Max'))\n",
    "    gaussians.append((poss[0], poss[1], grid_search_gaussian_standard, poss[2] + ' Standard'))\n",
    "    \n",
    "for X, y, gaussian, _ in gaussians:\n",
    "    gaussian.fit(X, y)\n",
    "    \n",
    "for X, y, gaussian, label in gaussians:\n",
    "    print (label)\n",
    "    print ('{:.4f}'.format(gaussian.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "0.9254\n",
      "Data Standard\n",
      "0.9559\n",
      "Data Invert Min Max\n",
      "0.7455\n",
      "Data Invert Standard\n",
      "0.9096\n",
      "CPU times: user 37.4 s, sys: 18.5 s, total: 55.8 s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_space_perceptron = {\n",
    "    'perceptron__penalty' : [None, 'l2', 'l1', 'elasticnet'],\n",
    "    'perceptron__alpha' : [10**-3]\n",
    "}\n",
    "\n",
    "perceptrons = []\n",
    "for poss in sklearn_space:\n",
    "    perceptron_min_max = Perceptron()\n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()), ('perceptron', perceptron_min_max)])\n",
    "    \n",
    "    grid_search_perceptron = GridSearchCV(estimator=pipe, param_grid=param_space_perceptron, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    perceptron_standard = Perceptron()\n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()), ('perceptron', perceptron_standard)])\n",
    "    \n",
    "    grid_search_perceptron_standard = GridSearchCV(estimator=pipe_standard, param_grid=param_space_perceptron, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    perceptrons.append((poss[0], poss[1], grid_search_perceptron, poss[2] + ' Min Max'))\n",
    "    perceptrons.append((poss[0], poss[1], grid_search_perceptron_standard, poss[2] + ' Standard'))\n",
    "    \n",
    "for X, y, perceptron, _ in perceptrons:\n",
    "    perceptron.fit(X, y)\n",
    "    \n",
    "for X, y, perceptron, label in perceptrons:\n",
    "    print (label)\n",
    "    print ('{:.4f}'.format(perceptron.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "{'perceptron__alpha': 0.001, 'perceptron__penalty': 'l1'}\n",
      "Data Standard\n",
      "{'perceptron__alpha': 0.001, 'perceptron__penalty': 'l1'}\n",
      "Data Invert Min Max\n",
      "{'perceptron__alpha': 0.001, 'perceptron__penalty': 'l1'}\n",
      "Data Invert Standard\n",
      "{'perceptron__alpha': 0.001, 'perceptron__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "for _, _, perceptron, label in perceptrons:\n",
    "    print (label)\n",
    "    print (perceptron.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.99679949\n",
      "Iteration 2, loss = 0.69336787\n",
      "Iteration 3, loss = 0.63696748\n",
      "Iteration 4, loss = 0.62494847\n",
      "Iteration 5, loss = 0.59357653\n",
      "Iteration 6, loss = 0.58582651\n",
      "Iteration 7, loss = 0.57128603\n",
      "Iteration 8, loss = 0.56406300\n",
      "Iteration 9, loss = 0.55771470\n",
      "Iteration 10, loss = 0.53816465\n",
      "Iteration 11, loss = 0.53819618\n",
      "Iteration 12, loss = 0.53073269\n",
      "Iteration 13, loss = 0.52082358\n",
      "Iteration 14, loss = 0.51415449\n",
      "Iteration 15, loss = 0.51183202\n",
      "Iteration 16, loss = 0.50299021\n",
      "Iteration 17, loss = 0.49812113\n",
      "Iteration 18, loss = 0.48651988\n",
      "Iteration 19, loss = 0.47992015\n",
      "Iteration 20, loss = 0.48111291\n",
      "Iteration 21, loss = 0.46525540\n",
      "Iteration 22, loss = 0.46112939\n",
      "Iteration 23, loss = 0.45301885\n",
      "Iteration 24, loss = 0.44184863\n",
      "Iteration 25, loss = 0.43388610\n",
      "Iteration 26, loss = 0.42727217\n",
      "Iteration 27, loss = 0.42396247\n",
      "Iteration 28, loss = 0.41020301\n",
      "Iteration 29, loss = 0.40377484\n",
      "Iteration 30, loss = 0.40477592\n",
      "Iteration 31, loss = 0.39158685\n",
      "Iteration 32, loss = 0.38026305\n",
      "Iteration 33, loss = 0.37566448\n",
      "Iteration 34, loss = 0.36789979\n",
      "Iteration 35, loss = 0.35955608\n",
      "Iteration 36, loss = 0.36006556\n",
      "Iteration 37, loss = 0.35072627\n",
      "Iteration 38, loss = 0.36191816\n",
      "Iteration 39, loss = 0.34694328\n",
      "Iteration 40, loss = 0.32544238\n",
      "Iteration 41, loss = 0.31873748\n",
      "Iteration 42, loss = 0.33117233\n",
      "Iteration 43, loss = 0.32594728\n",
      "Iteration 44, loss = 0.32312201\n",
      "Iteration 45, loss = 0.32069370\n",
      "Iteration 46, loss = 0.31606534\n",
      "Iteration 47, loss = 0.28899963\n",
      "Iteration 48, loss = 0.28507456\n",
      "Iteration 49, loss = 0.30024666\n",
      "Iteration 50, loss = 0.30390597\n",
      "Iteration 51, loss = 0.28568480\n",
      "Iteration 52, loss = 0.27709796\n",
      "Iteration 53, loss = 0.27550016\n",
      "Iteration 54, loss = 0.25986166\n",
      "Iteration 55, loss = 0.27160833\n",
      "Iteration 56, loss = 0.25465348\n",
      "Iteration 57, loss = 0.25207793\n",
      "Iteration 58, loss = 0.25130304\n",
      "Iteration 59, loss = 0.24118652\n",
      "Iteration 60, loss = 0.22965324\n",
      "Iteration 61, loss = 0.22707503\n",
      "Iteration 62, loss = 0.22874117\n",
      "Iteration 63, loss = 0.21755865\n",
      "Iteration 64, loss = 0.22123882\n",
      "Iteration 65, loss = 0.21160567\n",
      "Iteration 66, loss = 0.20655420\n",
      "Iteration 67, loss = 0.20433275\n",
      "Iteration 68, loss = 0.20133818\n",
      "Iteration 69, loss = 0.19345557\n",
      "Iteration 70, loss = 0.19173384\n",
      "Iteration 71, loss = 0.18716934\n",
      "Iteration 72, loss = 0.18748817\n",
      "Iteration 73, loss = 0.18567312\n",
      "Iteration 74, loss = 0.17579640\n",
      "Iteration 75, loss = 0.17338903\n",
      "Iteration 76, loss = 0.17413264\n",
      "Iteration 77, loss = 0.17115434\n",
      "Iteration 78, loss = 0.16338155\n",
      "Iteration 79, loss = 0.17781163\n",
      "Iteration 80, loss = 0.17260521\n",
      "Iteration 81, loss = 0.16342790\n",
      "Iteration 82, loss = 0.15503456\n",
      "Iteration 83, loss = 0.15283525\n",
      "Iteration 84, loss = 0.15215551\n",
      "Iteration 85, loss = 0.14773829\n",
      "Iteration 86, loss = 0.14706681\n",
      "Iteration 87, loss = 0.14921975\n",
      "Iteration 88, loss = 0.14573521\n",
      "Iteration 89, loss = 0.13971671\n",
      "Iteration 90, loss = 0.12706930\n",
      "Iteration 91, loss = 0.12449264\n",
      "Iteration 92, loss = 0.12392016\n",
      "Iteration 93, loss = 0.11904674\n",
      "Iteration 94, loss = 0.11717373\n",
      "Iteration 95, loss = 0.12743184\n",
      "Iteration 96, loss = 0.12011996\n",
      "Iteration 97, loss = 0.11564940\n",
      "Iteration 98, loss = 0.11288244\n",
      "Iteration 99, loss = 0.10892673\n",
      "Iteration 100, loss = 0.10650069\n",
      "Iteration 101, loss = 0.10930161\n",
      "Iteration 102, loss = 0.10721136\n",
      "Iteration 103, loss = 0.10294137\n",
      "Iteration 104, loss = 0.09454615\n",
      "Iteration 105, loss = 0.09000103\n",
      "Iteration 106, loss = 0.09038242\n",
      "Iteration 107, loss = 0.08889730\n",
      "Iteration 108, loss = 0.09203928\n",
      "Iteration 109, loss = 0.08507094\n",
      "Iteration 110, loss = 0.08316212\n",
      "Iteration 111, loss = 0.08559174\n",
      "Iteration 112, loss = 0.08616130\n",
      "Iteration 113, loss = 0.08001614\n",
      "Iteration 114, loss = 0.08189513\n",
      "Iteration 115, loss = 0.07369609\n",
      "Iteration 116, loss = 0.07119895\n",
      "Iteration 117, loss = 0.07166066\n",
      "Iteration 118, loss = 0.06921356\n",
      "Iteration 119, loss = 0.06623962\n",
      "Iteration 120, loss = 0.06465546\n",
      "Iteration 121, loss = 0.06152230\n",
      "Iteration 122, loss = 0.06051486\n",
      "Iteration 123, loss = 0.05923596\n",
      "Iteration 124, loss = 0.06435924\n",
      "Iteration 125, loss = 0.06650458\n",
      "Iteration 126, loss = 0.06204992\n",
      "Iteration 127, loss = 0.05689849\n",
      "Iteration 128, loss = 0.05451473\n",
      "Iteration 129, loss = 0.05571805\n",
      "Iteration 130, loss = 0.05303329\n",
      "Iteration 131, loss = 0.05241313\n",
      "Iteration 132, loss = 0.05187347\n",
      "Iteration 133, loss = 0.05238232\n",
      "Iteration 134, loss = 0.05278603\n",
      "Iteration 135, loss = 0.04696985\n",
      "Iteration 136, loss = 0.05911851\n",
      "Iteration 137, loss = 0.04653195\n",
      "Iteration 138, loss = 0.04307560\n",
      "Iteration 139, loss = 0.04173509\n",
      "Iteration 140, loss = 0.04146613\n",
      "Iteration 141, loss = 0.03965241\n",
      "Iteration 142, loss = 0.04009488\n",
      "Iteration 143, loss = 0.03928259\n",
      "Iteration 144, loss = 0.03739600\n",
      "Iteration 145, loss = 0.03607128\n",
      "Iteration 146, loss = 0.03565419\n",
      "Iteration 147, loss = 0.03503768\n",
      "Iteration 148, loss = 0.03370795\n",
      "Iteration 149, loss = 0.03372537\n",
      "Iteration 150, loss = 0.03257993\n",
      "Iteration 151, loss = 0.03299374\n",
      "Iteration 152, loss = 0.03155963\n",
      "Iteration 153, loss = 0.03154528\n",
      "Iteration 154, loss = 0.03008967\n",
      "Iteration 155, loss = 0.02985999\n",
      "Iteration 156, loss = 0.02954276\n",
      "Iteration 157, loss = 0.02927886\n",
      "Iteration 158, loss = 0.02758821\n",
      "Iteration 159, loss = 0.02772672\n",
      "Iteration 160, loss = 0.02664066\n",
      "Iteration 161, loss = 0.02614112\n",
      "Iteration 162, loss = 0.02573012\n",
      "Iteration 163, loss = 0.02590664\n",
      "Iteration 164, loss = 0.02554448\n",
      "Iteration 165, loss = 0.02546527\n",
      "Iteration 166, loss = 0.02676324\n",
      "Iteration 167, loss = 0.02435198\n",
      "Iteration 168, loss = 0.02445514\n",
      "Iteration 169, loss = 0.02350504\n",
      "Iteration 170, loss = 0.02281613\n",
      "Iteration 171, loss = 0.02252355\n",
      "Iteration 172, loss = 0.02183222\n",
      "Iteration 173, loss = 0.02157021\n",
      "Iteration 174, loss = 0.02131184\n",
      "Iteration 175, loss = 0.02056568\n",
      "Iteration 176, loss = 0.02118956\n",
      "Iteration 177, loss = 0.02103712\n",
      "Iteration 178, loss = 0.02175810\n",
      "Iteration 179, loss = 0.01985568\n",
      "Iteration 180, loss = 0.01975609\n",
      "Iteration 181, loss = 0.01904509\n",
      "Iteration 182, loss = 0.01797291\n",
      "Iteration 183, loss = 0.01748317\n",
      "Iteration 184, loss = 0.01738897\n",
      "Iteration 185, loss = 0.01702751\n",
      "Iteration 186, loss = 0.01709773\n",
      "Iteration 187, loss = 0.01643449\n",
      "Iteration 188, loss = 0.01613278\n",
      "Iteration 189, loss = 0.01584603\n",
      "Iteration 190, loss = 0.01602680\n",
      "Iteration 191, loss = 0.01579093\n",
      "Iteration 192, loss = 0.01521743\n",
      "Iteration 193, loss = 0.01524253\n",
      "Iteration 194, loss = 0.01468845\n",
      "Iteration 195, loss = 0.01487721\n",
      "Iteration 196, loss = 0.01471397\n",
      "Iteration 197, loss = 0.01446732\n",
      "Iteration 198, loss = 0.01410479\n",
      "Iteration 199, loss = 0.01503356\n",
      "Iteration 200, loss = 0.01353824\n",
      "Iteration 201, loss = 0.01355236\n",
      "Iteration 202, loss = 0.01391616\n",
      "Iteration 203, loss = 0.01331511\n",
      "Iteration 204, loss = 0.01302723\n",
      "Iteration 205, loss = 0.01265019\n",
      "Iteration 206, loss = 0.01238712\n",
      "Iteration 207, loss = 0.01291304\n",
      "Iteration 208, loss = 0.01247454\n",
      "Iteration 209, loss = 0.01248976\n",
      "Iteration 210, loss = 0.01179647\n",
      "Iteration 211, loss = 0.01146683\n",
      "Iteration 212, loss = 0.01141102\n",
      "Iteration 213, loss = 0.01195531\n",
      "Iteration 214, loss = 0.01109265\n",
      "Iteration 215, loss = 0.01104116\n",
      "Iteration 216, loss = 0.01078747\n",
      "Iteration 217, loss = 0.01064600\n",
      "Iteration 218, loss = 0.01064671\n",
      "Iteration 219, loss = 0.01052492\n",
      "Iteration 220, loss = 0.01023338\n",
      "Iteration 221, loss = 0.01007945\n",
      "Iteration 222, loss = 0.01022094\n",
      "Iteration 223, loss = 0.01046936\n",
      "Iteration 224, loss = 0.01083638\n",
      "Iteration 225, loss = 0.01015626\n",
      "Iteration 226, loss = 0.00970899\n",
      "Iteration 227, loss = 0.00936659\n",
      "Iteration 228, loss = 0.00931775\n",
      "Iteration 229, loss = 0.00949276\n",
      "Iteration 230, loss = 0.00918907\n",
      "Iteration 231, loss = 0.00893026\n",
      "Iteration 232, loss = 0.00879866\n",
      "Iteration 233, loss = 0.00899443\n",
      "Iteration 234, loss = 0.00876638\n",
      "Iteration 235, loss = 0.00892411\n",
      "Iteration 236, loss = 0.00851446\n",
      "Iteration 237, loss = 0.00827868\n",
      "Iteration 238, loss = 0.00814028\n",
      "Iteration 239, loss = 0.00824114\n",
      "Iteration 240, loss = 0.00800738\n",
      "Iteration 241, loss = 0.00787204\n",
      "Iteration 242, loss = 0.00784709\n",
      "Iteration 243, loss = 0.00779564\n",
      "Iteration 244, loss = 0.00780537\n",
      "Iteration 245, loss = 0.00781468\n",
      "Iteration 246, loss = 0.00751212\n",
      "Iteration 247, loss = 0.00749257\n",
      "Iteration 248, loss = 0.00739622\n",
      "Iteration 249, loss = 0.00737163\n",
      "Iteration 250, loss = 0.00733711\n",
      "Iteration 251, loss = 0.00716881\n",
      "Iteration 252, loss = 0.00702707\n",
      "Iteration 253, loss = 0.00713925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.00692102\n",
      "Iteration 255, loss = 0.00690603\n",
      "Iteration 256, loss = 0.00667177\n",
      "Iteration 257, loss = 0.00672188\n",
      "Iteration 258, loss = 0.00677265\n",
      "Iteration 259, loss = 0.00666687\n",
      "Iteration 260, loss = 0.00658869\n",
      "Iteration 261, loss = 0.00639447\n",
      "Iteration 262, loss = 0.00646033\n",
      "Iteration 263, loss = 0.00621229\n",
      "Iteration 264, loss = 0.00611323\n",
      "Iteration 265, loss = 0.00613388\n",
      "Iteration 266, loss = 0.00617140\n",
      "Iteration 267, loss = 0.00594079\n",
      "Iteration 268, loss = 0.00598730\n",
      "Iteration 269, loss = 0.00596502\n",
      "Iteration 270, loss = 0.00584177\n",
      "Iteration 271, loss = 0.00567066\n",
      "Iteration 272, loss = 0.00587461\n",
      "Iteration 273, loss = 0.00561742\n",
      "Iteration 274, loss = 0.00567765\n",
      "Iteration 275, loss = 0.00552899\n",
      "Iteration 276, loss = 0.00545224\n",
      "Iteration 277, loss = 0.00538433\n",
      "Iteration 278, loss = 0.00535404\n",
      "Iteration 279, loss = 0.00536733\n",
      "Iteration 280, loss = 0.00525737\n",
      "Iteration 281, loss = 0.00524896\n",
      "Iteration 282, loss = 0.00518113\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06196066\n",
      "Iteration 2, loss = 0.72180434\n",
      "Iteration 3, loss = 0.61146343\n",
      "Iteration 4, loss = 0.60400309\n",
      "Iteration 5, loss = 0.58850274\n",
      "Iteration 6, loss = 0.56686773\n",
      "Iteration 7, loss = 0.55333064\n",
      "Iteration 8, loss = 0.54512860\n",
      "Iteration 9, loss = 0.53984467\n",
      "Iteration 10, loss = 0.53121000\n",
      "Iteration 11, loss = 0.52093784\n",
      "Iteration 12, loss = 0.51406124\n",
      "Iteration 13, loss = 0.51351694\n",
      "Iteration 14, loss = 0.50302490\n",
      "Iteration 15, loss = 0.49284401\n",
      "Iteration 16, loss = 0.48874540\n",
      "Iteration 17, loss = 0.47930023\n",
      "Iteration 18, loss = 0.47502485\n",
      "Iteration 19, loss = 0.46910927\n",
      "Iteration 20, loss = 0.46620249\n",
      "Iteration 21, loss = 0.45906490\n",
      "Iteration 22, loss = 0.45099000\n",
      "Iteration 23, loss = 0.44085785\n",
      "Iteration 24, loss = 0.43282608\n",
      "Iteration 25, loss = 0.42374083\n",
      "Iteration 26, loss = 0.42787292\n",
      "Iteration 27, loss = 0.41626097\n",
      "Iteration 28, loss = 0.41851327\n",
      "Iteration 29, loss = 0.39772554\n",
      "Iteration 30, loss = 0.38810194\n",
      "Iteration 31, loss = 0.38002219\n",
      "Iteration 32, loss = 0.37360501\n",
      "Iteration 33, loss = 0.36604035\n",
      "Iteration 34, loss = 0.36603528\n",
      "Iteration 35, loss = 0.35557021\n",
      "Iteration 36, loss = 0.36309822\n",
      "Iteration 37, loss = 0.36276094\n",
      "Iteration 38, loss = 0.34348466\n",
      "Iteration 39, loss = 0.33536708\n",
      "Iteration 40, loss = 0.32723707\n",
      "Iteration 41, loss = 0.32646605\n",
      "Iteration 42, loss = 0.31486824\n",
      "Iteration 43, loss = 0.30630917\n",
      "Iteration 44, loss = 0.30076691\n",
      "Iteration 45, loss = 0.30623636\n",
      "Iteration 46, loss = 0.29646801\n",
      "Iteration 47, loss = 0.28939389\n",
      "Iteration 48, loss = 0.28368060\n",
      "Iteration 49, loss = 0.29447377\n",
      "Iteration 50, loss = 0.27926203\n",
      "Iteration 51, loss = 0.26953766\n",
      "Iteration 52, loss = 0.26399981\n",
      "Iteration 53, loss = 0.25490524\n",
      "Iteration 54, loss = 0.26549909\n",
      "Iteration 55, loss = 0.26305312\n",
      "Iteration 56, loss = 0.25735930\n",
      "Iteration 57, loss = 0.24508950\n",
      "Iteration 58, loss = 0.23165576\n",
      "Iteration 59, loss = 0.23908524\n",
      "Iteration 60, loss = 0.23022303\n",
      "Iteration 61, loss = 0.23641730\n",
      "Iteration 62, loss = 0.21844036\n",
      "Iteration 63, loss = 0.21916753\n",
      "Iteration 64, loss = 0.21177637\n",
      "Iteration 65, loss = 0.20490774\n",
      "Iteration 66, loss = 0.21509023\n",
      "Iteration 67, loss = 0.19748026\n",
      "Iteration 68, loss = 0.20227124\n",
      "Iteration 69, loss = 0.19568582\n",
      "Iteration 70, loss = 0.19380074\n",
      "Iteration 71, loss = 0.17988546\n",
      "Iteration 72, loss = 0.18081710\n",
      "Iteration 73, loss = 0.17404089\n",
      "Iteration 74, loss = 0.17421313\n",
      "Iteration 75, loss = 0.18562711\n",
      "Iteration 76, loss = 0.19270015\n",
      "Iteration 77, loss = 0.17503288\n",
      "Iteration 78, loss = 0.16966099\n",
      "Iteration 79, loss = 0.15229203\n",
      "Iteration 80, loss = 0.15209113\n",
      "Iteration 81, loss = 0.16201960\n",
      "Iteration 82, loss = 0.15017582\n",
      "Iteration 83, loss = 0.14472220\n",
      "Iteration 84, loss = 0.13755581\n",
      "Iteration 85, loss = 0.13938461\n",
      "Iteration 86, loss = 0.14102395\n",
      "Iteration 87, loss = 0.13602331\n",
      "Iteration 88, loss = 0.14251530\n",
      "Iteration 89, loss = 0.13752474\n",
      "Iteration 90, loss = 0.14880694\n",
      "Iteration 91, loss = 0.13128476\n",
      "Iteration 92, loss = 0.11691221\n",
      "Iteration 93, loss = 0.11670647\n",
      "Iteration 94, loss = 0.11203825\n",
      "Iteration 95, loss = 0.10817777\n",
      "Iteration 96, loss = 0.10978935\n",
      "Iteration 97, loss = 0.10544233\n",
      "Iteration 98, loss = 0.10066213\n",
      "Iteration 99, loss = 0.10526864\n",
      "Iteration 100, loss = 0.12404332\n",
      "Iteration 101, loss = 0.12837390\n",
      "Iteration 102, loss = 0.11715143\n",
      "Iteration 103, loss = 0.09979156\n",
      "Iteration 104, loss = 0.10052324\n",
      "Iteration 105, loss = 0.10998997\n",
      "Iteration 106, loss = 0.09808091\n",
      "Iteration 107, loss = 0.08369163\n",
      "Iteration 108, loss = 0.08166196\n",
      "Iteration 109, loss = 0.08022072\n",
      "Iteration 110, loss = 0.07944496\n",
      "Iteration 111, loss = 0.08457517\n",
      "Iteration 112, loss = 0.08470504\n",
      "Iteration 113, loss = 0.08365397\n",
      "Iteration 114, loss = 0.07432983\n",
      "Iteration 115, loss = 0.07243816\n",
      "Iteration 116, loss = 0.06911629\n",
      "Iteration 117, loss = 0.06557760\n",
      "Iteration 118, loss = 0.06477902\n",
      "Iteration 119, loss = 0.06966247\n",
      "Iteration 120, loss = 0.06669356\n",
      "Iteration 121, loss = 0.06079196\n",
      "Iteration 122, loss = 0.05924482\n",
      "Iteration 123, loss = 0.05726919\n",
      "Iteration 124, loss = 0.05665003\n",
      "Iteration 125, loss = 0.05661459\n",
      "Iteration 126, loss = 0.05434826\n",
      "Iteration 127, loss = 0.05273307\n",
      "Iteration 128, loss = 0.05209589\n",
      "Iteration 129, loss = 0.05134558\n",
      "Iteration 130, loss = 0.05153143\n",
      "Iteration 131, loss = 0.04961388\n",
      "Iteration 132, loss = 0.05032125\n",
      "Iteration 133, loss = 0.04720065\n",
      "Iteration 134, loss = 0.04579443\n",
      "Iteration 135, loss = 0.04509858\n",
      "Iteration 136, loss = 0.04457607\n",
      "Iteration 137, loss = 0.04326669\n",
      "Iteration 138, loss = 0.04445998\n",
      "Iteration 139, loss = 0.04606196\n",
      "Iteration 140, loss = 0.04370906\n",
      "Iteration 141, loss = 0.03900589\n",
      "Iteration 142, loss = 0.03867876\n",
      "Iteration 143, loss = 0.03723398\n",
      "Iteration 144, loss = 0.03634683\n",
      "Iteration 145, loss = 0.03662684\n",
      "Iteration 146, loss = 0.03671480\n",
      "Iteration 147, loss = 0.03622850\n",
      "Iteration 148, loss = 0.03471894\n",
      "Iteration 149, loss = 0.03759480\n",
      "Iteration 150, loss = 0.03548992\n",
      "Iteration 151, loss = 0.03619199\n",
      "Iteration 152, loss = 0.03127890\n",
      "Iteration 153, loss = 0.03141502\n",
      "Iteration 154, loss = 0.03101913\n",
      "Iteration 155, loss = 0.03046296\n",
      "Iteration 156, loss = 0.03051761\n",
      "Iteration 157, loss = 0.03013092\n",
      "Iteration 158, loss = 0.03141032\n",
      "Iteration 159, loss = 0.03412871\n",
      "Iteration 160, loss = 0.03520013\n",
      "Iteration 161, loss = 0.03399833\n",
      "Iteration 162, loss = 0.02892409\n",
      "Iteration 163, loss = 0.02596726\n",
      "Iteration 164, loss = 0.02560109\n",
      "Iteration 165, loss = 0.02599842\n",
      "Iteration 166, loss = 0.02641021\n",
      "Iteration 167, loss = 0.02422541\n",
      "Iteration 168, loss = 0.02260509\n",
      "Iteration 169, loss = 0.02204169\n",
      "Iteration 170, loss = 0.02273319\n",
      "Iteration 171, loss = 0.02236785\n",
      "Iteration 172, loss = 0.02196165\n",
      "Iteration 173, loss = 0.02197701\n",
      "Iteration 174, loss = 0.02256615\n",
      "Iteration 175, loss = 0.02301844\n",
      "Iteration 176, loss = 0.02026995\n",
      "Iteration 177, loss = 0.02022719\n",
      "Iteration 178, loss = 0.01973967\n",
      "Iteration 179, loss = 0.01904357\n",
      "Iteration 180, loss = 0.02041334\n",
      "Iteration 181, loss = 0.01998301\n",
      "Iteration 182, loss = 0.01926536\n",
      "Iteration 183, loss = 0.01809027\n",
      "Iteration 184, loss = 0.01798194\n",
      "Iteration 185, loss = 0.01831788\n",
      "Iteration 186, loss = 0.01714656\n",
      "Iteration 187, loss = 0.01718850\n",
      "Iteration 188, loss = 0.01656655\n",
      "Iteration 189, loss = 0.01749107\n",
      "Iteration 190, loss = 0.01628066\n",
      "Iteration 191, loss = 0.01581649\n",
      "Iteration 192, loss = 0.01529076\n",
      "Iteration 193, loss = 0.01514344\n",
      "Iteration 194, loss = 0.01482904\n",
      "Iteration 195, loss = 0.01459214\n",
      "Iteration 196, loss = 0.01435781\n",
      "Iteration 197, loss = 0.01409770\n",
      "Iteration 198, loss = 0.01387947\n",
      "Iteration 199, loss = 0.01376185\n",
      "Iteration 200, loss = 0.01377781\n",
      "Iteration 201, loss = 0.01372532\n",
      "Iteration 202, loss = 0.01329031\n",
      "Iteration 203, loss = 0.01294118\n",
      "Iteration 204, loss = 0.01292863\n",
      "Iteration 205, loss = 0.01273066\n",
      "Iteration 206, loss = 0.01238331\n",
      "Iteration 207, loss = 0.01232724\n",
      "Iteration 208, loss = 0.01231807\n",
      "Iteration 209, loss = 0.01192403\n",
      "Iteration 210, loss = 0.01192981\n",
      "Iteration 211, loss = 0.01170856\n",
      "Iteration 212, loss = 0.01164093\n",
      "Iteration 213, loss = 0.01150225\n",
      "Iteration 214, loss = 0.01127728\n",
      "Iteration 215, loss = 0.01107716\n",
      "Iteration 216, loss = 0.01089041\n",
      "Iteration 217, loss = 0.01089878\n",
      "Iteration 218, loss = 0.01058825\n",
      "Iteration 219, loss = 0.01049425\n",
      "Iteration 220, loss = 0.01053750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 221, loss = 0.01031984\n",
      "Iteration 222, loss = 0.01019511\n",
      "Iteration 223, loss = 0.00998028\n",
      "Iteration 224, loss = 0.00998338\n",
      "Iteration 225, loss = 0.00974403\n",
      "Iteration 226, loss = 0.00967363\n",
      "Iteration 227, loss = 0.00984690\n",
      "Iteration 228, loss = 0.00992522\n",
      "Iteration 229, loss = 0.00959892\n",
      "Iteration 230, loss = 0.00927099\n",
      "Iteration 231, loss = 0.00901833\n",
      "Iteration 232, loss = 0.00892239\n",
      "Iteration 233, loss = 0.00900710\n",
      "Iteration 234, loss = 0.00874799\n",
      "Iteration 235, loss = 0.00871382\n",
      "Iteration 236, loss = 0.00852084\n",
      "Iteration 237, loss = 0.00844967\n",
      "Iteration 238, loss = 0.00825572\n",
      "Iteration 239, loss = 0.00826475\n",
      "Iteration 240, loss = 0.00811035\n",
      "Iteration 241, loss = 0.00804963\n",
      "Iteration 242, loss = 0.00816108\n",
      "Iteration 243, loss = 0.00827825\n",
      "Iteration 244, loss = 0.00805948\n",
      "Iteration 245, loss = 0.00789677\n",
      "Iteration 246, loss = 0.00771386\n",
      "Iteration 247, loss = 0.00753071\n",
      "Iteration 248, loss = 0.00750387\n",
      "Iteration 249, loss = 0.00738022\n",
      "Iteration 250, loss = 0.00728991\n",
      "Iteration 251, loss = 0.00719732\n",
      "Iteration 252, loss = 0.00712608\n",
      "Iteration 253, loss = 0.00702683\n",
      "Iteration 254, loss = 0.00704620\n",
      "Iteration 255, loss = 0.00688554\n",
      "Iteration 256, loss = 0.00692846\n",
      "Iteration 257, loss = 0.00686245\n",
      "Iteration 258, loss = 0.00673993\n",
      "Iteration 259, loss = 0.00658956\n",
      "Iteration 260, loss = 0.00659499\n",
      "Iteration 261, loss = 0.00663510\n",
      "Iteration 262, loss = 0.00640327\n",
      "Iteration 263, loss = 0.00643869\n",
      "Iteration 264, loss = 0.00625594\n",
      "Iteration 265, loss = 0.00621234\n",
      "Iteration 266, loss = 0.00612627\n",
      "Iteration 267, loss = 0.00607019\n",
      "Iteration 268, loss = 0.00605275\n",
      "Iteration 269, loss = 0.00595177\n",
      "Iteration 270, loss = 0.00591165\n",
      "Iteration 271, loss = 0.00604861\n",
      "Iteration 272, loss = 0.00589787\n",
      "Iteration 273, loss = 0.00577779\n",
      "Iteration 274, loss = 0.00572059\n",
      "Iteration 275, loss = 0.00565152\n",
      "Iteration 276, loss = 0.00561852\n",
      "Iteration 277, loss = 0.00552472\n",
      "Iteration 278, loss = 0.00552358\n",
      "Iteration 279, loss = 0.00544805\n",
      "Iteration 280, loss = 0.00543032\n",
      "Iteration 281, loss = 0.00531467\n",
      "Iteration 282, loss = 0.00526371\n",
      "Iteration 283, loss = 0.00525102\n",
      "Iteration 284, loss = 0.00527239\n",
      "Iteration 285, loss = 0.00527194\n",
      "Iteration 286, loss = 0.00554580\n",
      "Iteration 287, loss = 0.00526090\n",
      "Iteration 288, loss = 0.00504861\n",
      "Iteration 289, loss = 0.00496773\n",
      "Iteration 290, loss = 0.00492150\n",
      "Iteration 291, loss = 0.00487247\n",
      "Iteration 292, loss = 0.00488149\n",
      "Iteration 293, loss = 0.00485187\n",
      "Iteration 294, loss = 0.00477189\n",
      "Iteration 295, loss = 0.00480223\n",
      "Iteration 296, loss = 0.00467306\n",
      "Iteration 297, loss = 0.00461611\n",
      "Iteration 298, loss = 0.00458417\n",
      "Iteration 299, loss = 0.00454711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03985852\n",
      "Iteration 2, loss = 1.13673508\n",
      "Iteration 3, loss = 0.62060966\n",
      "Iteration 4, loss = 0.67849599\n",
      "Iteration 5, loss = 0.60843084\n",
      "Iteration 6, loss = 0.60013615\n",
      "Iteration 7, loss = 0.59058568\n",
      "Iteration 8, loss = 0.60139383\n",
      "Iteration 9, loss = 0.57898591\n",
      "Iteration 10, loss = 0.56721836\n",
      "Iteration 11, loss = 0.58170975\n",
      "Iteration 12, loss = 0.55839296\n",
      "Iteration 13, loss = 0.54823247\n",
      "Iteration 14, loss = 0.56373870\n",
      "Iteration 15, loss = 0.54672273\n",
      "Iteration 16, loss = 0.54540431\n",
      "Iteration 17, loss = 0.69051219\n",
      "Iteration 18, loss = 0.60998578\n",
      "Iteration 19, loss = 0.57014584\n",
      "Iteration 20, loss = 0.56415609\n",
      "Iteration 21, loss = 0.60838249\n",
      "Iteration 22, loss = 0.54654137\n",
      "Iteration 23, loss = 0.53743389\n",
      "Iteration 24, loss = 0.56729046\n",
      "Iteration 25, loss = 0.54357015\n",
      "Iteration 26, loss = 0.60569851\n",
      "Iteration 27, loss = 0.53254971\n",
      "Iteration 28, loss = 0.52553480\n",
      "Iteration 29, loss = 0.53358362\n",
      "Iteration 30, loss = 0.51632695\n",
      "Iteration 31, loss = 0.51335614\n",
      "Iteration 32, loss = 0.60953927\n",
      "Iteration 33, loss = 0.57531325\n",
      "Iteration 34, loss = 0.59377884\n",
      "Iteration 35, loss = 0.59899537\n",
      "Iteration 36, loss = 0.54098644\n",
      "Iteration 37, loss = 0.57352108\n",
      "Iteration 38, loss = 0.50703816\n",
      "Iteration 39, loss = 0.49927609\n",
      "Iteration 40, loss = 0.57824350\n",
      "Iteration 41, loss = 0.52598119\n",
      "Iteration 42, loss = 0.57329877\n",
      "Iteration 43, loss = 0.52670743\n",
      "Iteration 44, loss = 0.49519160\n",
      "Iteration 45, loss = 0.53094122\n",
      "Iteration 46, loss = 0.50598687\n",
      "Iteration 47, loss = 0.62078192\n",
      "Iteration 48, loss = 0.52479734\n",
      "Iteration 49, loss = 0.59030707\n",
      "Iteration 50, loss = 0.49461858\n",
      "Iteration 51, loss = 0.57987881\n",
      "Iteration 52, loss = 0.50935343\n",
      "Iteration 53, loss = 0.58408679\n",
      "Iteration 54, loss = 0.49198152\n",
      "Iteration 55, loss = 0.56172793\n",
      "Iteration 56, loss = 0.49512470\n",
      "Iteration 57, loss = 0.48193341\n",
      "Iteration 58, loss = 0.51676526\n",
      "Iteration 59, loss = 0.47932504\n",
      "Iteration 60, loss = 0.46981643\n",
      "Iteration 61, loss = 0.48889435\n",
      "Iteration 62, loss = 0.46776193\n",
      "Iteration 63, loss = 0.47176505\n",
      "Iteration 64, loss = 0.47440337\n",
      "Iteration 65, loss = 0.46640070\n",
      "Iteration 66, loss = 0.48539243\n",
      "Iteration 67, loss = 0.46813437\n",
      "Iteration 68, loss = 0.48634593\n",
      "Iteration 69, loss = 0.48032093\n",
      "Iteration 70, loss = 0.46866732\n",
      "Iteration 71, loss = 0.51879272\n",
      "Iteration 72, loss = 0.53259962\n",
      "Iteration 73, loss = 0.50061251\n",
      "Iteration 74, loss = 0.70164913\n",
      "Iteration 75, loss = 0.48759973\n",
      "Iteration 76, loss = 0.53852179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04233256\n",
      "Iteration 2, loss = 0.71735415\n",
      "Iteration 3, loss = 0.63170684\n",
      "Iteration 4, loss = 0.58725892\n",
      "Iteration 5, loss = 0.63040374\n",
      "Iteration 6, loss = 0.65596334\n",
      "Iteration 7, loss = 0.62732127\n",
      "Iteration 8, loss = 0.60286527\n",
      "Iteration 9, loss = 0.67926733\n",
      "Iteration 10, loss = 0.66160107\n",
      "Iteration 11, loss = 0.66695633\n",
      "Iteration 12, loss = 0.57469317\n",
      "Iteration 13, loss = 0.56390642\n",
      "Iteration 14, loss = 0.59104450\n",
      "Iteration 15, loss = 0.55200469\n",
      "Iteration 16, loss = 0.63121643\n",
      "Iteration 17, loss = 0.57719609\n",
      "Iteration 18, loss = 0.55780945\n",
      "Iteration 19, loss = 0.68151642\n",
      "Iteration 20, loss = 0.57040999\n",
      "Iteration 21, loss = 0.64292966\n",
      "Iteration 22, loss = 0.55361602\n",
      "Iteration 23, loss = 0.63682103\n",
      "Iteration 24, loss = 0.56519547\n",
      "Iteration 25, loss = 0.55135426\n",
      "Iteration 26, loss = 0.55191161\n",
      "Iteration 27, loss = 0.55336281\n",
      "Iteration 28, loss = 0.55488588\n",
      "Iteration 29, loss = 0.54468386\n",
      "Iteration 30, loss = 0.55590541\n",
      "Iteration 31, loss = 0.53169207\n",
      "Iteration 32, loss = 0.52376639\n",
      "Iteration 33, loss = 0.52918555\n",
      "Iteration 34, loss = 0.52463546\n",
      "Iteration 35, loss = 0.52770827\n",
      "Iteration 36, loss = 0.52753836\n",
      "Iteration 37, loss = 0.61359287\n",
      "Iteration 38, loss = 0.55823355\n",
      "Iteration 39, loss = 0.56093723\n",
      "Iteration 40, loss = 0.50604871\n",
      "Iteration 41, loss = 0.57441320\n",
      "Iteration 42, loss = 0.50320646\n",
      "Iteration 43, loss = 0.52314978\n",
      "Iteration 44, loss = 0.51178226\n",
      "Iteration 45, loss = 0.56671108\n",
      "Iteration 46, loss = 0.50299297\n",
      "Iteration 47, loss = 0.49460749\n",
      "Iteration 48, loss = 0.52394840\n",
      "Iteration 49, loss = 0.50361081\n",
      "Iteration 50, loss = 0.48755184\n",
      "Iteration 51, loss = 0.50611507\n",
      "Iteration 52, loss = 0.50657431\n",
      "Iteration 53, loss = 0.48368701\n",
      "Iteration 54, loss = 0.47826303\n",
      "Iteration 55, loss = 0.49230798\n",
      "Iteration 56, loss = 0.47942147\n",
      "Iteration 57, loss = 0.48283859\n",
      "Iteration 58, loss = 0.46930502\n",
      "Iteration 59, loss = 0.65173749\n",
      "Iteration 60, loss = 0.48078234\n",
      "Iteration 61, loss = 0.49659331\n",
      "Iteration 62, loss = 0.46714701\n",
      "Iteration 63, loss = 0.46786111\n",
      "Iteration 64, loss = 0.48151186\n",
      "Iteration 65, loss = 0.46056340\n",
      "Iteration 66, loss = 0.48537049\n",
      "Iteration 67, loss = 0.46898698\n",
      "Iteration 68, loss = 0.45674568\n",
      "Iteration 69, loss = 0.44555621\n",
      "Iteration 70, loss = 0.45267639\n",
      "Iteration 71, loss = 0.43975261\n",
      "Iteration 72, loss = 0.45699498\n",
      "Iteration 73, loss = 0.43541442\n",
      "Iteration 74, loss = 0.45300467\n",
      "Iteration 75, loss = 0.44715397\n",
      "Iteration 76, loss = 0.42805782\n",
      "Iteration 77, loss = 0.43659199\n",
      "Iteration 78, loss = 0.42615870\n",
      "Iteration 79, loss = 0.42773160\n",
      "Iteration 80, loss = 0.45372087\n",
      "Iteration 81, loss = 0.50788024\n",
      "Iteration 82, loss = 0.55893410\n",
      "Iteration 83, loss = 0.44134373\n",
      "Iteration 84, loss = 0.47980331\n",
      "Iteration 85, loss = 0.42587633\n",
      "Iteration 86, loss = 0.46623337\n",
      "Iteration 87, loss = 0.42182848\n",
      "Iteration 88, loss = 0.54275995\n",
      "Iteration 89, loss = 0.41720943\n",
      "Iteration 90, loss = 0.42533997\n",
      "Iteration 91, loss = 0.41696969\n",
      "Iteration 92, loss = 0.48738907\n",
      "Iteration 93, loss = 0.42760030\n",
      "Iteration 94, loss = 0.47217090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 95, loss = 0.41123826\n",
      "Iteration 96, loss = 0.40532598\n",
      "Iteration 97, loss = 0.41660283\n",
      "Iteration 98, loss = 0.39829156\n",
      "Iteration 99, loss = 0.43074168\n",
      "Iteration 100, loss = 0.39456349\n",
      "Iteration 101, loss = 0.38628368\n",
      "Iteration 102, loss = 0.38560152\n",
      "Iteration 103, loss = 0.38479170\n",
      "Iteration 104, loss = 0.43538878\n",
      "Iteration 105, loss = 0.38898240\n",
      "Iteration 106, loss = 0.39214348\n",
      "Iteration 107, loss = 0.40519010\n",
      "Iteration 108, loss = 0.71824287\n",
      "Iteration 109, loss = 0.66750927\n",
      "Iteration 110, loss = 0.43230835\n",
      "Iteration 111, loss = 0.46573176\n",
      "Iteration 112, loss = 0.43756301\n",
      "Iteration 113, loss = 0.77234130\n",
      "Iteration 114, loss = 0.59738195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05537415\n",
      "Iteration 2, loss = 0.70583129\n",
      "Iteration 3, loss = 0.68444552\n",
      "Iteration 4, loss = 0.60652791\n",
      "Iteration 5, loss = 0.64436769\n",
      "Iteration 6, loss = 0.58096692\n",
      "Iteration 7, loss = 0.59078721\n",
      "Iteration 8, loss = 0.58609224\n",
      "Iteration 9, loss = 0.56204499\n",
      "Iteration 10, loss = 0.55626614\n",
      "Iteration 11, loss = 0.74895301\n",
      "Iteration 12, loss = 0.74340345\n",
      "Iteration 13, loss = 0.66511294\n",
      "Iteration 14, loss = 0.56796443\n",
      "Iteration 15, loss = 0.57137111\n",
      "Iteration 16, loss = 0.55979310\n",
      "Iteration 17, loss = 0.53603441\n",
      "Iteration 18, loss = 0.53604991\n",
      "Iteration 19, loss = 0.52976655\n",
      "Iteration 20, loss = 0.54801807\n",
      "Iteration 21, loss = 0.52077762\n",
      "Iteration 22, loss = 0.53755532\n",
      "Iteration 23, loss = 0.52625319\n",
      "Iteration 24, loss = 0.51630238\n",
      "Iteration 25, loss = 0.51886459\n",
      "Iteration 26, loss = 0.51571761\n",
      "Iteration 27, loss = 0.50560366\n",
      "Iteration 28, loss = 0.53156667\n",
      "Iteration 29, loss = 0.50454195\n",
      "Iteration 30, loss = 0.67374316\n",
      "Iteration 31, loss = 0.69683650\n",
      "Iteration 32, loss = 0.52307078\n",
      "Iteration 33, loss = 0.50731521\n",
      "Iteration 34, loss = 0.58716641\n",
      "Iteration 35, loss = 0.49035524\n",
      "Iteration 36, loss = 0.50323180\n",
      "Iteration 37, loss = 0.49543295\n",
      "Iteration 38, loss = 0.49052426\n",
      "Iteration 39, loss = 0.56827750\n",
      "Iteration 40, loss = 0.58627548\n",
      "Iteration 41, loss = 0.66585383\n",
      "Iteration 42, loss = 0.55478716\n",
      "Iteration 43, loss = 0.52876993\n",
      "Iteration 44, loss = 0.56719236\n",
      "Iteration 45, loss = 0.51119153\n",
      "Iteration 46, loss = 0.51115326\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03549764\n",
      "Iteration 2, loss = 0.66152070\n",
      "Iteration 3, loss = 0.68175613\n",
      "Iteration 4, loss = 0.61328459\n",
      "Iteration 5, loss = 0.59388385\n",
      "Iteration 6, loss = 0.58138315\n",
      "Iteration 7, loss = 0.64342009\n",
      "Iteration 8, loss = 0.58280027\n",
      "Iteration 9, loss = 0.55807772\n",
      "Iteration 10, loss = 0.56966101\n",
      "Iteration 11, loss = 0.55119055\n",
      "Iteration 12, loss = 0.62441172\n",
      "Iteration 13, loss = 0.57807290\n",
      "Iteration 14, loss = 0.60051156\n",
      "Iteration 15, loss = 0.58184833\n",
      "Iteration 16, loss = 0.65568036\n",
      "Iteration 17, loss = 0.54767598\n",
      "Iteration 18, loss = 0.63227045\n",
      "Iteration 19, loss = 0.54429527\n",
      "Iteration 20, loss = 0.57336933\n",
      "Iteration 21, loss = 0.58657082\n",
      "Iteration 22, loss = 0.54114489\n",
      "Iteration 23, loss = 0.53360099\n",
      "Iteration 24, loss = 0.52492525\n",
      "Iteration 25, loss = 0.54174308\n",
      "Iteration 26, loss = 0.53194947\n",
      "Iteration 27, loss = 0.61779994\n",
      "Iteration 28, loss = 0.53559484\n",
      "Iteration 29, loss = 0.59802004\n",
      "Iteration 30, loss = 0.60566410\n",
      "Iteration 31, loss = 0.53990768\n",
      "Iteration 32, loss = 0.64982950\n",
      "Iteration 33, loss = 0.57824441\n",
      "Iteration 34, loss = 0.52449131\n",
      "Iteration 35, loss = 0.52003566\n",
      "Iteration 36, loss = 0.53087972\n",
      "Iteration 37, loss = 0.52469057\n",
      "Iteration 38, loss = 0.55592760\n",
      "Iteration 39, loss = 0.56161782\n",
      "Iteration 40, loss = 0.51944536\n",
      "Iteration 41, loss = 0.56615908\n",
      "Iteration 42, loss = 0.57908095\n",
      "Iteration 43, loss = 0.50863641\n",
      "Iteration 44, loss = 0.53719268\n",
      "Iteration 45, loss = 0.55673438\n",
      "Iteration 46, loss = 0.53459069\n",
      "Iteration 47, loss = 0.52865763\n",
      "Iteration 48, loss = 0.68110504\n",
      "Iteration 49, loss = 0.50326651\n",
      "Iteration 50, loss = 0.50278566\n",
      "Iteration 51, loss = 0.49479787\n",
      "Iteration 52, loss = 0.50899628\n",
      "Iteration 53, loss = 0.52116143\n",
      "Iteration 54, loss = 0.53198987\n",
      "Iteration 55, loss = 0.50563355\n",
      "Iteration 56, loss = 0.50340819\n",
      "Iteration 57, loss = 0.54597886\n",
      "Iteration 58, loss = 0.55185533\n",
      "Iteration 59, loss = 0.50952970\n",
      "Iteration 60, loss = 0.57338595\n",
      "Iteration 61, loss = 0.51065087\n",
      "Iteration 62, loss = 0.49088957\n",
      "Iteration 63, loss = 0.50300143\n",
      "Iteration 64, loss = 0.47941122\n",
      "Iteration 65, loss = 0.49373505\n",
      "Iteration 66, loss = 0.47839027\n",
      "Iteration 67, loss = 0.48334599\n",
      "Iteration 68, loss = 0.47379468\n",
      "Iteration 69, loss = 0.51463664\n",
      "Iteration 70, loss = 0.47013684\n",
      "Iteration 71, loss = 0.46124775\n",
      "Iteration 72, loss = 0.46357196\n",
      "Iteration 73, loss = 0.55955555\n",
      "Iteration 74, loss = 0.47392512\n",
      "Iteration 75, loss = 0.47177070\n",
      "Iteration 76, loss = 0.48913908\n",
      "Iteration 77, loss = 0.48194250\n",
      "Iteration 78, loss = 0.45594950\n",
      "Iteration 79, loss = 0.47758033\n",
      "Iteration 80, loss = 0.46957858\n",
      "Iteration 81, loss = 0.45535309\n",
      "Iteration 82, loss = 0.50696165\n",
      "Iteration 83, loss = 0.46883527\n",
      "Iteration 84, loss = 0.46585741\n",
      "Iteration 85, loss = 0.47268640\n",
      "Iteration 86, loss = 0.45422378\n",
      "Iteration 87, loss = 0.44974230\n",
      "Iteration 88, loss = 0.43567449\n",
      "Iteration 89, loss = 0.44111183\n",
      "Iteration 90, loss = 0.44866197\n",
      "Iteration 91, loss = 0.43578890\n",
      "Iteration 92, loss = 0.42758226\n",
      "Iteration 93, loss = 0.42627079\n",
      "Iteration 94, loss = 0.44272375\n",
      "Iteration 95, loss = 0.42607345\n",
      "Iteration 96, loss = 0.55820674\n",
      "Iteration 97, loss = 0.48794661\n",
      "Iteration 98, loss = 0.44527265\n",
      "Iteration 99, loss = 0.41883079\n",
      "Iteration 100, loss = 0.46483135\n",
      "Iteration 101, loss = 0.43946824\n",
      "Iteration 102, loss = 0.46463840\n",
      "Iteration 103, loss = 0.42333154\n",
      "Iteration 104, loss = 0.40910576\n",
      "Iteration 105, loss = 0.41084708\n",
      "Iteration 106, loss = 0.47048155\n",
      "Iteration 107, loss = 0.56477746\n",
      "Iteration 108, loss = 0.47652739\n",
      "Iteration 109, loss = 0.41826080\n",
      "Iteration 110, loss = 0.41156982\n",
      "Iteration 111, loss = 0.40823641\n",
      "Iteration 112, loss = 0.40515704\n",
      "Iteration 113, loss = 0.39666881\n",
      "Iteration 114, loss = 0.39590864\n",
      "Iteration 115, loss = 0.39009070\n",
      "Iteration 116, loss = 0.39230222\n",
      "Iteration 117, loss = 0.39695770\n",
      "Iteration 118, loss = 0.45519792\n",
      "Iteration 119, loss = 0.41145422\n",
      "Iteration 120, loss = 0.41461529\n",
      "Iteration 121, loss = 0.57749427\n",
      "Iteration 122, loss = 0.44665969\n",
      "Iteration 123, loss = 0.45473580\n",
      "Iteration 124, loss = 0.42791950\n",
      "Iteration 125, loss = 0.60472043\n",
      "Iteration 126, loss = 0.42376142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.21399697\n",
      "Iteration 2, loss = 0.62954668\n",
      "Iteration 3, loss = 0.65316120\n",
      "Iteration 4, loss = 0.58579655\n",
      "Iteration 5, loss = 0.67100671\n",
      "Iteration 6, loss = 0.63010424\n",
      "Iteration 7, loss = 0.63639818\n",
      "Iteration 8, loss = 0.57223701\n",
      "Iteration 9, loss = 0.56904089\n",
      "Iteration 10, loss = 0.57593112\n",
      "Iteration 11, loss = 0.55316162\n",
      "Iteration 12, loss = 0.55768047\n",
      "Iteration 13, loss = 0.55616073\n",
      "Iteration 14, loss = 0.54562268\n",
      "Iteration 15, loss = 0.53846284\n",
      "Iteration 16, loss = 0.56090878\n",
      "Iteration 17, loss = 0.53355812\n",
      "Iteration 18, loss = 0.55661168\n",
      "Iteration 19, loss = 0.52629833\n",
      "Iteration 20, loss = 0.63340324\n",
      "Iteration 21, loss = 0.66987426\n",
      "Iteration 22, loss = 0.53739917\n",
      "Iteration 23, loss = 0.53193644\n",
      "Iteration 24, loss = 0.58287292\n",
      "Iteration 25, loss = 0.51728201\n",
      "Iteration 26, loss = 0.52989191\n",
      "Iteration 27, loss = 0.64536791\n",
      "Iteration 28, loss = 0.54178972\n",
      "Iteration 29, loss = 0.57043453\n",
      "Iteration 30, loss = 0.50259678\n",
      "Iteration 31, loss = 0.52835660\n",
      "Iteration 32, loss = 0.52790977\n",
      "Iteration 33, loss = 0.54122310\n",
      "Iteration 34, loss = 0.50433732\n",
      "Iteration 35, loss = 0.48928049\n",
      "Iteration 36, loss = 0.50198592\n",
      "Iteration 37, loss = 0.63178153\n",
      "Iteration 38, loss = 0.60084523\n",
      "Iteration 39, loss = 0.57591986\n",
      "Iteration 40, loss = 0.69987868\n",
      "Iteration 41, loss = 0.63858667\n",
      "Iteration 42, loss = 0.67988017\n",
      "Iteration 43, loss = 0.53104017\n",
      "Iteration 44, loss = 0.52408887\n",
      "Iteration 45, loss = 0.55045460\n",
      "Iteration 46, loss = 0.56128289\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.13718886\n",
      "Iteration 2, loss = 0.68313330\n",
      "Iteration 3, loss = 0.68371774\n",
      "Iteration 4, loss = 0.59079530\n",
      "Iteration 5, loss = 0.60316961\n",
      "Iteration 6, loss = 0.64436144\n",
      "Iteration 7, loss = 0.62524571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.60820612\n",
      "Iteration 9, loss = 0.73557372\n",
      "Iteration 10, loss = 0.57350856\n",
      "Iteration 11, loss = 0.58620927\n",
      "Iteration 12, loss = 0.57351423\n",
      "Iteration 13, loss = 0.65918335\n",
      "Iteration 14, loss = 0.58504586\n",
      "Iteration 15, loss = 0.63855433\n",
      "Iteration 16, loss = 0.67435531\n",
      "Iteration 17, loss = 0.58401106\n",
      "Iteration 18, loss = 0.64511634\n",
      "Iteration 19, loss = 0.60147589\n",
      "Iteration 20, loss = 0.59889935\n",
      "Iteration 21, loss = 0.64336986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.02191933\n",
      "Iteration 2, loss = 0.62995987\n",
      "Iteration 3, loss = 0.63130978\n",
      "Iteration 4, loss = 0.57860757\n",
      "Iteration 5, loss = 0.60309136\n",
      "Iteration 6, loss = 0.55989019\n",
      "Iteration 7, loss = 0.64388797\n",
      "Iteration 8, loss = 0.55495870\n",
      "Iteration 9, loss = 0.63727168\n",
      "Iteration 10, loss = 0.56073630\n",
      "Iteration 11, loss = 0.62158834\n",
      "Iteration 12, loss = 0.57224283\n",
      "Iteration 13, loss = 0.59125387\n",
      "Iteration 14, loss = 0.58375645\n",
      "Iteration 15, loss = 0.56404925\n",
      "Iteration 16, loss = 0.66513847\n",
      "Iteration 17, loss = 0.58953804\n",
      "Iteration 18, loss = 0.56311288\n",
      "Iteration 19, loss = 0.55792394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05196778\n",
      "Iteration 2, loss = 0.80096375\n",
      "Iteration 3, loss = 0.60758986\n",
      "Iteration 4, loss = 0.64974266\n",
      "Iteration 5, loss = 0.67413103\n",
      "Iteration 6, loss = 0.60385734\n",
      "Iteration 7, loss = 0.57060688\n",
      "Iteration 8, loss = 0.60021961\n",
      "Iteration 9, loss = 0.70631433\n",
      "Iteration 10, loss = 0.63557450\n",
      "Iteration 11, loss = 0.64835554\n",
      "Iteration 12, loss = 0.63626500\n",
      "Iteration 13, loss = 0.62362529\n",
      "Iteration 14, loss = 0.54772992\n",
      "Iteration 15, loss = 0.55341158\n",
      "Iteration 16, loss = 0.54884193\n",
      "Iteration 17, loss = 0.54080589\n",
      "Iteration 18, loss = 0.58548841\n",
      "Iteration 19, loss = 0.54992190\n",
      "Iteration 20, loss = 0.56068649\n",
      "Iteration 21, loss = 0.52287190\n",
      "Iteration 22, loss = 0.52014173\n",
      "Iteration 23, loss = 0.51853780\n",
      "Iteration 24, loss = 0.58223314\n",
      "Iteration 25, loss = 0.69963239\n",
      "Iteration 26, loss = 0.57974702\n",
      "Iteration 27, loss = 0.53937826\n",
      "Iteration 28, loss = 0.53531107\n",
      "Iteration 29, loss = 0.57201697\n",
      "Iteration 30, loss = 0.51640586\n",
      "Iteration 31, loss = 0.51460837\n",
      "Iteration 32, loss = 0.51283502\n",
      "Iteration 33, loss = 0.64108031\n",
      "Iteration 34, loss = 0.57960758\n",
      "Iteration 35, loss = 0.65662418\n",
      "Iteration 36, loss = 0.63091418\n",
      "Iteration 37, loss = 0.62869387\n",
      "Iteration 38, loss = 0.59837219\n",
      "Iteration 39, loss = 0.56950056\n",
      "Iteration 40, loss = 0.51947390\n",
      "Iteration 41, loss = 0.51333444\n",
      "Iteration 42, loss = 0.55096003\n",
      "Iteration 43, loss = 0.50341743\n",
      "Iteration 44, loss = 0.50031423\n",
      "Iteration 45, loss = 0.53663882\n",
      "Iteration 46, loss = 0.50757374\n",
      "Iteration 47, loss = 0.50936860\n",
      "Iteration 48, loss = 0.50616759\n",
      "Iteration 49, loss = 0.52100368\n",
      "Iteration 50, loss = 0.56153322\n",
      "Iteration 51, loss = 0.55467303\n",
      "Iteration 52, loss = 0.52085581\n",
      "Iteration 53, loss = 0.62077748\n",
      "Iteration 54, loss = 0.53168675\n",
      "Iteration 55, loss = 0.50024268\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99082306\n",
      "Iteration 2, loss = 0.67987696\n",
      "Iteration 3, loss = 0.62649780\n",
      "Iteration 4, loss = 0.60193399\n",
      "Iteration 5, loss = 0.58246138\n",
      "Iteration 6, loss = 0.56878413\n",
      "Iteration 7, loss = 0.56262718\n",
      "Iteration 8, loss = 0.55441367\n",
      "Iteration 9, loss = 0.54228695\n",
      "Iteration 10, loss = 0.53304866\n",
      "Iteration 11, loss = 0.52869419\n",
      "Iteration 12, loss = 0.51988463\n",
      "Iteration 13, loss = 0.51002730\n",
      "Iteration 14, loss = 0.51220894\n",
      "Iteration 15, loss = 0.49763785\n",
      "Iteration 16, loss = 0.49906177\n",
      "Iteration 17, loss = 0.48184502\n",
      "Iteration 18, loss = 0.47631396\n",
      "Iteration 19, loss = 0.46910924\n",
      "Iteration 20, loss = 0.45977440\n",
      "Iteration 21, loss = 0.45328904\n",
      "Iteration 22, loss = 0.44878174\n",
      "Iteration 23, loss = 0.43413651\n",
      "Iteration 24, loss = 0.43218968\n",
      "Iteration 25, loss = 0.42583665\n",
      "Iteration 26, loss = 0.41780796\n",
      "Iteration 27, loss = 0.40559296\n",
      "Iteration 28, loss = 0.39363221\n",
      "Iteration 29, loss = 0.39788351\n",
      "Iteration 30, loss = 0.39678609\n",
      "Iteration 31, loss = 0.38109660\n",
      "Iteration 32, loss = 0.37503413\n",
      "Iteration 33, loss = 0.35754901\n",
      "Iteration 34, loss = 0.37053571\n",
      "Iteration 35, loss = 0.36645759\n",
      "Iteration 36, loss = 0.34251951\n",
      "Iteration 37, loss = 0.34911618\n",
      "Iteration 38, loss = 0.33375065\n",
      "Iteration 39, loss = 0.32003212\n",
      "Iteration 40, loss = 0.31311254\n",
      "Iteration 41, loss = 0.30628869\n",
      "Iteration 42, loss = 0.30296768\n",
      "Iteration 43, loss = 0.30310038\n",
      "Iteration 44, loss = 0.31976579\n",
      "Iteration 45, loss = 0.29315599\n",
      "Iteration 46, loss = 0.28609748\n",
      "Iteration 47, loss = 0.29043275\n",
      "Iteration 48, loss = 0.32748350\n",
      "Iteration 49, loss = 0.29058639\n",
      "Iteration 50, loss = 0.26539905\n",
      "Iteration 51, loss = 0.25819851\n",
      "Iteration 52, loss = 0.24958741\n",
      "Iteration 53, loss = 0.26174070\n",
      "Iteration 54, loss = 0.26315385\n",
      "Iteration 55, loss = 0.24683954\n",
      "Iteration 56, loss = 0.24184588\n",
      "Iteration 57, loss = 0.23334069\n",
      "Iteration 58, loss = 0.23209208\n",
      "Iteration 59, loss = 0.22622107\n",
      "Iteration 60, loss = 0.22253824\n",
      "Iteration 61, loss = 0.21137727\n",
      "Iteration 62, loss = 0.20665039\n",
      "Iteration 63, loss = 0.20929368\n",
      "Iteration 64, loss = 0.20017942\n",
      "Iteration 65, loss = 0.19705977\n",
      "Iteration 66, loss = 0.19900673\n",
      "Iteration 67, loss = 0.20578903\n",
      "Iteration 68, loss = 0.23579662\n",
      "Iteration 69, loss = 0.22006118\n",
      "Iteration 70, loss = 0.18717956\n",
      "Iteration 71, loss = 0.17744319\n",
      "Iteration 72, loss = 0.17483232\n",
      "Iteration 73, loss = 0.16641001\n",
      "Iteration 74, loss = 0.16319093\n",
      "Iteration 75, loss = 0.17408455\n",
      "Iteration 76, loss = 0.16493914\n",
      "Iteration 77, loss = 0.15276029\n",
      "Iteration 78, loss = 0.15283479\n",
      "Iteration 79, loss = 0.14698897\n",
      "Iteration 80, loss = 0.17220755\n",
      "Iteration 81, loss = 0.18480597\n",
      "Iteration 82, loss = 0.16162489\n",
      "Iteration 83, loss = 0.14217564\n",
      "Iteration 84, loss = 0.13516724\n",
      "Iteration 85, loss = 0.13599287\n",
      "Iteration 86, loss = 0.12972956\n",
      "Iteration 87, loss = 0.13114169\n",
      "Iteration 88, loss = 0.14449015\n",
      "Iteration 89, loss = 0.13609547\n",
      "Iteration 90, loss = 0.13975858\n",
      "Iteration 91, loss = 0.11937792\n",
      "Iteration 92, loss = 0.11887993\n",
      "Iteration 93, loss = 0.13588033\n",
      "Iteration 94, loss = 0.11257997\n",
      "Iteration 95, loss = 0.11555517\n",
      "Iteration 96, loss = 0.10842007\n",
      "Iteration 97, loss = 0.10073203\n",
      "Iteration 98, loss = 0.09675696\n",
      "Iteration 99, loss = 0.09658887\n",
      "Iteration 100, loss = 0.09589676\n",
      "Iteration 101, loss = 0.10597815\n",
      "Iteration 102, loss = 0.09713731\n",
      "Iteration 103, loss = 0.09754426\n",
      "Iteration 104, loss = 0.09221694\n",
      "Iteration 105, loss = 0.09251988\n",
      "Iteration 106, loss = 0.08320443\n",
      "Iteration 107, loss = 0.08173277\n",
      "Iteration 108, loss = 0.07613985\n",
      "Iteration 109, loss = 0.07546403\n",
      "Iteration 110, loss = 0.07491933\n",
      "Iteration 111, loss = 0.07540155\n",
      "Iteration 112, loss = 0.07516582\n",
      "Iteration 113, loss = 0.07135800\n",
      "Iteration 114, loss = 0.06780910\n",
      "Iteration 115, loss = 0.06746377\n",
      "Iteration 116, loss = 0.06252792\n",
      "Iteration 117, loss = 0.06391060\n",
      "Iteration 118, loss = 0.06217862\n",
      "Iteration 119, loss = 0.07164316\n",
      "Iteration 120, loss = 0.07948737\n",
      "Iteration 121, loss = 0.06673408\n",
      "Iteration 122, loss = 0.05684692\n",
      "Iteration 123, loss = 0.05450279\n",
      "Iteration 124, loss = 0.05360636\n",
      "Iteration 125, loss = 0.05540750\n",
      "Iteration 126, loss = 0.05064235\n",
      "Iteration 127, loss = 0.04965016\n",
      "Iteration 128, loss = 0.04833512\n",
      "Iteration 129, loss = 0.04855422\n",
      "Iteration 130, loss = 0.04615493\n",
      "Iteration 131, loss = 0.04666774\n",
      "Iteration 132, loss = 0.04320450\n",
      "Iteration 133, loss = 0.04523282\n",
      "Iteration 134, loss = 0.04206938\n",
      "Iteration 135, loss = 0.04355716\n",
      "Iteration 136, loss = 0.04190414\n",
      "Iteration 137, loss = 0.03997535\n",
      "Iteration 138, loss = 0.03994183\n",
      "Iteration 139, loss = 0.04142349\n",
      "Iteration 140, loss = 0.03793408\n",
      "Iteration 141, loss = 0.03995845\n",
      "Iteration 142, loss = 0.04087837\n",
      "Iteration 143, loss = 0.04005550\n",
      "Iteration 144, loss = 0.03686403\n",
      "Iteration 145, loss = 0.03457012\n",
      "Iteration 146, loss = 0.03585429\n",
      "Iteration 147, loss = 0.03605311\n",
      "Iteration 148, loss = 0.03449684\n",
      "Iteration 149, loss = 0.03256089\n",
      "Iteration 150, loss = 0.03164942\n",
      "Iteration 151, loss = 0.03312824\n",
      "Iteration 152, loss = 0.02944314\n",
      "Iteration 153, loss = 0.02815766\n",
      "Iteration 154, loss = 0.02817409\n",
      "Iteration 155, loss = 0.02954572\n",
      "Iteration 156, loss = 0.02854063\n",
      "Iteration 157, loss = 0.02631559\n",
      "Iteration 158, loss = 0.02546034\n",
      "Iteration 159, loss = 0.02479281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 160, loss = 0.02430463\n",
      "Iteration 161, loss = 0.02394354\n",
      "Iteration 162, loss = 0.02351693\n",
      "Iteration 163, loss = 0.02431581\n",
      "Iteration 164, loss = 0.02356250\n",
      "Iteration 165, loss = 0.02290545\n",
      "Iteration 166, loss = 0.02256057\n",
      "Iteration 167, loss = 0.02223827\n",
      "Iteration 168, loss = 0.02152507\n",
      "Iteration 169, loss = 0.02240500\n",
      "Iteration 170, loss = 0.02346281\n",
      "Iteration 171, loss = 0.02102430\n",
      "Iteration 172, loss = 0.02082838\n",
      "Iteration 173, loss = 0.02010252\n",
      "Iteration 174, loss = 0.01910083\n",
      "Iteration 175, loss = 0.02016595\n",
      "Iteration 176, loss = 0.01845947\n",
      "Iteration 177, loss = 0.01780085\n",
      "Iteration 178, loss = 0.01757769\n",
      "Iteration 179, loss = 0.01771941\n",
      "Iteration 180, loss = 0.01910158\n",
      "Iteration 181, loss = 0.01868981\n",
      "Iteration 182, loss = 0.01739815\n",
      "Iteration 183, loss = 0.01685261\n",
      "Iteration 184, loss = 0.01610332\n",
      "Iteration 185, loss = 0.01611399\n",
      "Iteration 186, loss = 0.01608323\n",
      "Iteration 187, loss = 0.01632011\n",
      "Iteration 188, loss = 0.01498493\n",
      "Iteration 189, loss = 0.01546180\n",
      "Iteration 190, loss = 0.01538510\n",
      "Iteration 191, loss = 0.01453307\n",
      "Iteration 192, loss = 0.01493061\n",
      "Iteration 193, loss = 0.01436960\n",
      "Iteration 194, loss = 0.01370260\n",
      "Iteration 195, loss = 0.01374519\n",
      "Iteration 196, loss = 0.01379194\n",
      "Iteration 197, loss = 0.01335371\n",
      "Iteration 198, loss = 0.01310006\n",
      "Iteration 199, loss = 0.01381411\n",
      "Iteration 200, loss = 0.01356060\n",
      "Iteration 201, loss = 0.01313107\n",
      "Iteration 202, loss = 0.01246631\n",
      "Iteration 203, loss = 0.01216969\n",
      "Iteration 204, loss = 0.01185927\n",
      "Iteration 205, loss = 0.01187823\n",
      "Iteration 206, loss = 0.01218655\n",
      "Iteration 207, loss = 0.01156629\n",
      "Iteration 208, loss = 0.01219265\n",
      "Iteration 209, loss = 0.01221684\n",
      "Iteration 210, loss = 0.01094215\n",
      "Iteration 211, loss = 0.01116026\n",
      "Iteration 212, loss = 0.01089943\n",
      "Iteration 213, loss = 0.01051177\n",
      "Iteration 214, loss = 0.01060357\n",
      "Iteration 215, loss = 0.01062783\n",
      "Iteration 216, loss = 0.01017689\n",
      "Iteration 217, loss = 0.00976789\n",
      "Iteration 218, loss = 0.00988478\n",
      "Iteration 219, loss = 0.00986343\n",
      "Iteration 220, loss = 0.01054822\n",
      "Iteration 221, loss = 0.00987040\n",
      "Iteration 222, loss = 0.00928843\n",
      "Iteration 223, loss = 0.00935706\n",
      "Iteration 224, loss = 0.00941847\n",
      "Iteration 225, loss = 0.00910535\n",
      "Iteration 226, loss = 0.00890796\n",
      "Iteration 227, loss = 0.00871191\n",
      "Iteration 228, loss = 0.00858126\n",
      "Iteration 229, loss = 0.00851995\n",
      "Iteration 230, loss = 0.00852119\n",
      "Iteration 231, loss = 0.00835725\n",
      "Iteration 232, loss = 0.00839693\n",
      "Iteration 233, loss = 0.00832705\n",
      "Iteration 234, loss = 0.00846933\n",
      "Iteration 235, loss = 0.00826923\n",
      "Iteration 236, loss = 0.00825006\n",
      "Iteration 237, loss = 0.00806464\n",
      "Iteration 238, loss = 0.00786198\n",
      "Iteration 239, loss = 0.00761482\n",
      "Iteration 240, loss = 0.00746332\n",
      "Iteration 241, loss = 0.00758331\n",
      "Iteration 242, loss = 0.00748474\n",
      "Iteration 243, loss = 0.00747506\n",
      "Iteration 244, loss = 0.00777387\n",
      "Iteration 245, loss = 0.00752320\n",
      "Iteration 246, loss = 0.00769832\n",
      "Iteration 247, loss = 0.00746742\n",
      "Iteration 248, loss = 0.00713302\n",
      "Iteration 249, loss = 0.00686962\n",
      "Iteration 250, loss = 0.00664286\n",
      "Iteration 251, loss = 0.00663782\n",
      "Iteration 252, loss = 0.00662261\n",
      "Iteration 253, loss = 0.00659028\n",
      "Iteration 254, loss = 0.00638052\n",
      "Iteration 255, loss = 0.00640739\n",
      "Iteration 256, loss = 0.00624179\n",
      "Iteration 257, loss = 0.00617401\n",
      "Iteration 258, loss = 0.00610247\n",
      "Iteration 259, loss = 0.00605264\n",
      "Iteration 260, loss = 0.00603539\n",
      "Iteration 261, loss = 0.00628590\n",
      "Iteration 262, loss = 0.00615518\n",
      "Iteration 263, loss = 0.00589845\n",
      "Iteration 264, loss = 0.00597495\n",
      "Iteration 265, loss = 0.00582451\n",
      "Iteration 266, loss = 0.00583054\n",
      "Iteration 267, loss = 0.00579017\n",
      "Iteration 268, loss = 0.00553799\n",
      "Iteration 269, loss = 0.00548702\n",
      "Iteration 270, loss = 0.00540730\n",
      "Iteration 271, loss = 0.00564362\n",
      "Iteration 272, loss = 0.00548031\n",
      "Iteration 273, loss = 0.00533680\n",
      "Iteration 274, loss = 0.00530527\n",
      "Iteration 275, loss = 0.00515677\n",
      "Iteration 276, loss = 0.00521140\n",
      "Iteration 277, loss = 0.00507281\n",
      "Iteration 278, loss = 0.00505388\n",
      "Iteration 279, loss = 0.00502403\n",
      "Iteration 280, loss = 0.00498182\n",
      "Iteration 281, loss = 0.00501052\n",
      "Iteration 282, loss = 0.00489927\n",
      "Iteration 283, loss = 0.00481627\n",
      "Iteration 284, loss = 0.00475236\n",
      "Iteration 285, loss = 0.00469096\n",
      "Iteration 286, loss = 0.00466314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72470496\n",
      "Iteration 2, loss = 0.36683953\n",
      "Iteration 3, loss = 0.18244816\n",
      "Iteration 4, loss = 0.11013848\n",
      "Iteration 5, loss = 0.06883223\n",
      "Iteration 6, loss = 0.04643099\n",
      "Iteration 7, loss = 0.03385795\n",
      "Iteration 8, loss = 0.02513109\n",
      "Iteration 9, loss = 0.02023988\n",
      "Iteration 10, loss = 0.01637794\n",
      "Iteration 11, loss = 0.01393914\n",
      "Iteration 12, loss = 0.01205092\n",
      "Iteration 13, loss = 0.01066473\n",
      "Iteration 14, loss = 0.00957997\n",
      "Iteration 15, loss = 0.00868252\n",
      "Iteration 16, loss = 0.00798214\n",
      "Iteration 17, loss = 0.00734482\n",
      "Iteration 18, loss = 0.00680782\n",
      "Iteration 19, loss = 0.00634508\n",
      "Iteration 20, loss = 0.00593603\n",
      "Iteration 21, loss = 0.00556282\n",
      "Iteration 22, loss = 0.00523120\n",
      "Iteration 23, loss = 0.00493535\n",
      "Iteration 24, loss = 0.00466315\n",
      "Iteration 25, loss = 0.00440815\n",
      "Iteration 26, loss = 0.00418915\n",
      "Iteration 27, loss = 0.00397792\n",
      "Iteration 28, loss = 0.00378408\n",
      "Iteration 29, loss = 0.00361196\n",
      "Iteration 30, loss = 0.00344525\n",
      "Iteration 31, loss = 0.00329731\n",
      "Iteration 32, loss = 0.00315407\n",
      "Iteration 33, loss = 0.00302179\n",
      "Iteration 34, loss = 0.00290282\n",
      "Iteration 35, loss = 0.00278597\n",
      "Iteration 36, loss = 0.00268240\n",
      "Iteration 37, loss = 0.00257943\n",
      "Iteration 38, loss = 0.00248523\n",
      "Iteration 39, loss = 0.00239637\n",
      "Iteration 40, loss = 0.00231394\n",
      "Iteration 41, loss = 0.00223467\n",
      "Iteration 42, loss = 0.00216176\n",
      "Iteration 43, loss = 0.00209222\n",
      "Iteration 44, loss = 0.00202555\n",
      "Iteration 45, loss = 0.00196466\n",
      "Iteration 46, loss = 0.00190462\n",
      "Iteration 47, loss = 0.00184898\n",
      "Iteration 48, loss = 0.00179589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63950362\n",
      "Iteration 2, loss = 0.30714083\n",
      "Iteration 3, loss = 0.17104931\n",
      "Iteration 4, loss = 0.10351664\n",
      "Iteration 5, loss = 0.06554690\n",
      "Iteration 6, loss = 0.04310889\n",
      "Iteration 7, loss = 0.02931085\n",
      "Iteration 8, loss = 0.02157066\n",
      "Iteration 9, loss = 0.01694670\n",
      "Iteration 10, loss = 0.01313812\n",
      "Iteration 11, loss = 0.01077336\n",
      "Iteration 12, loss = 0.00919699\n",
      "Iteration 13, loss = 0.00792816\n",
      "Iteration 14, loss = 0.00700677\n",
      "Iteration 15, loss = 0.00623370\n",
      "Iteration 16, loss = 0.00566457\n",
      "Iteration 17, loss = 0.00517240\n",
      "Iteration 18, loss = 0.00475216\n",
      "Iteration 19, loss = 0.00439688\n",
      "Iteration 20, loss = 0.00409254\n",
      "Iteration 21, loss = 0.00381723\n",
      "Iteration 22, loss = 0.00356504\n",
      "Iteration 23, loss = 0.00335288\n",
      "Iteration 24, loss = 0.00316046\n",
      "Iteration 25, loss = 0.00298125\n",
      "Iteration 26, loss = 0.00282058\n",
      "Iteration 27, loss = 0.00267602\n",
      "Iteration 28, loss = 0.00254220\n",
      "Iteration 29, loss = 0.00242185\n",
      "Iteration 30, loss = 0.00230722\n",
      "Iteration 31, loss = 0.00220494\n",
      "Iteration 32, loss = 0.00210960\n",
      "Iteration 33, loss = 0.00202017\n",
      "Iteration 34, loss = 0.00193673\n",
      "Iteration 35, loss = 0.00186085\n",
      "Iteration 36, loss = 0.00178804\n",
      "Iteration 37, loss = 0.00172179\n",
      "Iteration 38, loss = 0.00165811\n",
      "Iteration 39, loss = 0.00159921\n",
      "Iteration 40, loss = 0.00154471\n",
      "Iteration 41, loss = 0.00149338\n",
      "Iteration 42, loss = 0.00144431\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65275501\n",
      "Iteration 2, loss = 0.34442638\n",
      "Iteration 3, loss = 0.25818471\n",
      "Iteration 4, loss = 0.14594267\n",
      "Iteration 5, loss = 0.15490896\n",
      "Iteration 6, loss = 0.18292400\n",
      "Iteration 7, loss = 0.19547343\n",
      "Iteration 8, loss = 0.13038976\n",
      "Iteration 9, loss = 0.12549338\n",
      "Iteration 10, loss = 0.07656967\n",
      "Iteration 11, loss = 0.05935382\n",
      "Iteration 12, loss = 0.04194756\n",
      "Iteration 13, loss = 0.07302525\n",
      "Iteration 14, loss = 0.11030853\n",
      "Iteration 15, loss = 0.06347450\n",
      "Iteration 16, loss = 0.04839383\n",
      "Iteration 17, loss = 0.08477877\n",
      "Iteration 18, loss = 0.14333661\n",
      "Iteration 19, loss = 0.09449841\n",
      "Iteration 20, loss = 0.05479308\n",
      "Iteration 21, loss = 0.03539027\n",
      "Iteration 22, loss = 0.02592196\n",
      "Iteration 23, loss = 0.01757711\n",
      "Iteration 24, loss = 0.01375805\n",
      "Iteration 25, loss = 0.01218049\n",
      "Iteration 26, loss = 0.01091720\n",
      "Iteration 27, loss = 0.00982348\n",
      "Iteration 28, loss = 0.00890805\n",
      "Iteration 29, loss = 0.00876393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.01059884\n",
      "Iteration 31, loss = 0.01047125\n",
      "Iteration 32, loss = 0.00832882\n",
      "Iteration 33, loss = 0.00725315\n",
      "Iteration 34, loss = 0.00708453\n",
      "Iteration 35, loss = 0.00763465\n",
      "Iteration 36, loss = 0.00799399\n",
      "Iteration 37, loss = 0.00752608\n",
      "Iteration 38, loss = 0.00671087\n",
      "Iteration 39, loss = 0.00594507\n",
      "Iteration 40, loss = 0.00552855\n",
      "Iteration 41, loss = 0.00517140\n",
      "Iteration 42, loss = 0.00489832\n",
      "Iteration 43, loss = 0.00464582\n",
      "Iteration 44, loss = 0.00441837\n",
      "Iteration 45, loss = 0.00423079\n",
      "Iteration 46, loss = 0.00407291\n",
      "Iteration 47, loss = 0.00392032\n",
      "Iteration 48, loss = 0.00378664\n",
      "Iteration 49, loss = 0.00367956\n",
      "Iteration 50, loss = 0.00360571\n",
      "Iteration 51, loss = 0.00351223\n",
      "Iteration 52, loss = 0.00339620\n",
      "Iteration 53, loss = 0.00328274\n",
      "Iteration 54, loss = 0.00317596\n",
      "Iteration 55, loss = 0.00308991\n",
      "Iteration 56, loss = 0.00300903\n",
      "Iteration 57, loss = 0.00294953\n",
      "Iteration 58, loss = 0.00287916\n",
      "Iteration 59, loss = 0.00279853\n",
      "Iteration 60, loss = 0.00277054\n",
      "Iteration 61, loss = 0.00272288\n",
      "Iteration 62, loss = 0.00265603\n",
      "Iteration 63, loss = 0.00258019\n",
      "Iteration 64, loss = 0.00251227\n",
      "Iteration 65, loss = 0.00253164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73366280\n",
      "Iteration 2, loss = 0.40117552\n",
      "Iteration 3, loss = 0.21125125\n",
      "Iteration 4, loss = 0.12769533\n",
      "Iteration 5, loss = 0.08657453\n",
      "Iteration 6, loss = 0.07125987\n",
      "Iteration 7, loss = 0.04798677\n",
      "Iteration 8, loss = 0.05661668\n",
      "Iteration 9, loss = 0.03988115\n",
      "Iteration 10, loss = 0.03567817\n",
      "Iteration 11, loss = 0.02398872\n",
      "Iteration 12, loss = 0.01838148\n",
      "Iteration 13, loss = 0.01640281\n",
      "Iteration 14, loss = 0.01529695\n",
      "Iteration 15, loss = 0.01230735\n",
      "Iteration 16, loss = 0.01023464\n",
      "Iteration 17, loss = 0.00905402\n",
      "Iteration 18, loss = 0.00810288\n",
      "Iteration 19, loss = 0.00909034\n",
      "Iteration 20, loss = 0.01367790\n",
      "Iteration 21, loss = 0.05455120\n",
      "Iteration 22, loss = 0.04891850\n",
      "Iteration 23, loss = 0.03124390\n",
      "Iteration 24, loss = 0.01726951\n",
      "Iteration 25, loss = 0.01004935\n",
      "Iteration 26, loss = 0.00800357\n",
      "Iteration 27, loss = 0.00740550\n",
      "Iteration 28, loss = 0.00573812\n",
      "Iteration 29, loss = 0.00464819\n",
      "Iteration 30, loss = 0.00418180\n",
      "Iteration 31, loss = 0.00384586\n",
      "Iteration 32, loss = 0.00358180\n",
      "Iteration 33, loss = 0.00343047\n",
      "Iteration 34, loss = 0.00323189\n",
      "Iteration 35, loss = 0.00331579\n",
      "Iteration 36, loss = 0.00369362\n",
      "Iteration 37, loss = 0.00318016\n",
      "Iteration 38, loss = 0.00281712\n",
      "Iteration 39, loss = 0.00268750\n",
      "Iteration 40, loss = 0.00260948\n",
      "Iteration 41, loss = 0.00263519\n",
      "Iteration 42, loss = 0.00284137\n",
      "Iteration 43, loss = 0.00333098\n",
      "Iteration 44, loss = 0.00367069\n",
      "Iteration 45, loss = 0.00251102\n",
      "Iteration 46, loss = 0.00231604\n",
      "Iteration 47, loss = 0.00222133\n",
      "Iteration 48, loss = 0.00207386\n",
      "Iteration 49, loss = 0.00194990\n",
      "Iteration 50, loss = 0.00187887\n",
      "Iteration 51, loss = 0.00182302\n",
      "Iteration 52, loss = 0.00175805\n",
      "Iteration 53, loss = 0.00169808\n",
      "Iteration 54, loss = 0.00166134\n",
      "Iteration 55, loss = 0.00169603\n",
      "Iteration 56, loss = 0.00166486\n",
      "Iteration 57, loss = 0.00162591\n",
      "Iteration 58, loss = 0.00163011\n",
      "Iteration 59, loss = 0.00160513\n",
      "Iteration 60, loss = 0.00153011\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70472646\n",
      "Iteration 2, loss = 0.38417571\n",
      "Iteration 3, loss = 0.36769664\n",
      "Iteration 4, loss = 0.24140156\n",
      "Iteration 5, loss = 0.22656418\n",
      "Iteration 6, loss = 0.23720148\n",
      "Iteration 7, loss = 0.26255903\n",
      "Iteration 8, loss = 0.20891835\n",
      "Iteration 9, loss = 0.15484946\n",
      "Iteration 10, loss = 0.11618521\n",
      "Iteration 11, loss = 0.09059277\n",
      "Iteration 12, loss = 0.07182705\n",
      "Iteration 13, loss = 0.06385248\n",
      "Iteration 14, loss = 0.05205125\n",
      "Iteration 15, loss = 0.08490941\n",
      "Iteration 16, loss = 0.10006973\n",
      "Iteration 17, loss = 0.07580114\n",
      "Iteration 18, loss = 0.07018908\n",
      "Iteration 19, loss = 0.05027339\n",
      "Iteration 20, loss = 0.06824521\n",
      "Iteration 21, loss = 0.04628831\n",
      "Iteration 22, loss = 0.03122374\n",
      "Iteration 23, loss = 0.02844811\n",
      "Iteration 24, loss = 0.02649304\n",
      "Iteration 25, loss = 0.02299388\n",
      "Iteration 26, loss = 0.02092483\n",
      "Iteration 27, loss = 0.02148549\n",
      "Iteration 28, loss = 0.02316368\n",
      "Iteration 29, loss = 0.01865372\n",
      "Iteration 30, loss = 0.01636984\n",
      "Iteration 31, loss = 0.01865666\n",
      "Iteration 32, loss = 0.01733120\n",
      "Iteration 33, loss = 0.01488517\n",
      "Iteration 34, loss = 0.01293070\n",
      "Iteration 35, loss = 0.01146538\n",
      "Iteration 36, loss = 0.01039061\n",
      "Iteration 37, loss = 0.00992393\n",
      "Iteration 38, loss = 0.01101843\n",
      "Iteration 39, loss = 0.01355448\n",
      "Iteration 40, loss = 0.01219642\n",
      "Iteration 41, loss = 0.00960898\n",
      "Iteration 42, loss = 0.00844759\n",
      "Iteration 43, loss = 0.00785273\n",
      "Iteration 44, loss = 0.00740395\n",
      "Iteration 45, loss = 0.00700210\n",
      "Iteration 46, loss = 0.00690831\n",
      "Iteration 47, loss = 0.00836592\n",
      "Iteration 48, loss = 0.00785642\n",
      "Iteration 49, loss = 0.00735856\n",
      "Iteration 50, loss = 0.00686280\n",
      "Iteration 51, loss = 0.00616208\n",
      "Iteration 52, loss = 0.00588928\n",
      "Iteration 53, loss = 0.00613477\n",
      "Iteration 54, loss = 0.00589981\n",
      "Iteration 55, loss = 0.00563417\n",
      "Iteration 56, loss = 0.00555187\n",
      "Iteration 57, loss = 0.00526754\n",
      "Iteration 58, loss = 0.00493630\n",
      "Iteration 59, loss = 0.00468898\n",
      "Iteration 60, loss = 0.00457176\n",
      "Iteration 61, loss = 0.00528812\n",
      "Iteration 62, loss = 0.00490771\n",
      "Iteration 63, loss = 0.00434344\n",
      "Iteration 64, loss = 0.00489574\n",
      "Iteration 65, loss = 0.00471460\n",
      "Iteration 66, loss = 0.00631079\n",
      "Iteration 67, loss = 0.00690280\n",
      "Iteration 68, loss = 0.00845191\n",
      "Iteration 69, loss = 0.01268759\n",
      "Iteration 70, loss = 0.00841578\n",
      "Iteration 71, loss = 0.00593417\n",
      "Iteration 72, loss = 0.00472816\n",
      "Iteration 73, loss = 0.00427282\n",
      "Iteration 74, loss = 0.00393918\n",
      "Iteration 75, loss = 0.00370433\n",
      "Iteration 76, loss = 0.00349827\n",
      "Iteration 77, loss = 0.00356684\n",
      "Iteration 78, loss = 0.00358570\n",
      "Iteration 79, loss = 0.00329909\n",
      "Iteration 80, loss = 0.00303141\n",
      "Iteration 81, loss = 0.00287958\n",
      "Iteration 82, loss = 0.00285255\n",
      "Iteration 83, loss = 0.00279676\n",
      "Iteration 84, loss = 0.00272155\n",
      "Iteration 85, loss = 0.00263701\n",
      "Iteration 86, loss = 0.00254884\n",
      "Iteration 87, loss = 0.00246990\n",
      "Iteration 88, loss = 0.00243747\n",
      "Iteration 89, loss = 0.00248364\n",
      "Iteration 90, loss = 0.00249540\n",
      "Iteration 91, loss = 0.00244278\n",
      "Iteration 92, loss = 0.00235413\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70377458\n",
      "Iteration 2, loss = 0.33489918\n",
      "Iteration 3, loss = 0.19405059\n",
      "Iteration 4, loss = 0.11573406\n",
      "Iteration 5, loss = 0.09821800\n",
      "Iteration 6, loss = 0.07133658\n",
      "Iteration 7, loss = 0.04497783\n",
      "Iteration 8, loss = 0.03612975\n",
      "Iteration 9, loss = 0.02789686\n",
      "Iteration 10, loss = 0.02539201\n",
      "Iteration 11, loss = 0.02286407\n",
      "Iteration 12, loss = 0.02324988\n",
      "Iteration 13, loss = 0.01707656\n",
      "Iteration 14, loss = 0.01558145\n",
      "Iteration 15, loss = 0.01259646\n",
      "Iteration 16, loss = 0.01336697\n",
      "Iteration 17, loss = 0.01931482\n",
      "Iteration 18, loss = 0.02456077\n",
      "Iteration 19, loss = 0.01352986\n",
      "Iteration 20, loss = 0.01101544\n",
      "Iteration 21, loss = 0.02479966\n",
      "Iteration 22, loss = 0.04183269\n",
      "Iteration 23, loss = 0.02653085\n",
      "Iteration 24, loss = 0.01340267\n",
      "Iteration 25, loss = 0.01030866\n",
      "Iteration 26, loss = 0.00656192\n",
      "Iteration 27, loss = 0.00542789\n",
      "Iteration 28, loss = 0.00448296\n",
      "Iteration 29, loss = 0.00402458\n",
      "Iteration 30, loss = 0.00363282\n",
      "Iteration 31, loss = 0.00335897\n",
      "Iteration 32, loss = 0.00316258\n",
      "Iteration 33, loss = 0.00301233\n",
      "Iteration 34, loss = 0.00285895\n",
      "Iteration 35, loss = 0.00272032\n",
      "Iteration 36, loss = 0.00258464\n",
      "Iteration 37, loss = 0.00246890\n",
      "Iteration 38, loss = 0.00249005\n",
      "Iteration 39, loss = 0.00269100\n",
      "Iteration 40, loss = 0.00258138\n",
      "Iteration 41, loss = 0.00231992\n",
      "Iteration 42, loss = 0.00214280\n",
      "Iteration 43, loss = 0.00205040\n",
      "Iteration 44, loss = 0.00196989\n",
      "Iteration 45, loss = 0.00190376\n",
      "Iteration 46, loss = 0.00184998\n",
      "Iteration 47, loss = 0.00179672\n",
      "Iteration 48, loss = 0.00173398\n",
      "Iteration 49, loss = 0.00171482\n",
      "Iteration 50, loss = 0.00172975\n",
      "Iteration 51, loss = 0.00168035\n",
      "Iteration 52, loss = 0.00159468\n",
      "Iteration 53, loss = 0.00152846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71507066\n",
      "Iteration 2, loss = 0.33267209\n",
      "Iteration 3, loss = 0.20421864\n",
      "Iteration 4, loss = 0.12690152\n",
      "Iteration 5, loss = 0.11821889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.15205392\n",
      "Iteration 7, loss = 0.10057294\n",
      "Iteration 8, loss = 0.08075171\n",
      "Iteration 9, loss = 0.05265246\n",
      "Iteration 10, loss = 0.04003459\n",
      "Iteration 11, loss = 0.03035750\n",
      "Iteration 12, loss = 0.02301638\n",
      "Iteration 13, loss = 0.01922031\n",
      "Iteration 14, loss = 0.01719024\n",
      "Iteration 15, loss = 0.01559877\n",
      "Iteration 16, loss = 0.01477821\n",
      "Iteration 17, loss = 0.01269072\n",
      "Iteration 18, loss = 0.01146119\n",
      "Iteration 19, loss = 0.01035094\n",
      "Iteration 20, loss = 0.00957728\n",
      "Iteration 21, loss = 0.00901510\n",
      "Iteration 22, loss = 0.00858071\n",
      "Iteration 23, loss = 0.01084759\n",
      "Iteration 24, loss = 0.00938260\n",
      "Iteration 25, loss = 0.00728833\n",
      "Iteration 26, loss = 0.00641467\n",
      "Iteration 27, loss = 0.00569930\n",
      "Iteration 28, loss = 0.00597365\n",
      "Iteration 29, loss = 0.00672132\n",
      "Iteration 30, loss = 0.00552105\n",
      "Iteration 31, loss = 0.00507667\n",
      "Iteration 32, loss = 0.00495391\n",
      "Iteration 33, loss = 0.00524160\n",
      "Iteration 34, loss = 0.00640223\n",
      "Iteration 35, loss = 0.00928614\n",
      "Iteration 36, loss = 0.01506150\n",
      "Iteration 37, loss = 0.00733504\n",
      "Iteration 38, loss = 0.02535375\n",
      "Iteration 39, loss = 0.02714618\n",
      "Iteration 40, loss = 0.01423457\n",
      "Iteration 41, loss = 0.01291835\n",
      "Iteration 42, loss = 0.00540286\n",
      "Iteration 43, loss = 0.00535912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65790655\n",
      "Iteration 2, loss = 0.34763505\n",
      "Iteration 3, loss = 0.29091268\n",
      "Iteration 4, loss = 0.20299853\n",
      "Iteration 5, loss = 0.13208639\n",
      "Iteration 6, loss = 0.10048267\n",
      "Iteration 7, loss = 0.06909773\n",
      "Iteration 8, loss = 0.05383804\n",
      "Iteration 9, loss = 0.04058382\n",
      "Iteration 10, loss = 0.03127505\n",
      "Iteration 11, loss = 0.02706798\n",
      "Iteration 12, loss = 0.02288512\n",
      "Iteration 13, loss = 0.01896487\n",
      "Iteration 14, loss = 0.01711363\n",
      "Iteration 15, loss = 0.01539613\n",
      "Iteration 16, loss = 0.01353027\n",
      "Iteration 17, loss = 0.01176394\n",
      "Iteration 18, loss = 0.01031910\n",
      "Iteration 19, loss = 0.01155233\n",
      "Iteration 20, loss = 0.01466797\n",
      "Iteration 21, loss = 0.00970997\n",
      "Iteration 22, loss = 0.00856644\n",
      "Iteration 23, loss = 0.00728320\n",
      "Iteration 24, loss = 0.00705812\n",
      "Iteration 25, loss = 0.00661152\n",
      "Iteration 26, loss = 0.00610688\n",
      "Iteration 27, loss = 0.01017340\n",
      "Iteration 28, loss = 0.01287127\n",
      "Iteration 29, loss = 0.00716510\n",
      "Iteration 30, loss = 0.00685707\n",
      "Iteration 31, loss = 0.00654620\n",
      "Iteration 32, loss = 0.00484631\n",
      "Iteration 33, loss = 0.00428571\n",
      "Iteration 34, loss = 0.00376882\n",
      "Iteration 35, loss = 0.00339565\n",
      "Iteration 36, loss = 0.00315338\n",
      "Iteration 37, loss = 0.00303729\n",
      "Iteration 38, loss = 0.00302051\n",
      "Iteration 39, loss = 0.00285435\n",
      "Iteration 40, loss = 0.00272526\n",
      "Iteration 41, loss = 0.00278534\n",
      "Iteration 42, loss = 0.00292612\n",
      "Iteration 43, loss = 0.00272980\n",
      "Iteration 44, loss = 0.00243534\n",
      "Iteration 45, loss = 0.00223499\n",
      "Iteration 46, loss = 0.00210210\n",
      "Iteration 47, loss = 0.00230663\n",
      "Iteration 48, loss = 0.00238378\n",
      "Iteration 49, loss = 0.00225425\n",
      "Iteration 50, loss = 0.00238327\n",
      "Iteration 51, loss = 0.00212455\n",
      "Iteration 52, loss = 0.00192903\n",
      "Iteration 53, loss = 0.00181951\n",
      "Iteration 54, loss = 0.00172921\n",
      "Iteration 55, loss = 0.00165889\n",
      "Iteration 56, loss = 0.00161142\n",
      "Iteration 57, loss = 0.00158097\n",
      "Iteration 58, loss = 0.00215788\n",
      "Iteration 59, loss = 0.00202593\n",
      "Iteration 60, loss = 0.00159618\n",
      "Iteration 61, loss = 0.00145728\n",
      "Iteration 62, loss = 0.00139339\n",
      "Iteration 63, loss = 0.00132834\n",
      "Iteration 64, loss = 0.00127985\n",
      "Iteration 65, loss = 0.00124747\n",
      "Iteration 66, loss = 0.00122090\n",
      "Iteration 67, loss = 0.00118487\n",
      "Iteration 68, loss = 0.00114748\n",
      "Iteration 69, loss = 0.00111546\n",
      "Iteration 70, loss = 0.00109020\n",
      "Iteration 71, loss = 0.00106804\n",
      "Iteration 72, loss = 0.00104696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66329058\n",
      "Iteration 2, loss = 0.42388505\n",
      "Iteration 3, loss = 0.30982443\n",
      "Iteration 4, loss = 0.21198608\n",
      "Iteration 5, loss = 0.16245307\n",
      "Iteration 6, loss = 0.18298881\n",
      "Iteration 7, loss = 0.11187539\n",
      "Iteration 8, loss = 0.08871044\n",
      "Iteration 9, loss = 0.06787818\n",
      "Iteration 10, loss = 0.06501327\n",
      "Iteration 11, loss = 0.09148285\n",
      "Iteration 12, loss = 0.06868420\n",
      "Iteration 13, loss = 0.05451891\n",
      "Iteration 14, loss = 0.04215157\n",
      "Iteration 15, loss = 0.03096230\n",
      "Iteration 16, loss = 0.02532848\n",
      "Iteration 17, loss = 0.02160142\n",
      "Iteration 18, loss = 0.01804036\n",
      "Iteration 19, loss = 0.01590197\n",
      "Iteration 20, loss = 0.01527540\n",
      "Iteration 21, loss = 0.01482740\n",
      "Iteration 22, loss = 0.01417565\n",
      "Iteration 23, loss = 0.01600655\n",
      "Iteration 24, loss = 0.01460077\n",
      "Iteration 25, loss = 0.01511631\n",
      "Iteration 26, loss = 0.01101161\n",
      "Iteration 27, loss = 0.00972259\n",
      "Iteration 28, loss = 0.00847847\n",
      "Iteration 29, loss = 0.00750701\n",
      "Iteration 30, loss = 0.00743793\n",
      "Iteration 31, loss = 0.00803343\n",
      "Iteration 32, loss = 0.00699829\n",
      "Iteration 33, loss = 0.00611541\n",
      "Iteration 34, loss = 0.00569164\n",
      "Iteration 35, loss = 0.00536269\n",
      "Iteration 36, loss = 0.00508661\n",
      "Iteration 37, loss = 0.00484132\n",
      "Iteration 38, loss = 0.00480445\n",
      "Iteration 39, loss = 0.00562737\n",
      "Iteration 40, loss = 0.00973405\n",
      "Iteration 41, loss = 0.00578092\n",
      "Iteration 42, loss = 0.00525046\n",
      "Iteration 43, loss = 0.00452575\n",
      "Iteration 44, loss = 0.00405139\n",
      "Iteration 45, loss = 0.00395037\n",
      "Iteration 46, loss = 0.00428929\n",
      "Iteration 47, loss = 0.00378005\n",
      "Iteration 48, loss = 0.00342300\n",
      "Iteration 49, loss = 0.00319744\n",
      "Iteration 50, loss = 0.00308970\n",
      "Iteration 51, loss = 0.00298905\n",
      "Iteration 52, loss = 0.00282959\n",
      "Iteration 53, loss = 0.00273643\n",
      "Iteration 54, loss = 0.00265586\n",
      "Iteration 55, loss = 0.00254200\n",
      "Iteration 56, loss = 0.00243736\n",
      "Iteration 57, loss = 0.00238357\n",
      "Iteration 58, loss = 0.00235648\n",
      "Iteration 59, loss = 0.00229844\n",
      "Iteration 60, loss = 0.00221366\n",
      "Iteration 61, loss = 0.00215688\n",
      "Iteration 62, loss = 0.00225529\n",
      "Iteration 63, loss = 0.00269827\n",
      "Iteration 64, loss = 0.00255673\n",
      "Iteration 65, loss = 0.00221989\n",
      "Iteration 66, loss = 0.00205330\n",
      "Iteration 67, loss = 0.00195690\n",
      "Iteration 68, loss = 0.00187105\n",
      "Iteration 69, loss = 0.00180596\n",
      "Iteration 70, loss = 0.00175021\n",
      "Iteration 71, loss = 0.00172210\n",
      "Iteration 72, loss = 0.00169293\n",
      "Iteration 73, loss = 0.00165552\n",
      "Iteration 74, loss = 0.00161738\n",
      "Iteration 75, loss = 0.00160393\n",
      "Iteration 76, loss = 0.00185108\n",
      "Iteration 77, loss = 0.00181330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62737776\n",
      "Iteration 2, loss = 0.28989278\n",
      "Iteration 3, loss = 0.17910565\n",
      "Iteration 4, loss = 0.14891088\n",
      "Iteration 5, loss = 0.16259687\n",
      "Iteration 6, loss = 0.12371043\n",
      "Iteration 7, loss = 0.08987215\n",
      "Iteration 8, loss = 0.07513470\n",
      "Iteration 9, loss = 0.13267177\n",
      "Iteration 10, loss = 0.10242735\n",
      "Iteration 11, loss = 0.08252149\n",
      "Iteration 12, loss = 0.13535752\n",
      "Iteration 13, loss = inf\n",
      "Iteration 14, loss = inf\n",
      "Iteration 15, loss = 0.14870136\n",
      "Iteration 16, loss = 0.10460286\n",
      "Iteration 17, loss = 0.08103821\n",
      "Iteration 18, loss = 0.05532567\n",
      "Iteration 19, loss = 0.04412237\n",
      "Iteration 20, loss = 0.03438087\n",
      "Iteration 21, loss = 0.02671180\n",
      "Iteration 22, loss = 0.02328333\n",
      "Iteration 23, loss = 0.01814745\n",
      "Iteration 24, loss = 0.01534073\n",
      "Iteration 25, loss = 0.01287241\n",
      "Iteration 26, loss = 0.00970298\n",
      "Iteration 27, loss = 0.00863006\n",
      "Iteration 28, loss = 0.00816116\n",
      "Iteration 29, loss = 0.00753264\n",
      "Iteration 30, loss = 0.00684627\n",
      "Iteration 31, loss = 0.00638787\n",
      "Iteration 32, loss = 0.00603556\n",
      "Iteration 33, loss = 0.00572490\n",
      "Iteration 34, loss = 0.00542402\n",
      "Iteration 35, loss = 0.00524102\n",
      "Iteration 36, loss = 0.00516151\n",
      "Iteration 37, loss = 0.00492950\n",
      "Iteration 38, loss = 0.00465138\n",
      "Iteration 39, loss = 0.00444335\n",
      "Iteration 40, loss = 0.00428903\n",
      "Iteration 41, loss = 0.00445399\n",
      "Iteration 42, loss = 0.00559886\n",
      "Iteration 43, loss = 0.00503940\n",
      "Iteration 44, loss = 0.00426643\n",
      "Iteration 45, loss = 0.00388644\n",
      "Iteration 46, loss = 0.00382356\n",
      "Iteration 47, loss = 0.00368896\n",
      "Iteration 48, loss = 0.00349789\n",
      "Iteration 49, loss = 0.00335419\n",
      "Iteration 50, loss = 0.00337699\n",
      "Iteration 51, loss = 0.00356889\n",
      "Iteration 52, loss = 0.00338955\n",
      "Iteration 53, loss = 0.00311252\n",
      "Iteration 54, loss = 0.00293016\n",
      "Iteration 55, loss = 0.00280679\n",
      "Iteration 56, loss = 0.00278892\n",
      "Iteration 57, loss = 0.00297696\n",
      "Iteration 58, loss = 0.00285882\n",
      "Iteration 59, loss = 0.00257884\n",
      "Iteration 60, loss = 0.00246779\n",
      "Iteration 61, loss = 0.00238667\n",
      "Iteration 62, loss = 0.00231956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63, loss = 0.00234543\n",
      "Iteration 64, loss = 0.00250197\n",
      "Iteration 65, loss = 0.00253432\n",
      "Iteration 66, loss = 0.00244753\n",
      "Iteration 67, loss = 0.00241796\n",
      "Iteration 68, loss = 0.00271027\n",
      "Iteration 69, loss = 0.00286829\n",
      "Iteration 70, loss = 0.00263798\n",
      "Iteration 71, loss = 0.00238319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75726139\n",
      "Iteration 2, loss = 0.36957858\n",
      "Iteration 3, loss = 0.19286450\n",
      "Iteration 4, loss = 0.10617032\n",
      "Iteration 5, loss = 0.06541759\n",
      "Iteration 6, loss = 0.04529076\n",
      "Iteration 7, loss = 0.03318912\n",
      "Iteration 8, loss = 0.02497587\n",
      "Iteration 9, loss = 0.02020144\n",
      "Iteration 10, loss = 0.01694448\n",
      "Iteration 11, loss = 0.01439670\n",
      "Iteration 12, loss = 0.01247095\n",
      "Iteration 13, loss = 0.01110706\n",
      "Iteration 14, loss = 0.00994594\n",
      "Iteration 15, loss = 0.00902975\n",
      "Iteration 16, loss = 0.00825415\n",
      "Iteration 17, loss = 0.00755251\n",
      "Iteration 18, loss = 0.00700249\n",
      "Iteration 19, loss = 0.00649905\n",
      "Iteration 20, loss = 0.00603131\n",
      "Iteration 21, loss = 0.00564206\n",
      "Iteration 22, loss = 0.00527657\n",
      "Iteration 23, loss = 0.00494528\n",
      "Iteration 24, loss = 0.00465587\n",
      "Iteration 25, loss = 0.00439288\n",
      "Iteration 26, loss = 0.00415953\n",
      "Iteration 27, loss = 0.00392761\n",
      "Iteration 28, loss = 0.00372892\n",
      "Iteration 29, loss = 0.00354587\n",
      "Iteration 30, loss = 0.00337485\n",
      "Iteration 31, loss = 0.00322041\n",
      "Iteration 32, loss = 0.00307509\n",
      "Iteration 33, loss = 0.00293980\n",
      "Iteration 34, loss = 0.00281689\n",
      "Iteration 35, loss = 0.00270032\n",
      "Iteration 36, loss = 0.00259500\n",
      "Iteration 37, loss = 0.00249470\n",
      "Iteration 38, loss = 0.00239950\n",
      "Iteration 39, loss = 0.00231039\n",
      "Iteration 40, loss = 0.00222920\n",
      "Iteration 41, loss = 0.00214791\n",
      "Iteration 42, loss = 0.00207537\n",
      "Iteration 43, loss = 0.00200611\n",
      "Iteration 44, loss = 0.00194138\n",
      "Iteration 45, loss = 0.00188045\n",
      "Iteration 46, loss = 0.00182326\n",
      "Iteration 47, loss = 0.00176777\n",
      "Iteration 48, loss = 0.00171643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06354003\n",
      "Iteration 2, loss = 0.70925005\n",
      "Iteration 3, loss = 0.71419121\n",
      "Iteration 4, loss = 0.71279442\n",
      "Iteration 5, loss = 0.64557230\n",
      "Iteration 6, loss = 0.65301365\n",
      "Iteration 7, loss = 0.65799574\n",
      "Iteration 8, loss = 0.60163795\n",
      "Iteration 9, loss = 0.62325011\n",
      "Iteration 10, loss = 0.62935386\n",
      "Iteration 11, loss = 0.60454252\n",
      "Iteration 12, loss = 0.58361373\n",
      "Iteration 13, loss = 0.57081453\n",
      "Iteration 14, loss = 0.57055088\n",
      "Iteration 15, loss = 0.54950291\n",
      "Iteration 16, loss = 0.54801435\n",
      "Iteration 17, loss = 0.56248219\n",
      "Iteration 18, loss = 0.54305905\n",
      "Iteration 19, loss = 0.52772441\n",
      "Iteration 20, loss = 0.54351993\n",
      "Iteration 21, loss = 0.59756065\n",
      "Iteration 22, loss = 0.56271448\n",
      "Iteration 23, loss = 0.52564408\n",
      "Iteration 24, loss = 0.51796670\n",
      "Iteration 25, loss = 0.50713118\n",
      "Iteration 26, loss = 0.51458570\n",
      "Iteration 27, loss = 0.57440388\n",
      "Iteration 28, loss = 0.58625999\n",
      "Iteration 29, loss = 0.50172433\n",
      "Iteration 30, loss = 0.49167702\n",
      "Iteration 31, loss = 0.58588367\n",
      "Iteration 32, loss = 0.58711210\n",
      "Iteration 33, loss = 0.52566755\n",
      "Iteration 34, loss = 0.48791370\n",
      "Iteration 35, loss = 0.48881430\n",
      "Iteration 36, loss = 0.48753170\n",
      "Iteration 37, loss = 0.56605155\n",
      "Iteration 38, loss = 0.51891304\n",
      "Iteration 39, loss = 0.51378579\n",
      "Iteration 40, loss = 0.48555760\n",
      "Iteration 41, loss = 0.50170842\n",
      "Iteration 42, loss = 0.52803359\n",
      "Iteration 43, loss = 0.48179540\n",
      "Iteration 44, loss = 0.49940513\n",
      "Iteration 45, loss = 0.56375543\n",
      "Iteration 46, loss = 0.56697013\n",
      "Iteration 47, loss = 0.51019400\n",
      "Iteration 48, loss = 0.47065631\n",
      "Iteration 49, loss = 0.58605774\n",
      "Iteration 50, loss = 0.56928631\n",
      "Iteration 51, loss = 0.50842457\n",
      "Iteration 52, loss = 0.49805212\n",
      "Iteration 53, loss = 0.46073340\n",
      "Iteration 54, loss = 0.54143842\n",
      "Iteration 55, loss = 0.51952889\n",
      "Iteration 56, loss = 0.47162118\n",
      "Iteration 57, loss = 0.46781021\n",
      "Iteration 58, loss = 0.46866018\n",
      "Iteration 59, loss = 0.45075009\n",
      "Iteration 60, loss = 0.46379036\n",
      "Iteration 61, loss = 0.47010114\n",
      "Iteration 62, loss = 0.45614691\n",
      "Iteration 63, loss = 0.44482255\n",
      "Iteration 64, loss = 0.43416190\n",
      "Iteration 65, loss = 0.46352679\n",
      "Iteration 66, loss = 0.55028939\n",
      "Iteration 67, loss = 0.63180072\n",
      "Iteration 68, loss = 0.49765765\n",
      "Iteration 69, loss = 0.43908559\n",
      "Iteration 70, loss = 0.45073297\n",
      "Iteration 71, loss = 0.45031127\n",
      "Iteration 72, loss = 0.43142855\n",
      "Iteration 73, loss = 0.42318248\n",
      "Iteration 74, loss = 0.43208570\n",
      "Iteration 75, loss = 0.42443238\n",
      "Iteration 76, loss = 0.42905066\n",
      "Iteration 77, loss = 0.45795296\n",
      "Iteration 78, loss = 0.44353966\n",
      "Iteration 79, loss = 0.44199267\n",
      "Iteration 80, loss = 0.41710669\n",
      "Iteration 81, loss = 0.40584356\n",
      "Iteration 82, loss = 0.42984452\n",
      "Iteration 83, loss = 0.41415158\n",
      "Iteration 84, loss = 0.58604794\n",
      "Iteration 85, loss = 0.51202615\n",
      "Iteration 86, loss = 0.50209678\n",
      "Iteration 87, loss = 0.42628201\n",
      "Iteration 88, loss = 0.41697076\n",
      "Iteration 89, loss = 0.44022786\n",
      "Iteration 90, loss = 0.46744480\n",
      "Iteration 91, loss = 0.45829906\n",
      "Iteration 92, loss = 0.41893127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.15476765\n",
      "Iteration 2, loss = 0.70707844\n",
      "Iteration 3, loss = 0.62490528\n",
      "Iteration 4, loss = 0.61033270\n",
      "Iteration 5, loss = 0.62780128\n",
      "Iteration 6, loss = 0.60192291\n",
      "Iteration 7, loss = 0.57282539\n",
      "Iteration 8, loss = 0.59257399\n",
      "Iteration 9, loss = 0.58270519\n",
      "Iteration 10, loss = 0.61000769\n",
      "Iteration 11, loss = 0.57906324\n",
      "Iteration 12, loss = 0.58487414\n",
      "Iteration 13, loss = 0.55102857\n",
      "Iteration 14, loss = 0.61290208\n",
      "Iteration 15, loss = 0.54242646\n",
      "Iteration 16, loss = 0.51685148\n",
      "Iteration 17, loss = 0.51315107\n",
      "Iteration 18, loss = 0.51339739\n",
      "Iteration 19, loss = 0.53299540\n",
      "Iteration 20, loss = 0.52810684\n",
      "Iteration 21, loss = 0.49628789\n",
      "Iteration 22, loss = 0.51082943\n",
      "Iteration 23, loss = 0.50383767\n",
      "Iteration 24, loss = 0.59338596\n",
      "Iteration 25, loss = 0.62674284\n",
      "Iteration 26, loss = 0.59691132\n",
      "Iteration 27, loss = 0.56303892\n",
      "Iteration 28, loss = 0.53665110\n",
      "Iteration 29, loss = 0.50163389\n",
      "Iteration 30, loss = 0.49334070\n",
      "Iteration 31, loss = 0.49069296\n",
      "Iteration 32, loss = 0.49327692\n",
      "Iteration 33, loss = 0.48131004\n",
      "Iteration 34, loss = 0.47318259\n",
      "Iteration 35, loss = 0.46863401\n",
      "Iteration 36, loss = 0.52171526\n",
      "Iteration 37, loss = 0.47842211\n",
      "Iteration 38, loss = 0.46604513\n",
      "Iteration 39, loss = 0.48954395\n",
      "Iteration 40, loss = 0.56091772\n",
      "Iteration 41, loss = 0.57619206\n",
      "Iteration 42, loss = 0.59022546\n",
      "Iteration 43, loss = 0.53811447\n",
      "Iteration 44, loss = 0.51887655\n",
      "Iteration 45, loss = 0.51051894\n",
      "Iteration 46, loss = 0.47552597\n",
      "Iteration 47, loss = 0.46456802\n",
      "Iteration 48, loss = 0.45523231\n",
      "Iteration 49, loss = 0.46153976\n",
      "Iteration 50, loss = 0.46743494\n",
      "Iteration 51, loss = 0.47563115\n",
      "Iteration 52, loss = 0.52989093\n",
      "Iteration 53, loss = 0.45977901\n",
      "Iteration 54, loss = 0.44293570\n",
      "Iteration 55, loss = 0.43738618\n",
      "Iteration 56, loss = 0.43357143\n",
      "Iteration 57, loss = 0.60173283\n",
      "Iteration 58, loss = 0.47941925\n",
      "Iteration 59, loss = 0.49032364\n",
      "Iteration 60, loss = 0.43809228\n",
      "Iteration 61, loss = 0.48482981\n",
      "Iteration 62, loss = 0.48198003\n",
      "Iteration 63, loss = 0.43864104\n",
      "Iteration 64, loss = 0.43014794\n",
      "Iteration 65, loss = 0.55986756\n",
      "Iteration 66, loss = 0.51019516\n",
      "Iteration 67, loss = 0.44051500\n",
      "Iteration 68, loss = 0.43917081\n",
      "Iteration 69, loss = 0.47438830\n",
      "Iteration 70, loss = 0.42245264\n",
      "Iteration 71, loss = 0.41939424\n",
      "Iteration 72, loss = 0.52159401\n",
      "Iteration 73, loss = 0.42645914\n",
      "Iteration 74, loss = 0.41642719\n",
      "Iteration 75, loss = 0.41453795\n",
      "Iteration 76, loss = 0.43713940\n",
      "Iteration 77, loss = 0.43735671\n",
      "Iteration 78, loss = 0.40608279\n",
      "Iteration 79, loss = 0.41007996\n",
      "Iteration 80, loss = 0.39764162\n",
      "Iteration 81, loss = 0.39805151\n",
      "Iteration 82, loss = 0.47975873\n",
      "Iteration 83, loss = 0.42268172\n",
      "Iteration 84, loss = 0.40605177\n",
      "Iteration 85, loss = 0.41113489\n",
      "Iteration 86, loss = 0.40174547\n",
      "Iteration 87, loss = 0.41591155\n",
      "Iteration 88, loss = 0.38667922\n",
      "Iteration 89, loss = 0.38012709\n",
      "Iteration 90, loss = 0.58099588\n",
      "Iteration 91, loss = 0.42974042\n",
      "Iteration 92, loss = 0.53479997\n",
      "Iteration 93, loss = 0.50954855\n",
      "Iteration 94, loss = 0.44787310\n",
      "Iteration 95, loss = 0.42678961\n",
      "Iteration 96, loss = 0.41722007\n",
      "Iteration 97, loss = 0.44250409\n",
      "Iteration 98, loss = 0.42948978\n",
      "Iteration 99, loss = 0.44131587\n",
      "Iteration 100, loss = 0.40356742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.12120851\n",
      "Iteration 2, loss = 0.70729679\n",
      "Iteration 3, loss = 0.64759262\n",
      "Iteration 4, loss = 0.65765340\n",
      "Iteration 5, loss = 0.66085017\n",
      "Iteration 6, loss = 0.61276283\n",
      "Iteration 7, loss = 0.60247564\n",
      "Iteration 8, loss = 0.57608387\n",
      "Iteration 9, loss = 0.58216993\n",
      "Iteration 10, loss = 0.56072107\n",
      "Iteration 11, loss = 0.54540635\n",
      "Iteration 12, loss = 0.61040738\n",
      "Iteration 13, loss = 0.65860380\n",
      "Iteration 14, loss = 0.58446085\n",
      "Iteration 15, loss = 0.54853365\n",
      "Iteration 16, loss = 0.54356248\n",
      "Iteration 17, loss = 0.55990282\n",
      "Iteration 18, loss = 0.53862990\n",
      "Iteration 19, loss = 0.55929118\n",
      "Iteration 20, loss = 0.57199023\n",
      "Iteration 21, loss = 0.52202285\n",
      "Iteration 22, loss = 0.57381754\n",
      "Iteration 23, loss = 0.54672851\n",
      "Iteration 24, loss = 0.53937144\n",
      "Iteration 25, loss = 0.51129056\n",
      "Iteration 26, loss = 0.51175373\n",
      "Iteration 27, loss = 0.50424591\n",
      "Iteration 28, loss = 0.48644890\n",
      "Iteration 29, loss = 0.51453035\n",
      "Iteration 30, loss = 0.50972358\n",
      "Iteration 31, loss = 0.51331518\n",
      "Iteration 32, loss = 0.49790803\n",
      "Iteration 33, loss = 0.50114622\n",
      "Iteration 34, loss = 0.48006055\n",
      "Iteration 35, loss = 0.48391994\n",
      "Iteration 36, loss = 0.49011699\n",
      "Iteration 37, loss = 0.46578762\n",
      "Iteration 38, loss = 0.46574394\n",
      "Iteration 39, loss = 0.45846254\n",
      "Iteration 40, loss = 0.44553264\n",
      "Iteration 41, loss = 0.48697224\n",
      "Iteration 42, loss = 0.48996151\n",
      "Iteration 43, loss = 0.47140476\n",
      "Iteration 44, loss = 0.43798675\n",
      "Iteration 45, loss = 0.45151664\n",
      "Iteration 46, loss = 0.45005566\n",
      "Iteration 47, loss = 0.71324648\n",
      "Iteration 48, loss = 0.63429221\n",
      "Iteration 49, loss = 0.51853810\n",
      "Iteration 50, loss = 0.58637166\n",
      "Iteration 51, loss = 0.49890274\n",
      "Iteration 52, loss = 0.50712410\n",
      "Iteration 53, loss = 0.51079741\n",
      "Iteration 54, loss = 0.48240747\n",
      "Iteration 55, loss = 0.45335868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09470041\n",
      "Iteration 2, loss = 0.82419215\n",
      "Iteration 3, loss = 0.67752523\n",
      "Iteration 4, loss = 0.64425883\n",
      "Iteration 5, loss = 0.60105635\n",
      "Iteration 6, loss = 0.60130717\n",
      "Iteration 7, loss = 0.64135992\n",
      "Iteration 8, loss = 0.57129250\n",
      "Iteration 9, loss = 0.59893549\n",
      "Iteration 10, loss = 0.61543478\n",
      "Iteration 11, loss = 0.58331002\n",
      "Iteration 12, loss = 0.65889915\n",
      "Iteration 13, loss = 0.56790834\n",
      "Iteration 14, loss = 0.59944986\n",
      "Iteration 15, loss = 0.65570991\n",
      "Iteration 16, loss = 0.57692437\n",
      "Iteration 17, loss = 0.59773551\n",
      "Iteration 18, loss = 0.55859120\n",
      "Iteration 19, loss = 0.53803519\n",
      "Iteration 20, loss = 0.56540852\n",
      "Iteration 21, loss = 0.55231426\n",
      "Iteration 22, loss = 0.54373733\n",
      "Iteration 23, loss = 0.54138776\n",
      "Iteration 24, loss = 0.58381945\n",
      "Iteration 25, loss = 0.59185563\n",
      "Iteration 26, loss = 0.58387713\n",
      "Iteration 27, loss = 0.53623081\n",
      "Iteration 28, loss = 0.54010045\n",
      "Iteration 29, loss = 0.51930021\n",
      "Iteration 30, loss = 0.51277390\n",
      "Iteration 31, loss = 0.50257029\n",
      "Iteration 32, loss = 0.52099841\n",
      "Iteration 33, loss = 0.52606073\n",
      "Iteration 34, loss = 0.57099117\n",
      "Iteration 35, loss = 0.58652621\n",
      "Iteration 36, loss = 0.64614625\n",
      "Iteration 37, loss = 0.54758850\n",
      "Iteration 38, loss = 0.51826184\n",
      "Iteration 39, loss = 0.52168148\n",
      "Iteration 40, loss = 0.51779164\n",
      "Iteration 41, loss = 0.50273763\n",
      "Iteration 42, loss = 0.50702156\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.13755401\n",
      "Iteration 2, loss = 0.66842741\n",
      "Iteration 3, loss = 0.61629837\n",
      "Iteration 4, loss = 0.61268633\n",
      "Iteration 5, loss = 0.64669700\n",
      "Iteration 6, loss = 0.56314386\n",
      "Iteration 7, loss = 0.56072729\n",
      "Iteration 8, loss = 0.56395801\n",
      "Iteration 9, loss = 0.57023828\n",
      "Iteration 10, loss = 0.55272874\n",
      "Iteration 11, loss = 0.55325810\n",
      "Iteration 12, loss = 0.53450575\n",
      "Iteration 13, loss = 0.54630285\n",
      "Iteration 14, loss = 0.53689477\n",
      "Iteration 15, loss = 0.49784369\n",
      "Iteration 16, loss = 0.49100936\n",
      "Iteration 17, loss = 0.48106163\n",
      "Iteration 18, loss = 0.48222215\n",
      "Iteration 19, loss = 0.49788208\n",
      "Iteration 20, loss = 0.47103698\n",
      "Iteration 21, loss = 0.71391583\n",
      "Iteration 22, loss = 0.54148297\n",
      "Iteration 23, loss = 0.53157258\n",
      "Iteration 24, loss = 0.46550116\n",
      "Iteration 25, loss = 0.46593741\n",
      "Iteration 26, loss = 0.45638514\n",
      "Iteration 27, loss = 0.45155588\n",
      "Iteration 28, loss = 0.50627712\n",
      "Iteration 29, loss = 0.44143413\n",
      "Iteration 30, loss = 0.44381613\n",
      "Iteration 31, loss = 0.43282190\n",
      "Iteration 32, loss = 0.43430873\n",
      "Iteration 33, loss = 0.42445632\n",
      "Iteration 34, loss = 0.43337757\n",
      "Iteration 35, loss = 0.51156655\n",
      "Iteration 36, loss = 0.44583553\n",
      "Iteration 37, loss = 0.45498457\n",
      "Iteration 38, loss = 0.42507763\n",
      "Iteration 39, loss = 0.41030593\n",
      "Iteration 40, loss = 0.40770564\n",
      "Iteration 41, loss = 0.39768063\n",
      "Iteration 42, loss = 0.40385877\n",
      "Iteration 43, loss = 0.39591786\n",
      "Iteration 44, loss = 0.41692151\n",
      "Iteration 45, loss = 0.63386994\n",
      "Iteration 46, loss = 0.48732110\n",
      "Iteration 47, loss = 0.49826260\n",
      "Iteration 48, loss = 0.39669765\n",
      "Iteration 49, loss = 0.38141205\n",
      "Iteration 50, loss = 0.38421422\n",
      "Iteration 51, loss = 0.39383929\n",
      "Iteration 52, loss = 0.51023266\n",
      "Iteration 53, loss = 0.42043244\n",
      "Iteration 54, loss = 0.37994686\n",
      "Iteration 55, loss = 0.40620196\n",
      "Iteration 56, loss = 0.41680659\n",
      "Iteration 57, loss = 0.37635439\n",
      "Iteration 58, loss = 0.36559577\n",
      "Iteration 59, loss = 0.38159180\n",
      "Iteration 60, loss = 0.44534376\n",
      "Iteration 61, loss = 0.37671491\n",
      "Iteration 62, loss = 0.35647711\n",
      "Iteration 63, loss = 0.37676602\n",
      "Iteration 64, loss = 0.37635843\n",
      "Iteration 65, loss = 0.37222441\n",
      "Iteration 66, loss = 0.48096669\n",
      "Iteration 67, loss = 0.45664869\n",
      "Iteration 68, loss = 0.40741529\n",
      "Iteration 69, loss = 0.34935507\n",
      "Iteration 70, loss = 0.34653129\n",
      "Iteration 71, loss = 0.34666324\n",
      "Iteration 72, loss = 0.34046183\n",
      "Iteration 73, loss = 0.35636577\n",
      "Iteration 74, loss = 0.33698395\n",
      "Iteration 75, loss = 0.36991069\n",
      "Iteration 76, loss = 0.35208994\n",
      "Iteration 77, loss = 0.33576579\n",
      "Iteration 78, loss = 0.35493145\n",
      "Iteration 79, loss = 0.33203183\n",
      "Iteration 80, loss = 0.38515338\n",
      "Iteration 81, loss = 0.42323038\n",
      "Iteration 82, loss = 0.34765855\n",
      "Iteration 83, loss = 0.33665184\n",
      "Iteration 84, loss = 0.40052821\n",
      "Iteration 85, loss = 0.37164314\n",
      "Iteration 86, loss = 0.32069651\n",
      "Iteration 87, loss = 0.31521736\n",
      "Iteration 88, loss = 0.51511564\n",
      "Iteration 89, loss = 0.51702798\n",
      "Iteration 90, loss = 0.41029570\n",
      "Iteration 91, loss = 0.33554335\n",
      "Iteration 92, loss = 0.35203521\n",
      "Iteration 93, loss = 0.40648169\n",
      "Iteration 94, loss = 0.32669739\n",
      "Iteration 95, loss = 0.31598416\n",
      "Iteration 96, loss = 0.30232895\n",
      "Iteration 97, loss = 0.29721946\n",
      "Iteration 98, loss = 0.29585907\n",
      "Iteration 99, loss = 0.45940823\n",
      "Iteration 100, loss = 0.42161156\n",
      "Iteration 101, loss = 0.34138545\n",
      "Iteration 102, loss = 0.31713097\n",
      "Iteration 103, loss = 0.30596234\n",
      "Iteration 104, loss = 0.29901859\n",
      "Iteration 105, loss = 0.45355322\n",
      "Iteration 106, loss = 0.41938128\n",
      "Iteration 107, loss = 0.37299020\n",
      "Iteration 108, loss = 0.30202206\n",
      "Iteration 109, loss = 0.29483038\n",
      "Iteration 110, loss = 0.29037896\n",
      "Iteration 111, loss = 0.29623601\n",
      "Iteration 112, loss = 0.29133437\n",
      "Iteration 113, loss = 0.28092036\n",
      "Iteration 114, loss = 0.30459120\n",
      "Iteration 115, loss = 0.33604653\n",
      "Iteration 116, loss = 0.32360760\n",
      "Iteration 117, loss = 0.32944570\n",
      "Iteration 118, loss = 0.46381140\n",
      "Iteration 119, loss = 0.30727271\n",
      "Iteration 120, loss = 0.28654495\n",
      "Iteration 121, loss = 0.29904428\n",
      "Iteration 122, loss = 0.29308881\n",
      "Iteration 123, loss = 0.28176666\n",
      "Iteration 124, loss = 0.30386005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05452833\n",
      "Iteration 2, loss = 0.72742430\n",
      "Iteration 3, loss = 0.69371090\n",
      "Iteration 4, loss = 0.63838598\n",
      "Iteration 5, loss = 0.66662262\n",
      "Iteration 6, loss = 0.62437648\n",
      "Iteration 7, loss = 0.62190823\n",
      "Iteration 8, loss = 0.59589660\n",
      "Iteration 9, loss = 0.58720287\n",
      "Iteration 10, loss = 0.59199815\n",
      "Iteration 11, loss = 0.56437822\n",
      "Iteration 12, loss = 0.55842298\n",
      "Iteration 13, loss = 0.54677618\n",
      "Iteration 14, loss = 0.54053886\n",
      "Iteration 15, loss = 0.53801103\n",
      "Iteration 16, loss = 0.55930418\n",
      "Iteration 17, loss = 0.54198469\n",
      "Iteration 18, loss = 0.51102659\n",
      "Iteration 19, loss = 0.50465111\n",
      "Iteration 20, loss = 0.49912868\n",
      "Iteration 21, loss = 0.50066938\n",
      "Iteration 22, loss = 0.57797698\n",
      "Iteration 23, loss = 0.61207281\n",
      "Iteration 24, loss = 0.50381408\n",
      "Iteration 25, loss = 0.52471444\n",
      "Iteration 26, loss = 0.64169826\n",
      "Iteration 27, loss = 0.51482866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.63796057\n",
      "Iteration 29, loss = 0.56606292\n",
      "Iteration 30, loss = 0.50092419\n",
      "Iteration 31, loss = 0.54572557\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06465606\n",
      "Iteration 2, loss = 0.66345709\n",
      "Iteration 3, loss = 0.61831718\n",
      "Iteration 4, loss = 0.59338254\n",
      "Iteration 5, loss = 0.56287155\n",
      "Iteration 6, loss = 0.68137716\n",
      "Iteration 7, loss = 0.60342329\n",
      "Iteration 8, loss = 0.56530303\n",
      "Iteration 9, loss = 0.54903155\n",
      "Iteration 10, loss = 0.66344932\n",
      "Iteration 11, loss = 0.60112977\n",
      "Iteration 12, loss = 0.54545429\n",
      "Iteration 13, loss = 0.51182552\n",
      "Iteration 14, loss = 0.50898151\n",
      "Iteration 15, loss = 0.51499453\n",
      "Iteration 16, loss = 0.49782429\n",
      "Iteration 17, loss = 0.49476314\n",
      "Iteration 18, loss = 0.50089278\n",
      "Iteration 19, loss = 0.49423568\n",
      "Iteration 20, loss = 0.47391612\n",
      "Iteration 21, loss = 0.50312449\n",
      "Iteration 22, loss = 0.46901344\n",
      "Iteration 23, loss = 0.45658143\n",
      "Iteration 24, loss = 0.45219383\n",
      "Iteration 25, loss = 0.56417803\n",
      "Iteration 26, loss = 0.46474359\n",
      "Iteration 27, loss = 0.45286962\n",
      "Iteration 28, loss = 0.44159655\n",
      "Iteration 29, loss = 0.44616667\n",
      "Iteration 30, loss = 0.43619415\n",
      "Iteration 31, loss = 0.42200521\n",
      "Iteration 32, loss = 0.43801031\n",
      "Iteration 33, loss = 0.42972575\n",
      "Iteration 34, loss = 0.48979502\n",
      "Iteration 35, loss = 0.45877333\n",
      "Iteration 36, loss = 0.48672472\n",
      "Iteration 37, loss = 0.48937248\n",
      "Iteration 38, loss = 0.55253331\n",
      "Iteration 39, loss = 0.50673757\n",
      "Iteration 40, loss = 0.55455256\n",
      "Iteration 41, loss = 0.44977155\n",
      "Iteration 42, loss = 0.49086941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05104963\n",
      "Iteration 2, loss = 0.66043723\n",
      "Iteration 3, loss = 0.60782459\n",
      "Iteration 4, loss = 0.58486965\n",
      "Iteration 5, loss = 0.57164649\n",
      "Iteration 6, loss = 0.56010563\n",
      "Iteration 7, loss = 0.53603400\n",
      "Iteration 8, loss = 0.68815044\n",
      "Iteration 9, loss = 0.62853589\n",
      "Iteration 10, loss = 0.58047608\n",
      "Iteration 11, loss = 0.59110082\n",
      "Iteration 12, loss = 0.53843941\n",
      "Iteration 13, loss = 0.53758039\n",
      "Iteration 14, loss = 0.53379815\n",
      "Iteration 15, loss = 0.57479641\n",
      "Iteration 16, loss = 0.55269376\n",
      "Iteration 17, loss = 0.50021568\n",
      "Iteration 18, loss = 0.49203081\n",
      "Iteration 19, loss = 0.49653887\n",
      "Iteration 20, loss = 0.66061352\n",
      "Iteration 21, loss = 0.50318926\n",
      "Iteration 22, loss = 0.47232619\n",
      "Iteration 23, loss = 0.47331413\n",
      "Iteration 24, loss = 0.46573269\n",
      "Iteration 25, loss = 0.52761206\n",
      "Iteration 26, loss = 0.46403742\n",
      "Iteration 27, loss = 0.45514288\n",
      "Iteration 28, loss = 0.45690950\n",
      "Iteration 29, loss = 0.44564955\n",
      "Iteration 30, loss = 0.51494253\n",
      "Iteration 31, loss = 0.45180859\n",
      "Iteration 32, loss = 0.43892004\n",
      "Iteration 33, loss = 0.44084449\n",
      "Iteration 34, loss = 0.51203032\n",
      "Iteration 35, loss = 0.48329575\n",
      "Iteration 36, loss = 0.59969034\n",
      "Iteration 37, loss = 0.58465473\n",
      "Iteration 38, loss = 0.52945662\n",
      "Iteration 39, loss = 0.55668711\n",
      "Iteration 40, loss = 0.51025900\n",
      "Iteration 41, loss = 0.46906804\n",
      "Iteration 42, loss = 0.46909777\n",
      "Iteration 43, loss = 0.47816456\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03439187\n",
      "Iteration 2, loss = 0.66973839\n",
      "Iteration 3, loss = 0.68092430\n",
      "Iteration 4, loss = 0.60372464\n",
      "Iteration 5, loss = 0.59542987\n",
      "Iteration 6, loss = 0.56752161\n",
      "Iteration 7, loss = 0.58311351\n",
      "Iteration 8, loss = 0.56629820\n",
      "Iteration 9, loss = 0.56871781\n",
      "Iteration 10, loss = 0.68672981\n",
      "Iteration 11, loss = 0.53939397\n",
      "Iteration 12, loss = 0.57567787\n",
      "Iteration 13, loss = 0.57653033\n",
      "Iteration 14, loss = 0.55405371\n",
      "Iteration 15, loss = 0.53624997\n",
      "Iteration 16, loss = 0.51451399\n",
      "Iteration 17, loss = 0.50329789\n",
      "Iteration 18, loss = 0.50016800\n",
      "Iteration 19, loss = 0.51489434\n",
      "Iteration 20, loss = 0.50209463\n",
      "Iteration 21, loss = 0.50020272\n",
      "Iteration 22, loss = 0.53194596\n",
      "Iteration 23, loss = 0.47655876\n",
      "Iteration 24, loss = 0.51404415\n",
      "Iteration 25, loss = 0.49396218\n",
      "Iteration 26, loss = 0.55615703\n",
      "Iteration 27, loss = 0.72549233\n",
      "Iteration 28, loss = 0.51026406\n",
      "Iteration 29, loss = 0.48870252\n",
      "Iteration 30, loss = 0.47582758\n",
      "Iteration 31, loss = 0.46422582\n",
      "Iteration 32, loss = 0.45836750\n",
      "Iteration 33, loss = 0.45856828\n",
      "Iteration 34, loss = 0.46951974\n",
      "Iteration 35, loss = 0.55944897\n",
      "Iteration 36, loss = 0.49124994\n",
      "Iteration 37, loss = 0.59455081\n",
      "Iteration 38, loss = 0.47348147\n",
      "Iteration 39, loss = 0.45322572\n",
      "Iteration 40, loss = 0.44661083\n",
      "Iteration 41, loss = 0.43729344\n",
      "Iteration 42, loss = 0.42869552\n",
      "Iteration 43, loss = 0.43101548\n",
      "Iteration 44, loss = 0.41787930\n",
      "Iteration 45, loss = 0.41771433\n",
      "Iteration 46, loss = 0.41742264\n",
      "Iteration 47, loss = 0.41070873\n",
      "Iteration 48, loss = 0.48013738\n",
      "Iteration 49, loss = 0.44863462\n",
      "Iteration 50, loss = 0.45480511\n",
      "Iteration 51, loss = 0.44112279\n",
      "Iteration 52, loss = 0.40571576\n",
      "Iteration 53, loss = 0.44424558\n",
      "Iteration 54, loss = 0.41557335\n",
      "Iteration 55, loss = 0.40380925\n",
      "Iteration 56, loss = 0.41547874\n",
      "Iteration 57, loss = 0.41451029\n",
      "Iteration 58, loss = 0.41147753\n",
      "Iteration 59, loss = 0.41723002\n",
      "Iteration 60, loss = 0.42609365\n",
      "Iteration 61, loss = 0.40139241\n",
      "Iteration 62, loss = 0.41294765\n",
      "Iteration 63, loss = 0.38984094\n",
      "Iteration 64, loss = 0.39513881\n",
      "Iteration 65, loss = 0.37020416\n",
      "Iteration 66, loss = 0.39010868\n",
      "Iteration 67, loss = 0.37807062\n",
      "Iteration 68, loss = 0.70936996\n",
      "Iteration 69, loss = 0.59676548\n",
      "Iteration 70, loss = 0.46649836\n",
      "Iteration 71, loss = 0.42475567\n",
      "Iteration 72, loss = 0.41501855\n",
      "Iteration 73, loss = 0.40165610\n",
      "Iteration 74, loss = 0.46051615\n",
      "Iteration 75, loss = 0.43967434\n",
      "Iteration 76, loss = 0.42698668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82919132\n",
      "Iteration 2, loss = 0.64611516\n",
      "Iteration 3, loss = 0.60797569\n",
      "Iteration 4, loss = 0.56566129\n",
      "Iteration 5, loss = 0.58801157\n",
      "Iteration 6, loss = 0.60310515\n",
      "Iteration 7, loss = 0.55557731\n",
      "Iteration 8, loss = 0.57972641\n",
      "Iteration 9, loss = 0.55370003\n",
      "Iteration 10, loss = 0.66103366\n",
      "Iteration 11, loss = 0.58862144\n",
      "Iteration 12, loss = 0.64218411\n",
      "Iteration 13, loss = 0.55649939\n",
      "Iteration 14, loss = 0.54772213\n",
      "Iteration 15, loss = 0.51889658\n",
      "Iteration 16, loss = 0.54170060\n",
      "Iteration 17, loss = 0.50783858\n",
      "Iteration 18, loss = 0.48409668\n",
      "Iteration 19, loss = 0.48949732\n",
      "Iteration 20, loss = 0.47117563\n",
      "Iteration 21, loss = 0.49443604\n",
      "Iteration 22, loss = 0.65038621\n",
      "Iteration 23, loss = 0.56503871\n",
      "Iteration 24, loss = 0.49297665\n",
      "Iteration 25, loss = 0.54604121\n",
      "Iteration 26, loss = 0.53571105\n",
      "Iteration 27, loss = 0.52916894\n",
      "Iteration 28, loss = 0.49110619\n",
      "Iteration 29, loss = 0.48040143\n",
      "Iteration 30, loss = 0.46605701\n",
      "Iteration 31, loss = 0.45199164\n",
      "Iteration 32, loss = 0.44860451\n",
      "Iteration 33, loss = 0.46348606\n",
      "Iteration 34, loss = 0.48481054\n",
      "Iteration 35, loss = 0.48127194\n",
      "Iteration 36, loss = 0.51672753\n",
      "Iteration 37, loss = 0.44754719\n",
      "Iteration 38, loss = 0.53226361\n",
      "Iteration 39, loss = 0.47255059\n",
      "Iteration 40, loss = 0.43460066\n",
      "Iteration 41, loss = 0.45693246\n",
      "Iteration 42, loss = 0.43701050\n",
      "Iteration 43, loss = 0.54878503\n",
      "Iteration 44, loss = 0.43668735\n",
      "Iteration 45, loss = 0.54729347\n",
      "Iteration 46, loss = 0.43567276\n",
      "Iteration 47, loss = 0.45041241\n",
      "Iteration 48, loss = 0.48103308\n",
      "Iteration 49, loss = 0.45279207\n",
      "Iteration 50, loss = 0.42876240\n",
      "Iteration 51, loss = 0.41270470\n",
      "Iteration 52, loss = 0.40902205\n",
      "Iteration 53, loss = 0.40373558\n",
      "Iteration 54, loss = 0.45328269\n",
      "Iteration 55, loss = 0.50958845\n",
      "Iteration 56, loss = 0.40448830\n",
      "Iteration 57, loss = 0.39707770\n",
      "Iteration 58, loss = 0.39394139\n",
      "Iteration 59, loss = 0.42789937\n",
      "Iteration 60, loss = 0.47429530\n",
      "Iteration 61, loss = 0.42608525\n",
      "Iteration 62, loss = 0.38568651\n",
      "Iteration 63, loss = 0.39940700\n",
      "Iteration 64, loss = 0.37880913\n",
      "Iteration 65, loss = 0.39395556\n",
      "Iteration 66, loss = 0.39113547\n",
      "Iteration 67, loss = 0.41358771\n",
      "Iteration 68, loss = 0.38911713\n",
      "Iteration 69, loss = 0.39754345\n",
      "Iteration 70, loss = 0.39850228\n",
      "Iteration 71, loss = 0.40525868\n",
      "Iteration 72, loss = 0.39292963\n",
      "Iteration 73, loss = 0.58386343\n",
      "Iteration 74, loss = 0.40067384\n",
      "Iteration 75, loss = 0.44926612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.96118838\n",
      "Iteration 2, loss = 0.65419865\n",
      "Iteration 3, loss = 0.61414940\n",
      "Iteration 4, loss = 0.57573931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.55547413\n",
      "Iteration 6, loss = 0.54445210\n",
      "Iteration 7, loss = 0.52670737\n",
      "Iteration 8, loss = 0.51707451\n",
      "Iteration 9, loss = 0.51182759\n",
      "Iteration 10, loss = 0.53866777\n",
      "Iteration 11, loss = 0.49828546\n",
      "Iteration 12, loss = 0.47947046\n",
      "Iteration 13, loss = 0.48165486\n",
      "Iteration 14, loss = 0.47721297\n",
      "Iteration 15, loss = 0.46718363\n",
      "Iteration 16, loss = 0.46955456\n",
      "Iteration 17, loss = 0.47121291\n",
      "Iteration 18, loss = 0.43553389\n",
      "Iteration 19, loss = 0.42886332\n",
      "Iteration 20, loss = 0.41717343\n",
      "Iteration 21, loss = 0.41443273\n",
      "Iteration 22, loss = 0.40460256\n",
      "Iteration 23, loss = 0.39906234\n",
      "Iteration 24, loss = 0.42266476\n",
      "Iteration 25, loss = 0.41475121\n",
      "Iteration 26, loss = 0.38875205\n",
      "Iteration 27, loss = 0.38068907\n",
      "Iteration 28, loss = 0.36715510\n",
      "Iteration 29, loss = 0.36520520\n",
      "Iteration 30, loss = 0.36324136\n",
      "Iteration 31, loss = 0.35380494\n",
      "Iteration 32, loss = 0.36050887\n",
      "Iteration 33, loss = 0.34698726\n",
      "Iteration 34, loss = 0.35435978\n",
      "Iteration 35, loss = 0.33693390\n",
      "Iteration 36, loss = 0.37240734\n",
      "Iteration 37, loss = 0.34185231\n",
      "Iteration 38, loss = 0.34554018\n",
      "Iteration 39, loss = 0.31829994\n",
      "Iteration 40, loss = 0.38700170\n",
      "Iteration 41, loss = 0.31796803\n",
      "Iteration 42, loss = 0.32087293\n",
      "Iteration 43, loss = 0.31222201\n",
      "Iteration 44, loss = 0.29374745\n",
      "Iteration 45, loss = 0.29223351\n",
      "Iteration 46, loss = 0.29827547\n",
      "Iteration 47, loss = 0.28642673\n",
      "Iteration 48, loss = 0.27368928\n",
      "Iteration 49, loss = 0.27282803\n",
      "Iteration 50, loss = 0.30099867\n",
      "Iteration 51, loss = 0.26373810\n",
      "Iteration 52, loss = 0.25607718\n",
      "Iteration 53, loss = 0.25387998\n",
      "Iteration 54, loss = 0.25192676\n",
      "Iteration 55, loss = 0.25640781\n",
      "Iteration 56, loss = 0.24408358\n",
      "Iteration 57, loss = 0.23450913\n",
      "Iteration 58, loss = 0.25605295\n",
      "Iteration 59, loss = 0.24689609\n",
      "Iteration 60, loss = 0.24771525\n",
      "Iteration 61, loss = 0.23585018\n",
      "Iteration 62, loss = 0.21504122\n",
      "Iteration 63, loss = 0.20881783\n",
      "Iteration 64, loss = 0.20778176\n",
      "Iteration 65, loss = 0.21797110\n",
      "Iteration 66, loss = 0.19794010\n",
      "Iteration 67, loss = 0.20831909\n",
      "Iteration 68, loss = 0.19816034\n",
      "Iteration 69, loss = 0.20419427\n",
      "Iteration 70, loss = 0.18443066\n",
      "Iteration 71, loss = 0.17243476\n",
      "Iteration 72, loss = 0.17126525\n",
      "Iteration 73, loss = 0.21521425\n",
      "Iteration 74, loss = 0.18728009\n",
      "Iteration 75, loss = 0.27821250\n",
      "Iteration 76, loss = 0.19582680\n",
      "Iteration 77, loss = 0.15867865\n",
      "Iteration 78, loss = 0.17330836\n",
      "Iteration 79, loss = 0.19483112\n",
      "Iteration 80, loss = 0.18376306\n",
      "Iteration 81, loss = 0.20111672\n",
      "Iteration 82, loss = 0.15086399\n",
      "Iteration 83, loss = 0.15036645\n",
      "Iteration 84, loss = 0.13678383\n",
      "Iteration 85, loss = 0.12807630\n",
      "Iteration 86, loss = 0.16159198\n",
      "Iteration 87, loss = 0.15892077\n",
      "Iteration 88, loss = 0.13486139\n",
      "Iteration 89, loss = 0.13601629\n",
      "Iteration 90, loss = 0.11171987\n",
      "Iteration 91, loss = 0.11290212\n",
      "Iteration 92, loss = 0.12269020\n",
      "Iteration 93, loss = 0.12090424\n",
      "Iteration 94, loss = 0.10663220\n",
      "Iteration 95, loss = 0.10263354\n",
      "Iteration 96, loss = 0.10196005\n",
      "Iteration 97, loss = 0.09584997\n",
      "Iteration 98, loss = 0.09777132\n",
      "Iteration 99, loss = 0.08656635\n",
      "Iteration 100, loss = 0.08487370\n",
      "Iteration 101, loss = 0.09259064\n",
      "Iteration 102, loss = 0.09681241\n",
      "Iteration 103, loss = 0.08685923\n",
      "Iteration 104, loss = 0.08694758\n",
      "Iteration 105, loss = 0.07614780\n",
      "Iteration 106, loss = 0.07456722\n",
      "Iteration 107, loss = 0.07766844\n",
      "Iteration 108, loss = 0.06950186\n",
      "Iteration 109, loss = 0.06734332\n",
      "Iteration 110, loss = 0.06640497\n",
      "Iteration 111, loss = 0.06162965\n",
      "Iteration 112, loss = 0.06489782\n",
      "Iteration 113, loss = 0.06347325\n",
      "Iteration 114, loss = 0.05809979\n",
      "Iteration 115, loss = 0.05812715\n",
      "Iteration 116, loss = 0.05657174\n",
      "Iteration 117, loss = 0.05407737\n",
      "Iteration 118, loss = 0.06170517\n",
      "Iteration 119, loss = 0.06504199\n",
      "Iteration 120, loss = 0.06380790\n",
      "Iteration 121, loss = 0.06246592\n",
      "Iteration 122, loss = 0.05466030\n",
      "Iteration 123, loss = 0.04965279\n",
      "Iteration 124, loss = 0.04662758\n",
      "Iteration 125, loss = 0.11647857\n",
      "Iteration 126, loss = 0.07532040\n",
      "Iteration 127, loss = 0.05673601\n",
      "Iteration 128, loss = 0.04634035\n",
      "Iteration 129, loss = 0.04446998\n",
      "Iteration 130, loss = 0.05585853\n",
      "Iteration 131, loss = 0.06153914\n",
      "Iteration 132, loss = 0.04342096\n",
      "Iteration 133, loss = 0.04368121\n",
      "Iteration 134, loss = 0.03993663\n",
      "Iteration 135, loss = 0.03792084\n",
      "Iteration 136, loss = 0.03374429\n",
      "Iteration 137, loss = 0.03179291\n",
      "Iteration 138, loss = 0.03303275\n",
      "Iteration 139, loss = 0.03085018\n",
      "Iteration 140, loss = 0.03303781\n",
      "Iteration 141, loss = 0.03867575\n",
      "Iteration 142, loss = 0.03158935\n",
      "Iteration 143, loss = 0.02847051\n",
      "Iteration 144, loss = 0.02742538\n",
      "Iteration 145, loss = 0.02675775\n",
      "Iteration 146, loss = 0.02496197\n",
      "Iteration 147, loss = 0.02627672\n",
      "Iteration 148, loss = 0.02610941\n",
      "Iteration 149, loss = 0.02863140\n",
      "Iteration 150, loss = 0.02584233\n",
      "Iteration 151, loss = 0.03031101\n",
      "Iteration 152, loss = 0.02466884\n",
      "Iteration 153, loss = 0.02273109\n",
      "Iteration 154, loss = 0.02277565\n",
      "Iteration 155, loss = 0.02361970\n",
      "Iteration 156, loss = 0.02424095\n",
      "Iteration 157, loss = 0.02694646\n",
      "Iteration 158, loss = 0.02082113\n",
      "Iteration 159, loss = 0.01984569\n",
      "Iteration 160, loss = 0.01977975\n",
      "Iteration 161, loss = 0.02024031\n",
      "Iteration 162, loss = 0.01782865\n",
      "Iteration 163, loss = 0.01806067\n",
      "Iteration 164, loss = 0.01852915\n",
      "Iteration 165, loss = 0.01765013\n",
      "Iteration 166, loss = 0.01673127\n",
      "Iteration 167, loss = 0.01654574\n",
      "Iteration 168, loss = 0.01721489\n",
      "Iteration 169, loss = 0.01591394\n",
      "Iteration 170, loss = 0.01638044\n",
      "Iteration 171, loss = 0.01670564\n",
      "Iteration 172, loss = 0.01612500\n",
      "Iteration 173, loss = 0.02428569\n",
      "Iteration 174, loss = 0.01875739\n",
      "Iteration 175, loss = 0.01532727\n",
      "Iteration 176, loss = 0.01606773\n",
      "Iteration 177, loss = 0.01584578\n",
      "Iteration 178, loss = 0.01552117\n",
      "Iteration 179, loss = 0.01312964\n",
      "Iteration 180, loss = 0.01274215\n",
      "Iteration 181, loss = 0.01302816\n",
      "Iteration 182, loss = 0.01339338\n",
      "Iteration 183, loss = 0.01393204\n",
      "Iteration 184, loss = 0.01276495\n",
      "Iteration 185, loss = 0.01262058\n",
      "Iteration 186, loss = 0.01224936\n",
      "Iteration 187, loss = 0.01198526\n",
      "Iteration 188, loss = 0.01220898\n",
      "Iteration 189, loss = 0.01281848\n",
      "Iteration 190, loss = 0.01342206\n",
      "Iteration 191, loss = 0.01181543\n",
      "Iteration 192, loss = 0.01142941\n",
      "Iteration 193, loss = 0.01054088\n",
      "Iteration 194, loss = 0.01073170\n",
      "Iteration 195, loss = 0.01134371\n",
      "Iteration 196, loss = 0.01028838\n",
      "Iteration 197, loss = 0.00974288\n",
      "Iteration 198, loss = 0.01057446\n",
      "Iteration 199, loss = 0.01039527\n",
      "Iteration 200, loss = 0.01001058\n",
      "Iteration 201, loss = 0.00921086\n",
      "Iteration 202, loss = 0.00905890\n",
      "Iteration 203, loss = 0.00895980\n",
      "Iteration 204, loss = 0.00900344\n",
      "Iteration 205, loss = 0.00921269\n",
      "Iteration 206, loss = 0.00957603\n",
      "Iteration 207, loss = 0.00992458\n",
      "Iteration 208, loss = 0.00862397\n",
      "Iteration 209, loss = 0.00831084\n",
      "Iteration 210, loss = 0.00815420\n",
      "Iteration 211, loss = 0.00793042\n",
      "Iteration 212, loss = 0.00782732\n",
      "Iteration 213, loss = 0.00768089\n",
      "Iteration 214, loss = 0.00769750\n",
      "Iteration 215, loss = 0.00762395\n",
      "Iteration 216, loss = 0.00741669\n",
      "Iteration 217, loss = 0.00725690\n",
      "Iteration 218, loss = 0.00720074\n",
      "Iteration 219, loss = 0.00740674\n",
      "Iteration 220, loss = 0.00698053\n",
      "Iteration 221, loss = 0.00764151\n",
      "Iteration 222, loss = 0.00743609\n",
      "Iteration 223, loss = 0.00749054\n",
      "Iteration 224, loss = 0.00673975\n",
      "Iteration 225, loss = 0.00670487\n",
      "Iteration 226, loss = 0.00641927\n",
      "Iteration 227, loss = 0.00653381\n",
      "Iteration 228, loss = 0.00681457\n",
      "Iteration 229, loss = 0.00669362\n",
      "Iteration 230, loss = 0.00655209\n",
      "Iteration 231, loss = 0.00653676\n",
      "Iteration 232, loss = 0.00715988\n",
      "Iteration 233, loss = 0.00694877\n",
      "Iteration 234, loss = 0.00640449\n",
      "Iteration 235, loss = 0.00647144\n",
      "Iteration 236, loss = 0.00587828\n",
      "Iteration 237, loss = 0.00569814\n",
      "Iteration 238, loss = 0.00552890\n",
      "Iteration 239, loss = 0.00547012\n",
      "Iteration 240, loss = 0.00544170\n",
      "Iteration 241, loss = 0.00538908\n",
      "Iteration 242, loss = 0.00536246\n",
      "Iteration 243, loss = 0.00568478\n",
      "Iteration 244, loss = 0.00545860\n",
      "Iteration 245, loss = 0.00511355\n",
      "Iteration 246, loss = 0.00511167\n",
      "Iteration 247, loss = 0.00514356\n",
      "Iteration 248, loss = 0.00493924\n",
      "Iteration 249, loss = 0.00496617\n",
      "Iteration 250, loss = 0.00524261\n",
      "Iteration 251, loss = 0.00528750\n",
      "Iteration 252, loss = 0.00491118\n",
      "Iteration 253, loss = 0.00467255\n",
      "Iteration 254, loss = 0.00462347\n",
      "Iteration 255, loss = 0.00468007\n",
      "Iteration 256, loss = 0.00448395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.00452787\n",
      "Iteration 258, loss = 0.00439916\n",
      "Iteration 259, loss = 0.00435659\n",
      "Iteration 260, loss = 0.00484494\n",
      "Iteration 261, loss = 0.00447481\n",
      "Iteration 262, loss = 0.00418235\n",
      "Iteration 263, loss = 0.00416337\n",
      "Iteration 264, loss = 0.00407372\n",
      "Iteration 265, loss = 0.00412732\n",
      "Iteration 266, loss = 0.00422538\n",
      "Iteration 267, loss = 0.00431326\n",
      "Iteration 268, loss = 0.00426203\n",
      "Iteration 269, loss = 0.00419599\n",
      "Iteration 270, loss = 0.00385062\n",
      "Iteration 271, loss = 0.00387557\n",
      "Iteration 272, loss = 0.00378903\n",
      "Iteration 273, loss = 0.00381323\n",
      "Iteration 274, loss = 0.00404019\n",
      "Iteration 275, loss = 0.00376207\n",
      "Iteration 276, loss = 0.00359831\n",
      "Iteration 277, loss = 0.00354320\n",
      "Iteration 278, loss = 0.00352426\n",
      "Iteration 279, loss = 0.00348108\n",
      "Iteration 280, loss = 0.00344113\n",
      "Iteration 281, loss = 0.00372768\n",
      "Iteration 282, loss = 0.00407411\n",
      "Iteration 283, loss = 0.00401664\n",
      "Iteration 284, loss = 0.00331797\n",
      "Iteration 285, loss = 0.00349082\n",
      "Iteration 286, loss = 0.00328310\n",
      "Iteration 287, loss = 0.00377189\n",
      "Iteration 288, loss = 0.00376612\n",
      "Iteration 289, loss = 0.00350337\n",
      "Iteration 290, loss = 0.00322469\n",
      "Iteration 291, loss = 0.00318673\n",
      "Iteration 292, loss = 0.00307514\n",
      "Iteration 293, loss = 0.00309589\n",
      "Iteration 294, loss = 0.00317264\n",
      "Iteration 295, loss = 0.00301002\n",
      "Iteration 296, loss = 0.00293960\n",
      "Iteration 297, loss = 0.00288487\n",
      "Iteration 298, loss = 0.00318818\n",
      "Iteration 299, loss = 0.00293702\n",
      "Iteration 300, loss = 0.00290957\n",
      "Iteration 301, loss = 0.00281409\n",
      "Iteration 302, loss = 0.00285598\n",
      "Iteration 303, loss = 0.00282368\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68149066\n",
      "Iteration 2, loss = 0.37841201\n",
      "Iteration 3, loss = 0.29928844\n",
      "Iteration 4, loss = 0.27859361\n",
      "Iteration 5, loss = 0.28497221\n",
      "Iteration 6, loss = 0.35239707\n",
      "Iteration 7, loss = 0.27641051\n",
      "Iteration 8, loss = 0.21325649\n",
      "Iteration 9, loss = 0.15594559\n",
      "Iteration 10, loss = 0.18114733\n",
      "Iteration 11, loss = 0.14344733\n",
      "Iteration 12, loss = 0.11034338\n",
      "Iteration 13, loss = 0.08038491\n",
      "Iteration 14, loss = 0.06276437\n",
      "Iteration 15, loss = 0.05103812\n",
      "Iteration 16, loss = 0.04358460\n",
      "Iteration 17, loss = 0.03862117\n",
      "Iteration 18, loss = 0.03359743\n",
      "Iteration 19, loss = 0.03124171\n",
      "Iteration 20, loss = 0.03731478\n",
      "Iteration 21, loss = 0.03475492\n",
      "Iteration 22, loss = 0.02905737\n",
      "Iteration 23, loss = 0.02365528\n",
      "Iteration 24, loss = 0.02026802\n",
      "Iteration 25, loss = 0.01812924\n",
      "Iteration 26, loss = 0.01636293\n",
      "Iteration 27, loss = 0.01512718\n",
      "Iteration 28, loss = 0.01653348\n",
      "Iteration 29, loss = 0.01502027\n",
      "Iteration 30, loss = 0.01306864\n",
      "Iteration 31, loss = 0.01192695\n",
      "Iteration 32, loss = 0.01099189\n",
      "Iteration 33, loss = 0.01068545\n",
      "Iteration 34, loss = 0.01132918\n",
      "Iteration 35, loss = 0.01116285\n",
      "Iteration 36, loss = 0.00954482\n",
      "Iteration 37, loss = 0.00861477\n",
      "Iteration 38, loss = 0.00841809\n",
      "Iteration 39, loss = 0.00813067\n",
      "Iteration 40, loss = 0.00743027\n",
      "Iteration 41, loss = 0.00690251\n",
      "Iteration 42, loss = 0.00649541\n",
      "Iteration 43, loss = 0.00616381\n",
      "Iteration 44, loss = 0.00606289\n",
      "Iteration 45, loss = 0.00587760\n",
      "Iteration 46, loss = 0.00562621\n",
      "Iteration 47, loss = 0.00565515\n",
      "Iteration 48, loss = 0.00579209\n",
      "Iteration 49, loss = 0.00699459\n",
      "Iteration 50, loss = 0.00546514\n",
      "Iteration 51, loss = 0.00482532\n",
      "Iteration 52, loss = 0.00444686\n",
      "Iteration 53, loss = 0.00422890\n",
      "Iteration 54, loss = 0.00407027\n",
      "Iteration 55, loss = 0.00389573\n",
      "Iteration 56, loss = 0.00375131\n",
      "Iteration 57, loss = 0.00362616\n",
      "Iteration 58, loss = 0.00365836\n",
      "Iteration 59, loss = 0.00368640\n",
      "Iteration 60, loss = 0.00344429\n",
      "Iteration 61, loss = 0.00327800\n",
      "Iteration 62, loss = 0.00316431\n",
      "Iteration 63, loss = 0.00302982\n",
      "Iteration 64, loss = 0.00291084\n",
      "Iteration 65, loss = 0.00283414\n",
      "Iteration 66, loss = 0.00274765\n",
      "Iteration 67, loss = 0.00266279\n",
      "Iteration 68, loss = 0.00259322\n",
      "Iteration 69, loss = 0.00258754\n",
      "Iteration 70, loss = 0.00319006\n",
      "Iteration 71, loss = 0.00257392\n",
      "Iteration 72, loss = 0.00245205\n",
      "Iteration 73, loss = 0.00236135\n",
      "Iteration 74, loss = 0.00248799\n",
      "Iteration 75, loss = 0.00247421\n",
      "Iteration 76, loss = 0.01632720\n",
      "Iteration 77, loss = 0.02157519\n",
      "Iteration 78, loss = 0.01279589\n",
      "Iteration 79, loss = 0.00744998\n",
      "Iteration 80, loss = 0.00480068\n",
      "Iteration 81, loss = 0.00353542\n",
      "Iteration 82, loss = 0.00550207\n",
      "Iteration 83, loss = 0.00304057\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65024393\n",
      "Iteration 2, loss = 0.38640100\n",
      "Iteration 3, loss = 0.29842932\n",
      "Iteration 4, loss = 0.19480172\n",
      "Iteration 5, loss = 0.16300473\n",
      "Iteration 6, loss = 0.15957567\n",
      "Iteration 7, loss = 0.19315143\n",
      "Iteration 8, loss = 0.13095608\n",
      "Iteration 9, loss = 0.09837687\n",
      "Iteration 10, loss = 0.07211669\n",
      "Iteration 11, loss = 0.05147134\n",
      "Iteration 12, loss = 0.03591990\n",
      "Iteration 13, loss = 0.04631866\n",
      "Iteration 14, loss = 0.03824160\n",
      "Iteration 15, loss = 0.02842704\n",
      "Iteration 16, loss = 0.02173670\n",
      "Iteration 17, loss = 0.01813296\n",
      "Iteration 18, loss = 0.01594724\n",
      "Iteration 19, loss = 0.01450031\n",
      "Iteration 20, loss = 0.01218171\n",
      "Iteration 21, loss = 0.01081214\n",
      "Iteration 22, loss = 0.00980936\n",
      "Iteration 23, loss = 0.01132001\n",
      "Iteration 24, loss = 0.00999803\n",
      "Iteration 25, loss = 0.00874219\n",
      "Iteration 26, loss = 0.00756124\n",
      "Iteration 27, loss = 0.00687974\n",
      "Iteration 28, loss = 0.00698884\n",
      "Iteration 29, loss = 0.00668939\n",
      "Iteration 30, loss = 0.00602067\n",
      "Iteration 31, loss = 0.00544798\n",
      "Iteration 32, loss = 0.00541688\n",
      "Iteration 33, loss = 0.00510722\n",
      "Iteration 34, loss = 0.00460750\n",
      "Iteration 35, loss = 0.00461157\n",
      "Iteration 36, loss = 0.01323761\n",
      "Iteration 37, loss = 0.01089128\n",
      "Iteration 38, loss = 0.00660789\n",
      "Iteration 39, loss = 0.00471632\n",
      "Iteration 40, loss = 0.00443331\n",
      "Iteration 41, loss = 0.00560591\n",
      "Iteration 42, loss = 0.00391581\n",
      "Iteration 43, loss = 0.00320230\n",
      "Iteration 44, loss = 0.00295475\n",
      "Iteration 45, loss = 0.00365449\n",
      "Iteration 46, loss = 0.00319685\n",
      "Iteration 47, loss = 0.00304566\n",
      "Iteration 48, loss = 0.00264735\n",
      "Iteration 49, loss = 0.00280909\n",
      "Iteration 50, loss = 0.00258274\n",
      "Iteration 51, loss = 0.00235802\n",
      "Iteration 52, loss = 0.00215698\n",
      "Iteration 53, loss = 0.00236568\n",
      "Iteration 54, loss = 0.00298537\n",
      "Iteration 55, loss = 0.00228525\n",
      "Iteration 56, loss = 0.00192742\n",
      "Iteration 57, loss = 0.00184999\n",
      "Iteration 58, loss = 0.00175397\n",
      "Iteration 59, loss = 0.00169182\n",
      "Iteration 60, loss = 0.00164063\n",
      "Iteration 61, loss = 0.00159173\n",
      "Iteration 62, loss = 0.00158554\n",
      "Iteration 63, loss = 0.00156056\n",
      "Iteration 64, loss = 0.00154195\n",
      "Iteration 65, loss = 0.00151478\n",
      "Iteration 66, loss = 0.00153842\n",
      "Iteration 67, loss = 0.00140381\n",
      "Iteration 68, loss = 0.00139819\n",
      "Iteration 69, loss = 0.00136797\n",
      "Iteration 70, loss = 0.00131089\n",
      "Iteration 71, loss = 0.00127174\n",
      "Iteration 72, loss = 0.00124614\n",
      "Iteration 73, loss = 0.00121957\n",
      "Iteration 74, loss = 0.00124113\n",
      "Iteration 75, loss = 0.00123845\n",
      "Iteration 76, loss = 0.00117332\n",
      "Iteration 77, loss = 0.00115188\n",
      "Iteration 78, loss = 0.00112939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71529033\n",
      "Iteration 2, loss = 0.43471826\n",
      "Iteration 3, loss = 0.38855617\n",
      "Iteration 4, loss = 0.38133691\n",
      "Iteration 5, loss = 0.30223462\n",
      "Iteration 6, loss = 0.22580566\n",
      "Iteration 7, loss = 0.15892252\n",
      "Iteration 8, loss = 0.13555976\n",
      "Iteration 9, loss = 0.15049893\n",
      "Iteration 10, loss = 0.12813830\n",
      "Iteration 11, loss = 0.09429191\n",
      "Iteration 12, loss = 0.07242604\n",
      "Iteration 13, loss = 0.05711718\n",
      "Iteration 14, loss = 0.08472734\n",
      "Iteration 15, loss = 0.07747123\n",
      "Iteration 16, loss = 0.05137659\n",
      "Iteration 17, loss = 0.10434323\n",
      "Iteration 18, loss = 0.09893046\n",
      "Iteration 19, loss = 0.06780899\n",
      "Iteration 20, loss = 0.04552493\n",
      "Iteration 21, loss = 0.03060248\n",
      "Iteration 22, loss = 0.02476211\n",
      "Iteration 23, loss = 0.02026995\n",
      "Iteration 24, loss = 0.01796915\n",
      "Iteration 25, loss = 0.01629627\n",
      "Iteration 26, loss = 0.01488283\n",
      "Iteration 27, loss = 0.01342013\n",
      "Iteration 28, loss = 0.01231360\n",
      "Iteration 29, loss = 0.01292806\n",
      "Iteration 30, loss = 0.01146138\n",
      "Iteration 31, loss = 0.01027105\n",
      "Iteration 32, loss = 0.00953141\n",
      "Iteration 33, loss = 0.00886026\n",
      "Iteration 34, loss = 0.00834390\n",
      "Iteration 35, loss = 0.00843534\n",
      "Iteration 36, loss = 0.00819986\n",
      "Iteration 37, loss = 0.00756348\n",
      "Iteration 38, loss = 0.00700364\n",
      "Iteration 39, loss = 0.00651621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 0.00644388\n",
      "Iteration 41, loss = 0.00655870\n",
      "Iteration 42, loss = 0.00639294\n",
      "Iteration 43, loss = 0.00564025\n",
      "Iteration 44, loss = 0.00527733\n",
      "Iteration 45, loss = 0.00510058\n",
      "Iteration 46, loss = 0.00529705\n",
      "Iteration 47, loss = 0.00489943\n",
      "Iteration 48, loss = 0.00551608\n",
      "Iteration 49, loss = 0.00684527\n",
      "Iteration 50, loss = 0.00553487\n",
      "Iteration 51, loss = 0.00467682\n",
      "Iteration 52, loss = 0.01097890\n",
      "Iteration 53, loss = 0.00932079\n",
      "Iteration 54, loss = 0.00633960\n",
      "Iteration 55, loss = 0.00428345\n",
      "Iteration 56, loss = 0.00466376\n",
      "Iteration 57, loss = 0.00359393\n",
      "Iteration 58, loss = 0.00330724\n",
      "Iteration 59, loss = 0.00339694\n",
      "Iteration 60, loss = 0.00327914\n",
      "Iteration 61, loss = 0.00293074\n",
      "Iteration 62, loss = 0.00274419\n",
      "Iteration 63, loss = 0.00262756\n",
      "Iteration 64, loss = 0.00253446\n",
      "Iteration 65, loss = 0.00245625\n",
      "Iteration 66, loss = 0.00257679\n",
      "Iteration 67, loss = 0.00308567\n",
      "Iteration 68, loss = 0.00264558\n",
      "Iteration 69, loss = 0.00240743\n",
      "Iteration 70, loss = 0.00232361\n",
      "Iteration 71, loss = 0.00237246\n",
      "Iteration 72, loss = 0.00234139\n",
      "Iteration 73, loss = 0.00218200\n",
      "Iteration 74, loss = 0.00204255\n",
      "Iteration 75, loss = 0.00213527\n",
      "Iteration 76, loss = 0.00220686\n",
      "Iteration 77, loss = 0.00201398\n",
      "Iteration 78, loss = 0.00251254\n",
      "Iteration 79, loss = 0.00185725\n",
      "Iteration 80, loss = 0.00186688\n",
      "Iteration 81, loss = 0.00178277\n",
      "Iteration 82, loss = 0.00169879\n",
      "Iteration 83, loss = 0.00164863\n",
      "Iteration 84, loss = 0.00160228\n",
      "Iteration 85, loss = 0.00156736\n",
      "Iteration 86, loss = 0.00154229\n",
      "Iteration 87, loss = 0.00156255\n",
      "Iteration 88, loss = 0.00156099\n",
      "Iteration 89, loss = 0.00149089\n",
      "Iteration 90, loss = 0.00155105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69973022\n",
      "Iteration 2, loss = 0.37281291\n",
      "Iteration 3, loss = 0.22819524\n",
      "Iteration 4, loss = 0.16748693\n",
      "Iteration 5, loss = 0.12535066\n",
      "Iteration 6, loss = 0.17583345\n",
      "Iteration 7, loss = 0.13810767\n",
      "Iteration 8, loss = 0.12354726\n",
      "Iteration 9, loss = 0.10005208\n",
      "Iteration 10, loss = 0.07055221\n",
      "Iteration 11, loss = 0.04227956\n",
      "Iteration 12, loss = 0.02922577\n",
      "Iteration 13, loss = 0.02393062\n",
      "Iteration 14, loss = 0.02584289\n",
      "Iteration 15, loss = 0.02303144\n",
      "Iteration 16, loss = 0.05891994\n",
      "Iteration 17, loss = 0.04573774\n",
      "Iteration 18, loss = 0.02404617\n",
      "Iteration 19, loss = 0.01475479\n",
      "Iteration 20, loss = 0.01311880\n",
      "Iteration 21, loss = 0.01149353\n",
      "Iteration 22, loss = 0.00872167\n",
      "Iteration 23, loss = 0.00839840\n",
      "Iteration 24, loss = 0.00681231\n",
      "Iteration 25, loss = 0.00645056\n",
      "Iteration 26, loss = 0.00590438\n",
      "Iteration 27, loss = 0.00646142\n",
      "Iteration 28, loss = 0.00551294\n",
      "Iteration 29, loss = 0.00466465\n",
      "Iteration 30, loss = 0.00432858\n",
      "Iteration 31, loss = 0.00401722\n",
      "Iteration 32, loss = 0.00378532\n",
      "Iteration 33, loss = 0.00357663\n",
      "Iteration 34, loss = 0.01120153\n",
      "Iteration 35, loss = 0.00834885\n",
      "Iteration 36, loss = 0.00443836\n",
      "Iteration 37, loss = 0.00544932\n",
      "Iteration 38, loss = 0.00362725\n",
      "Iteration 39, loss = 0.00350437\n",
      "Iteration 40, loss = 0.00303172\n",
      "Iteration 41, loss = 0.00272247\n",
      "Iteration 42, loss = 0.00252799\n",
      "Iteration 43, loss = 0.00242785\n",
      "Iteration 44, loss = 0.00269544\n",
      "Iteration 45, loss = 0.00270971\n",
      "Iteration 46, loss = 0.00249096\n",
      "Iteration 47, loss = 0.00294917\n",
      "Iteration 48, loss = 0.00289689\n",
      "Iteration 49, loss = 0.00226759\n",
      "Iteration 50, loss = 0.00207739\n",
      "Iteration 51, loss = 0.00195472\n",
      "Iteration 52, loss = 0.00190679\n",
      "Iteration 53, loss = 0.00182692\n",
      "Iteration 54, loss = 0.00171162\n",
      "Iteration 55, loss = 0.00171015\n",
      "Iteration 56, loss = 0.00181567\n",
      "Iteration 57, loss = 0.00165772\n",
      "Iteration 58, loss = 0.00152979\n",
      "Iteration 59, loss = 0.00145226\n",
      "Iteration 60, loss = 0.00138313\n",
      "Iteration 61, loss = 0.00133453\n",
      "Iteration 62, loss = 0.00130649\n",
      "Iteration 63, loss = 0.00127823\n",
      "Iteration 64, loss = 0.00124498\n",
      "Iteration 65, loss = 0.00121032\n",
      "Iteration 66, loss = 0.00117964\n",
      "Iteration 67, loss = 0.00116289\n",
      "Iteration 68, loss = 0.00114867\n",
      "Iteration 69, loss = 0.00111201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66944665\n",
      "Iteration 2, loss = 0.38534718\n",
      "Iteration 3, loss = 0.26453024\n",
      "Iteration 4, loss = 0.17232245\n",
      "Iteration 5, loss = 0.20261203\n",
      "Iteration 6, loss = 0.36573887\n",
      "Iteration 7, loss = 0.26071222\n",
      "Iteration 8, loss = 0.18785889\n",
      "Iteration 9, loss = 0.18693167\n",
      "Iteration 10, loss = 0.14658311\n",
      "Iteration 11, loss = 0.10324703\n",
      "Iteration 12, loss = 0.09780010\n",
      "Iteration 13, loss = 0.07704072\n",
      "Iteration 14, loss = 0.05709296\n",
      "Iteration 15, loss = 0.04252345\n",
      "Iteration 16, loss = 0.03573767\n",
      "Iteration 17, loss = 0.03002542\n",
      "Iteration 18, loss = 0.02573221\n",
      "Iteration 19, loss = 0.02655388\n",
      "Iteration 20, loss = 0.02278328\n",
      "Iteration 21, loss = 0.01958112\n",
      "Iteration 22, loss = 0.01757505\n",
      "Iteration 23, loss = 0.01914728\n",
      "Iteration 24, loss = 0.01743790\n",
      "Iteration 25, loss = 0.01466584\n",
      "Iteration 26, loss = 0.01290007\n",
      "Iteration 27, loss = 0.01395862\n",
      "Iteration 28, loss = 0.01323711\n",
      "Iteration 29, loss = 0.01220930\n",
      "Iteration 30, loss = 0.01058209\n",
      "Iteration 31, loss = 0.01246990\n",
      "Iteration 32, loss = 0.01379582\n",
      "Iteration 33, loss = 0.01362932\n",
      "Iteration 34, loss = 0.01033939\n",
      "Iteration 35, loss = 0.00809510\n",
      "Iteration 36, loss = 0.00705209\n",
      "Iteration 37, loss = 0.00647260\n",
      "Iteration 38, loss = 0.00595993\n",
      "Iteration 39, loss = 0.00576606\n",
      "Iteration 40, loss = 0.00552441\n",
      "Iteration 41, loss = 0.00518434\n",
      "Iteration 42, loss = 0.00488318\n",
      "Iteration 43, loss = 0.00561937\n",
      "Iteration 44, loss = 0.00515030\n",
      "Iteration 45, loss = 0.00439741\n",
      "Iteration 46, loss = 0.00504356\n",
      "Iteration 47, loss = 0.00456635\n",
      "Iteration 48, loss = 0.00422898\n",
      "Iteration 49, loss = 0.00381356\n",
      "Iteration 50, loss = 0.00359590\n",
      "Iteration 51, loss = 0.00348357\n",
      "Iteration 52, loss = 0.00331883\n",
      "Iteration 53, loss = 0.00315307\n",
      "Iteration 54, loss = 0.00302114\n",
      "Iteration 55, loss = 0.00289018\n",
      "Iteration 56, loss = 0.00288001\n",
      "Iteration 57, loss = 0.00283932\n",
      "Iteration 58, loss = 0.00269981\n",
      "Iteration 59, loss = 0.00258188\n",
      "Iteration 60, loss = 0.00247839\n",
      "Iteration 61, loss = 0.00242543\n",
      "Iteration 62, loss = 0.00238558\n",
      "Iteration 63, loss = 0.00229400\n",
      "Iteration 64, loss = 0.00236081\n",
      "Iteration 65, loss = 0.00241216\n",
      "Iteration 66, loss = 0.00218731\n",
      "Iteration 67, loss = 0.00210298\n",
      "Iteration 68, loss = 0.00201965\n",
      "Iteration 69, loss = 0.00195677\n",
      "Iteration 70, loss = 0.00191410\n",
      "Iteration 71, loss = 0.00186521\n",
      "Iteration 72, loss = 0.00185453\n",
      "Iteration 73, loss = 0.00182423\n",
      "Iteration 74, loss = 0.00174751\n",
      "Iteration 75, loss = 0.00170118\n",
      "Iteration 76, loss = 0.00165951\n",
      "Iteration 77, loss = 0.00161966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68892633\n",
      "Iteration 2, loss = 0.36106618\n",
      "Iteration 3, loss = 0.24897578\n",
      "Iteration 4, loss = 0.16174817\n",
      "Iteration 5, loss = 0.15728227\n",
      "Iteration 6, loss = 0.11713177\n",
      "Iteration 7, loss = 0.10927769\n",
      "Iteration 8, loss = 0.09641247\n",
      "Iteration 9, loss = 0.06413984\n",
      "Iteration 10, loss = 0.05697452\n",
      "Iteration 11, loss = 0.03877087\n",
      "Iteration 12, loss = 0.02713843\n",
      "Iteration 13, loss = 0.02347321\n",
      "Iteration 14, loss = 0.02106329\n",
      "Iteration 15, loss = 0.01701735\n",
      "Iteration 16, loss = 0.01854834\n",
      "Iteration 17, loss = 0.07425919\n",
      "Iteration 18, loss = 0.05748562\n",
      "Iteration 19, loss = 0.02229700\n",
      "Iteration 20, loss = 0.01970492\n",
      "Iteration 21, loss = 0.01432211\n",
      "Iteration 22, loss = 0.01122873\n",
      "Iteration 23, loss = 0.00816631\n",
      "Iteration 24, loss = 0.00622906\n",
      "Iteration 25, loss = 0.00543924\n",
      "Iteration 26, loss = 0.00499203\n",
      "Iteration 27, loss = 0.00527246\n",
      "Iteration 28, loss = 0.00477802\n",
      "Iteration 29, loss = 0.00459481\n",
      "Iteration 30, loss = 0.00388843\n",
      "Iteration 31, loss = 0.00352550\n",
      "Iteration 32, loss = 0.00368770\n",
      "Iteration 33, loss = 0.00326516\n",
      "Iteration 34, loss = 0.00319938\n",
      "Iteration 35, loss = 0.00325109\n",
      "Iteration 36, loss = 0.00740464\n",
      "Iteration 37, loss = 0.00512273\n",
      "Iteration 38, loss = 0.00362824\n",
      "Iteration 39, loss = 0.00275414\n",
      "Iteration 40, loss = 0.00261375\n",
      "Iteration 41, loss = 0.00240311\n",
      "Iteration 42, loss = 0.00219355\n",
      "Iteration 43, loss = 0.00211289\n",
      "Iteration 44, loss = 0.00198846\n",
      "Iteration 45, loss = 0.00190031\n",
      "Iteration 46, loss = 0.00180020\n",
      "Iteration 47, loss = 0.00192332\n",
      "Iteration 48, loss = 0.00198778\n",
      "Iteration 49, loss = 0.00169478\n",
      "Iteration 50, loss = 0.00163564\n",
      "Iteration 51, loss = 0.00158469\n",
      "Iteration 52, loss = 0.00150673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 0.00145576\n",
      "Iteration 54, loss = 0.00144371\n",
      "Iteration 55, loss = 0.00139399\n",
      "Iteration 56, loss = 0.00134373\n",
      "Iteration 57, loss = 0.00130052\n",
      "Iteration 58, loss = 0.00125804\n",
      "Iteration 59, loss = 0.00122077\n",
      "Iteration 60, loss = 0.00121178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73063834\n",
      "Iteration 2, loss = 0.39514126\n",
      "Iteration 3, loss = 0.28186424\n",
      "Iteration 4, loss = 0.22653758\n",
      "Iteration 5, loss = 0.16834701\n",
      "Iteration 6, loss = 0.18275288\n",
      "Iteration 7, loss = 0.14691324\n",
      "Iteration 8, loss = 0.12763235\n",
      "Iteration 9, loss = 0.10835671\n",
      "Iteration 10, loss = 0.08845603\n",
      "Iteration 11, loss = 0.06009020\n",
      "Iteration 12, loss = 0.04364210\n",
      "Iteration 13, loss = 0.03327953\n",
      "Iteration 14, loss = 0.02671119\n",
      "Iteration 15, loss = 0.02305254\n",
      "Iteration 16, loss = 0.02061284\n",
      "Iteration 17, loss = 0.01788743\n",
      "Iteration 18, loss = 0.01689848\n",
      "Iteration 19, loss = 0.01466075\n",
      "Iteration 20, loss = 0.01300497\n",
      "Iteration 21, loss = 0.01189869\n",
      "Iteration 22, loss = 0.01059671\n",
      "Iteration 23, loss = 0.01152019\n",
      "Iteration 24, loss = 0.01082981\n",
      "Iteration 25, loss = 0.00903266\n",
      "Iteration 26, loss = 0.00880632\n",
      "Iteration 27, loss = 0.00792768\n",
      "Iteration 28, loss = 0.00692268\n",
      "Iteration 29, loss = 0.00650659\n",
      "Iteration 30, loss = 0.01034700\n",
      "Iteration 31, loss = 0.00721413\n",
      "Iteration 32, loss = 0.00568911\n",
      "Iteration 33, loss = 0.00517349\n",
      "Iteration 34, loss = 0.00457360\n",
      "Iteration 35, loss = 0.00439144\n",
      "Iteration 36, loss = 0.00433370\n",
      "Iteration 37, loss = 0.00418498\n",
      "Iteration 38, loss = 0.00387362\n",
      "Iteration 39, loss = 0.00359902\n",
      "Iteration 40, loss = 0.00339481\n",
      "Iteration 41, loss = 0.00339366\n",
      "Iteration 42, loss = 0.00328173\n",
      "Iteration 43, loss = 0.00300962\n",
      "Iteration 44, loss = 0.00283019\n",
      "Iteration 45, loss = 0.00268143\n",
      "Iteration 46, loss = 0.00257651\n",
      "Iteration 47, loss = 0.00247629\n",
      "Iteration 48, loss = 0.00237298\n",
      "Iteration 49, loss = 0.00242353\n",
      "Iteration 50, loss = 0.00240233\n",
      "Iteration 51, loss = 0.00220456\n",
      "Iteration 52, loss = 0.00210338\n",
      "Iteration 53, loss = 0.00203997\n",
      "Iteration 54, loss = 0.00207598\n",
      "Iteration 55, loss = 0.00211337\n",
      "Iteration 56, loss = 0.00204451\n",
      "Iteration 57, loss = 0.00197784\n",
      "Iteration 58, loss = 0.00180464\n",
      "Iteration 59, loss = 0.00171615\n",
      "Iteration 60, loss = 0.00166022\n",
      "Iteration 61, loss = 0.00160934\n",
      "Iteration 62, loss = 0.00157156\n",
      "Iteration 63, loss = 0.00151077\n",
      "Iteration 64, loss = 0.00148618\n",
      "Iteration 65, loss = 0.00146499\n",
      "Iteration 66, loss = 0.00142336\n",
      "Iteration 67, loss = 0.00138475\n",
      "Iteration 68, loss = 0.00134939\n",
      "Iteration 69, loss = 0.00130284\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65820064\n",
      "Iteration 2, loss = 0.36502580\n",
      "Iteration 3, loss = 0.25618622\n",
      "Iteration 4, loss = 0.19798470\n",
      "Iteration 5, loss = 0.18874722\n",
      "Iteration 6, loss = 0.13825965\n",
      "Iteration 7, loss = 0.08691987\n",
      "Iteration 8, loss = 0.05818959\n",
      "Iteration 9, loss = 0.04489080\n",
      "Iteration 10, loss = 0.03927585\n",
      "Iteration 11, loss = 0.03132627\n",
      "Iteration 12, loss = 0.02468748\n",
      "Iteration 13, loss = 0.02706092\n",
      "Iteration 14, loss = 0.05571138\n",
      "Iteration 15, loss = 0.08786904\n",
      "Iteration 16, loss = 0.07083964\n",
      "Iteration 17, loss = 0.05761346\n",
      "Iteration 18, loss = 0.03622784\n",
      "Iteration 19, loss = 0.02101804\n",
      "Iteration 20, loss = 0.01230000\n",
      "Iteration 21, loss = 0.01578156\n",
      "Iteration 22, loss = 0.00952948\n",
      "Iteration 23, loss = 0.00835779\n",
      "Iteration 24, loss = 0.00676739\n",
      "Iteration 25, loss = 0.00599554\n",
      "Iteration 26, loss = 0.00598231\n",
      "Iteration 27, loss = 0.00622473\n",
      "Iteration 28, loss = 0.00501357\n",
      "Iteration 29, loss = 0.00516490\n",
      "Iteration 30, loss = 0.00491637\n",
      "Iteration 31, loss = 0.00405636\n",
      "Iteration 32, loss = 0.00370869\n",
      "Iteration 33, loss = 0.00345151\n",
      "Iteration 34, loss = 0.00329879\n",
      "Iteration 35, loss = 0.00316164\n",
      "Iteration 36, loss = 0.00298748\n",
      "Iteration 37, loss = 0.00294617\n",
      "Iteration 38, loss = 0.00284818\n",
      "Iteration 39, loss = 0.00269986\n",
      "Iteration 40, loss = 0.00291137\n",
      "Iteration 41, loss = 0.00270696\n",
      "Iteration 42, loss = 0.00266126\n",
      "Iteration 43, loss = 0.00237177\n",
      "Iteration 44, loss = 0.00223926\n",
      "Iteration 45, loss = 0.00212190\n",
      "Iteration 46, loss = 0.00201174\n",
      "Iteration 47, loss = 0.00194377\n",
      "Iteration 48, loss = 0.00190392\n",
      "Iteration 49, loss = 0.00188871\n",
      "Iteration 50, loss = 0.00178314\n",
      "Iteration 51, loss = 0.00170541\n",
      "Iteration 52, loss = 0.00164416\n",
      "Iteration 53, loss = 0.00158858\n",
      "Iteration 54, loss = 0.00154234\n",
      "Iteration 55, loss = 0.00150728\n",
      "Iteration 56, loss = 0.00147311\n",
      "Iteration 57, loss = 0.00145197\n",
      "Iteration 58, loss = 0.00145594\n",
      "Iteration 59, loss = 0.00145717\n",
      "Iteration 60, loss = 0.00138790\n",
      "Iteration 61, loss = 0.00134044\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69521067\n",
      "Iteration 2, loss = 0.37795850\n",
      "Iteration 3, loss = 0.24745034\n",
      "Iteration 4, loss = 0.15224108\n",
      "Iteration 5, loss = 0.10866743\n",
      "Iteration 6, loss = 0.07763091\n",
      "Iteration 7, loss = 0.06299626\n",
      "Iteration 8, loss = 0.05133978\n",
      "Iteration 9, loss = 0.03404476\n",
      "Iteration 10, loss = 0.03091117\n",
      "Iteration 11, loss = 0.03120376\n",
      "Iteration 12, loss = 0.02686225\n",
      "Iteration 13, loss = 0.02012538\n",
      "Iteration 14, loss = 0.01317135\n",
      "Iteration 15, loss = 0.01034833\n",
      "Iteration 16, loss = 0.01001908\n",
      "Iteration 17, loss = 0.00838647\n",
      "Iteration 18, loss = 0.00711474\n",
      "Iteration 19, loss = 0.00612895\n",
      "Iteration 20, loss = 0.00543755\n",
      "Iteration 21, loss = 0.00501199\n",
      "Iteration 22, loss = 0.00498754\n",
      "Iteration 23, loss = 0.00463236\n",
      "Iteration 24, loss = 0.00467224\n",
      "Iteration 25, loss = 0.00467653\n",
      "Iteration 26, loss = 0.00379166\n",
      "Iteration 27, loss = 0.00348767\n",
      "Iteration 28, loss = 0.00326620\n",
      "Iteration 29, loss = 0.00309578\n",
      "Iteration 30, loss = 0.00296233\n",
      "Iteration 31, loss = 0.00294648\n",
      "Iteration 32, loss = 0.00255467\n",
      "Iteration 33, loss = 0.00237713\n",
      "Iteration 34, loss = 0.00242933\n",
      "Iteration 35, loss = 0.00223729\n",
      "Iteration 36, loss = 0.00211043\n",
      "Iteration 37, loss = 0.00194215\n",
      "Iteration 38, loss = 0.00180834\n",
      "Iteration 39, loss = 0.00173539\n",
      "Iteration 40, loss = 0.00166578\n",
      "Iteration 41, loss = 0.00159716\n",
      "Iteration 42, loss = 0.00244774\n",
      "Iteration 43, loss = 0.00202166\n",
      "Iteration 44, loss = 0.00217967\n",
      "Iteration 45, loss = 0.00166469\n",
      "Iteration 46, loss = 0.00148699\n",
      "Iteration 47, loss = 0.00139571\n",
      "Iteration 48, loss = 0.00134514\n",
      "Iteration 49, loss = 0.00127792\n",
      "Iteration 50, loss = 0.00121217\n",
      "Iteration 51, loss = 0.00117972\n",
      "Iteration 52, loss = 0.00113945\n",
      "Iteration 53, loss = 0.00109832\n",
      "Iteration 54, loss = 0.00127854\n",
      "Iteration 55, loss = 0.00118394\n",
      "Iteration 56, loss = 0.01321344\n",
      "Iteration 57, loss = 0.05181810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71349384\n",
      "Iteration 2, loss = 0.40168989\n",
      "Iteration 3, loss = 0.32601454\n",
      "Iteration 4, loss = 0.33466857\n",
      "Iteration 5, loss = 0.29344372\n",
      "Iteration 6, loss = 0.20750999\n",
      "Iteration 7, loss = 0.15517566\n",
      "Iteration 8, loss = 0.14780176\n",
      "Iteration 9, loss = 0.11375801\n",
      "Iteration 10, loss = 0.08685683\n",
      "Iteration 11, loss = 0.07027312\n",
      "Iteration 12, loss = 0.05995396\n",
      "Iteration 13, loss = 0.06118344\n",
      "Iteration 14, loss = 0.04924778\n",
      "Iteration 15, loss = 0.05423467\n",
      "Iteration 16, loss = 0.04438570\n",
      "Iteration 17, loss = 0.04434155\n",
      "Iteration 18, loss = 0.03972983\n",
      "Iteration 19, loss = 0.03102226\n",
      "Iteration 20, loss = 0.06315290\n",
      "Iteration 21, loss = 0.13364249\n",
      "Iteration 22, loss = 0.40253911\n",
      "Iteration 23, loss = inf\n",
      "Iteration 24, loss = 0.44794291\n",
      "Iteration 25, loss = 0.32917805\n",
      "Iteration 26, loss = 0.16817335\n",
      "Iteration 27, loss = 0.07483205\n",
      "Iteration 28, loss = 0.05902037\n",
      "Iteration 29, loss = 0.03976066\n",
      "Iteration 30, loss = 0.03049884\n",
      "Iteration 31, loss = 0.03042814\n",
      "Iteration 32, loss = 0.02719099\n",
      "Iteration 33, loss = 0.02119512\n",
      "Iteration 34, loss = 0.01905000\n",
      "Iteration 35, loss = 0.01659603\n",
      "Iteration 36, loss = 0.01499172\n",
      "Iteration 37, loss = 0.01709452\n",
      "Iteration 38, loss = 0.01258172\n",
      "Iteration 39, loss = 0.01148962\n",
      "Iteration 40, loss = 0.01020069\n",
      "Iteration 41, loss = 0.00980867\n",
      "Iteration 42, loss = 0.01017224\n",
      "Iteration 43, loss = 0.00995897\n",
      "Iteration 44, loss = 0.00881372\n",
      "Iteration 45, loss = 0.01294785\n",
      "Iteration 46, loss = 0.01327947\n",
      "Iteration 47, loss = 0.00944834\n",
      "Iteration 48, loss = 0.00782593\n",
      "Iteration 49, loss = 0.00688703\n",
      "Iteration 50, loss = 0.00659592\n",
      "Iteration 51, loss = 0.00630187\n",
      "Iteration 52, loss = 0.00589654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 0.00600235\n",
      "Iteration 54, loss = 0.00599021\n",
      "Iteration 55, loss = 0.00547526\n",
      "Iteration 56, loss = 0.00517031\n",
      "Iteration 57, loss = 0.00489582\n",
      "Iteration 58, loss = 0.00475875\n",
      "Iteration 59, loss = 0.00472210\n",
      "Iteration 60, loss = 0.00456251\n",
      "Iteration 61, loss = 0.00438866\n",
      "Iteration 62, loss = 0.00425829\n",
      "Iteration 63, loss = 0.00424565\n",
      "Iteration 64, loss = 0.00427914\n",
      "Iteration 65, loss = 0.00403903\n",
      "Iteration 66, loss = 0.00380603\n",
      "Iteration 67, loss = 0.00361380\n",
      "Iteration 68, loss = 0.00355617\n",
      "Iteration 69, loss = 0.00351490\n",
      "Iteration 70, loss = 0.00337370\n",
      "Iteration 71, loss = 0.00324718\n",
      "Iteration 72, loss = 0.00316479\n",
      "Iteration 73, loss = 0.00309272\n",
      "Iteration 74, loss = 0.00305521\n",
      "Iteration 75, loss = 0.00334229\n",
      "Iteration 76, loss = 0.00324776\n",
      "Iteration 77, loss = 0.00299350\n",
      "Iteration 78, loss = 0.00286522\n",
      "Iteration 79, loss = 0.00276166\n",
      "Iteration 80, loss = 0.00269187\n",
      "Iteration 81, loss = 0.00265420\n",
      "Iteration 82, loss = 0.00261205\n",
      "Iteration 83, loss = 0.00251353\n",
      "Iteration 84, loss = 0.00244794\n",
      "Iteration 85, loss = 0.00242631\n",
      "Iteration 86, loss = 0.00244554\n",
      "Iteration 87, loss = 0.00236624\n",
      "Iteration 88, loss = 0.00230077\n",
      "Iteration 89, loss = 0.00227755\n",
      "Iteration 90, loss = 0.00225850\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68748074\n",
      "Iteration 2, loss = 0.38131196\n",
      "Iteration 3, loss = 0.23315556\n",
      "Iteration 4, loss = 0.15641240\n",
      "Iteration 5, loss = 0.10893523\n",
      "Iteration 6, loss = 0.07923181\n",
      "Iteration 7, loss = 0.05923907\n",
      "Iteration 8, loss = 0.04541687\n",
      "Iteration 9, loss = 0.03588192\n",
      "Iteration 10, loss = 0.02726439\n",
      "Iteration 11, loss = 0.02195435\n",
      "Iteration 12, loss = 0.01687021\n",
      "Iteration 13, loss = 0.01395770\n",
      "Iteration 14, loss = 0.01179239\n",
      "Iteration 15, loss = 0.00996990\n",
      "Iteration 16, loss = 0.00866491\n",
      "Iteration 17, loss = 0.00762420\n",
      "Iteration 18, loss = 0.00687584\n",
      "Iteration 19, loss = 0.00613563\n",
      "Iteration 20, loss = 0.00557203\n",
      "Iteration 21, loss = 0.00509412\n",
      "Iteration 22, loss = 0.00461055\n",
      "Iteration 23, loss = 0.00424899\n",
      "Iteration 24, loss = 0.00398348\n",
      "Iteration 25, loss = 0.00366383\n",
      "Iteration 26, loss = 0.00341077\n",
      "Iteration 27, loss = 0.00317191\n",
      "Iteration 28, loss = 0.00295774\n",
      "Iteration 29, loss = 0.00277947\n",
      "Iteration 30, loss = 0.00263870\n",
      "Iteration 31, loss = 0.00249217\n",
      "Iteration 32, loss = 0.00235189\n",
      "Iteration 33, loss = 0.00223095\n",
      "Iteration 34, loss = 0.00213867\n",
      "Iteration 35, loss = 0.00204774\n",
      "Iteration 36, loss = 0.00192806\n",
      "Iteration 37, loss = 0.00185048\n",
      "Iteration 38, loss = 0.00176610\n",
      "Iteration 39, loss = 0.00168805\n",
      "Iteration 40, loss = 0.00162576\n",
      "Iteration 41, loss = 0.00155658\n",
      "Iteration 42, loss = 0.00150205\n",
      "Iteration 43, loss = 0.00144529\n",
      "Iteration 44, loss = 0.00139449\n",
      "Iteration 45, loss = 0.00135115\n",
      "Iteration 46, loss = 0.00130342\n",
      "Iteration 47, loss = 0.00126679\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Data Min Max\n",
      "0.9110\n",
      "Data Standard\n",
      "0.9110\n",
      "Data Invert Min Max\n",
      "0.9110\n",
      "Data Invert Standard\n",
      "0.9110\n",
      "CPU times: user 40min 44s, sys: 15min 42s, total: 56min 26s\n",
      "Wall time: 17min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Logistic Regrerssion\n",
    "param_space_nn = {\n",
    "#     'lr__C' : [10**i for i in range(-5, 2)]\n",
    "}\n",
    "\n",
    "nns = []\n",
    "for poss in sklearn_space:\n",
    "    num_features = poss[0].shape[1]\n",
    "    nn_min_max = MLPClassifier(hidden_layer_sizes=(num_features,), verbose=True, max_iter=int(1e3))\n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()), ('nn', nn_min_max)])\n",
    "    \n",
    "    grid_search_nn = GridSearchCV(estimator=pipe, param_grid=param_space_nn, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    nn_standard = MLPClassifier(hidden_layer_sizes=(num_features,), verbose=True, max_iter=int(1e3))\n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()), ('nn', nn_standard)])\n",
    "    \n",
    "    grid_search_nn_standard = GridSearchCV(estimator=pipe_standard, param_grid=param_space_nn, cv=StratifiedKFold(n_splits=10))\n",
    "    \n",
    "    nns.append((poss[0], poss[1], grid_search_nn, poss[2] + ' Min Max'))\n",
    "    nns.append((poss[0], poss[1], grid_search_nn_standard, poss[2] + ' Standard'))\n",
    "    \n",
    "for X, y, nn, _ in nns:\n",
    "    nn.fit(X, y)\n",
    "    \n",
    "for X, y, nn, label in nns:\n",
    "    print (label)\n",
    "    print ('{:.4f}'.format(logreg.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "[[329   0]\n",
      " [  0 783]]\n",
      "Data Standard\n",
      "[[329   0]\n",
      " [  0 783]]\n",
      "Data Invert Min Max\n",
      "[[1112    0]\n",
      " [   0 1112]]\n",
      "Data Invert Standard\n",
      "[[1112    0]\n",
      " [   0 1112]]\n"
     ]
    }
   ],
   "source": [
    "for X, y, nn, label in nns:\n",
    "    print (label)\n",
    "    y_pred = nn.predict(X)\n",
    "    print (confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Scores - Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "0.8778\n",
      "Data Standard\n",
      "0.8765\n",
      "Data Invert Min Max\n",
      "0.8886\n",
      "Data Invert Standard\n",
      "0.8877\n",
      "CPU times: user 6.48 s, sys: 4.98 s, total: 11.5 s\n",
      "Wall time: 23min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Random Forest\n",
    "param_space_score_random_forest = {\n",
    "#     'multi__estimator__n_estimators' : [100, 1000],\n",
    "#     'multi__estimator__criterion' : ['mse', 'mae'],\n",
    "#     'multi__estimator__bootstrap' : [True, False]\n",
    "}\n",
    "\n",
    "random_score_forests = []\n",
    "for poss in sklearn_score_space:\n",
    "    random_forest_min_max = RandomForestRegressor(n_jobs=-1)\n",
    "    multi_output_min_max = MultiOutputRegressor(random_forest_min_max, n_jobs=-1)\n",
    "    \n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()),\n",
    "                           ('multi', multi_output_min_max)])\n",
    "    \n",
    "    grid_search_forest = GridSearchCV(estimator=pipe, \n",
    "                                      param_grid=param_space_score_random_forest, \n",
    "                                      cv=10)\n",
    "    \n",
    "    random_forest_standard = RandomForestRegressor(n_jobs=-1)\n",
    "    multi_output_standard = MultiOutputRegressor(random_forest_standard, n_jobs=-1)\n",
    "    \n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()),\n",
    "                                    ('multi', multi_output_standard)])\n",
    "    \n",
    "    grid_search_forest_standard = GridSearchCV(estimator=pipe_standard, \n",
    "                                               param_grid=param_space_score_random_forest, \n",
    "                                               cv=10)\n",
    "    \n",
    "    random_score_forests.append((poss[0], poss[1], grid_search_forest, poss[2] + ' Min Max'))\n",
    "    random_score_forests.append((poss[0], poss[1], grid_search_forest_standard, poss[2] + ' Standard'))\n",
    "\n",
    "for X, y, forest, _ in random_score_forests:\n",
    "    forest.fit(X, y)\n",
    "    \n",
    "for X, y, forest, label in random_score_forests:\n",
    "    print (label)\n",
    "    # this returns R^2 (not accuracy)\n",
    "    print ('{:.4f}'.format(forest.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "{}\n",
      "Data Standard\n",
      "{}\n",
      "Data Invert Min Max\n",
      "{}\n",
      "Data Invert Standard\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "for _, _, forest, label in random_score_forests:\n",
    "    print (label)\n",
    "    print (forest.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "[[63.81 71.84]]\n",
      "[[69.11 71.06]]\n",
      "Data Standard\n",
      "[[62.25 70.08]]\n",
      "[[68.02 71.18]]\n",
      "Data Invert Min Max\n",
      "[[60.24 67.59]]\n",
      "[[67.72 70.53]]\n",
      "Data Invert Standard\n",
      "[[63.59 67.43]]\n",
      "[[66.74 69.13]]\n"
     ]
    }
   ],
   "source": [
    "for _, _, forest, label in random_score_forests:\n",
    "    print (label)\n",
    "    print (forest.predict(X=row))\n",
    "    print (forest.predict(X=row_unc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data None\n",
      "0.8458\n",
      "Data Min Max\n",
      "0.8458\n",
      "Data Standard\n",
      "0.8458\n",
      "Data Invert None\n",
      "0.5756\n",
      "Data Invert Min Max\n",
      "0.5748\n",
      "Data Invert Standard\n",
      "0.5755\n",
      "CPU times: user 3.73 s, sys: 10.5 s, total: 14.2 s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Random Forest\n",
    "param_space_score_linear_regression = {\n",
    "    'multi__estimator__fit_intercept' : [True, False]\n",
    "}\n",
    "\n",
    "score_linear_regressions = []\n",
    "for poss in sklearn_score_space:\n",
    "    linear_none = LinearRegression()\n",
    "    multi_none = MultiOutputRegressor(linear_none, n_jobs=-1)\n",
    "    pipe_none = Pipeline(steps=[('multi', multi_none)])\n",
    "    grid_search_none = GridSearchCV(estimator=pipe_none,\n",
    "                                   param_grid=param_space_score_linear_regression,\n",
    "                                   cv=10)\n",
    "    \n",
    "    linear_min_max = LinearRegression()\n",
    "    multi_output_min_max = MultiOutputRegressor(linear_min_max, n_jobs=-1)\n",
    "    \n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()),\n",
    "                           ('multi', multi_output_min_max)])\n",
    "    \n",
    "    grid_search_linear = GridSearchCV(estimator=pipe, \n",
    "                                      param_grid=param_space_score_linear_regression, \n",
    "                                      cv=10)\n",
    "    \n",
    "    linear_standard = LinearRegression()\n",
    "    multi_output_standard = MultiOutputRegressor(linear_standard, n_jobs=-1)\n",
    "    \n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()),\n",
    "                                    ('multi', multi_output_standard)])\n",
    "    \n",
    "    grid_search_linear_standard = GridSearchCV(estimator=pipe_standard, \n",
    "                                               param_grid=param_space_score_linear_regression, \n",
    "                                               cv=10)\n",
    "    \n",
    "    score_linear_regressions.append((poss[0], poss[1], grid_search_none, poss[2] + ' None'))\n",
    "    score_linear_regressions.append((poss[0], poss[1], grid_search_linear, poss[2] + ' Min Max'))\n",
    "    score_linear_regressions.append((poss[0], poss[1], grid_search_linear_standard, poss[2] + ' Standard'))\n",
    "\n",
    "for X, y, lin, _ in score_linear_regressions:\n",
    "    lin.fit(X, y)\n",
    "    \n",
    "for X, y, lin, label in score_linear_regressions:\n",
    "    print (label)\n",
    "    # this returns R^2 (not accuracy)\n",
    "    print ('{:.4f}'.format(lin.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data None\n",
      "{'multi__estimator__fit_intercept': True}\n",
      "Data Min Max\n",
      "{'multi__estimator__fit_intercept': False}\n",
      "Data Standard\n",
      "{'multi__estimator__fit_intercept': True}\n",
      "Data Invert None\n",
      "{'multi__estimator__fit_intercept': True}\n",
      "Data Invert Min Max\n",
      "{'multi__estimator__fit_intercept': False}\n",
      "Data Invert Standard\n",
      "{'multi__estimator__fit_intercept': True}\n"
     ]
    }
   ],
   "source": [
    "for _, _, lin, label in score_linear_regressions:\n",
    "    print (label)\n",
    "    print (lin.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data None\n",
      "[[58.44418803 71.07457488]]\n",
      "[[73.13338337 67.15193635]]\n",
      "Data Min Max\n",
      "[[58.44418803 71.07457488]]\n",
      "[[73.13338337 67.15193635]]\n",
      "Data Standard\n",
      "[[58.44418803 71.07457488]]\n",
      "[[73.13338337 67.15193635]]\n",
      "Data Invert None\n",
      "[[57.13170851 44.14608095]]\n",
      "[[74.74826886 74.63820606]]\n",
      "Data Invert Min Max\n",
      "[[57.3678851  44.38225753]]\n",
      "[[74.73944021 74.6293774 ]]\n",
      "Data Invert Standard\n",
      "[[57.13319458 44.1465042 ]]\n",
      "[[74.7504211  74.66476279]]\n"
     ]
    }
   ],
   "source": [
    "for _, _, lin, label in score_linear_regressions:\n",
    "    print (label)\n",
    "    print (lin.predict(X=row))\n",
    "    print (lin.predict(X=row_unc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge, Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "0.2617\n",
      "Data Standard\n",
      "0.3156\n",
      "Data Invert Min Max\n",
      "0.3015\n",
      "Data Invert Standard\n",
      "0.3210\n",
      "CPU times: user 32.2 s, sys: 24.8 s, total: 57 s\n",
      "Wall time: 16min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Random Forest\n",
    "param_space_score_lasso = {\n",
    "    'multi__estimator__alpha' : [10**i for i in range(-5, 2) if i != 0],\n",
    "    'multi__estimator__positive' : [True, False],\n",
    "    'multi__estimator__fit_intercept' : [True, False]\n",
    "}\n",
    "\n",
    "score_lassos = []\n",
    "for poss in sklearn_score_space:\n",
    "    lasso_min_max = Lasso()\n",
    "    multi_output_min_max = MultiOutputRegressor(lasso_min_max, n_jobs=-1)\n",
    "    \n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()),\n",
    "                           ('multi', multi_output_min_max)])\n",
    "    \n",
    "    grid_search_lasso = GridSearchCV(estimator=pipe, \n",
    "                                      param_grid=param_space_score_lasso, \n",
    "                                      cv=10)\n",
    "    \n",
    "    lasso_standard = Lasso()\n",
    "    multi_output_standard = MultiOutputRegressor(lasso_standard, n_jobs=-1)\n",
    "    \n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()),\n",
    "                                    ('multi', multi_output_standard)])\n",
    "    \n",
    "    grid_search_lasso_standard = GridSearchCV(estimator=pipe_standard, \n",
    "                                               param_grid=param_space_score_lasso, \n",
    "                                               cv=10)\n",
    "    \n",
    "    score_lassos.append((poss[0], poss[1], grid_search_lasso, poss[2] + ' Min Max'))\n",
    "    score_lassos.append((poss[0], poss[1], grid_search_lasso_standard, poss[2] + ' Standard'))\n",
    "\n",
    "for X, y, lin, _ in score_lassos:\n",
    "    lin.fit(X, y)\n",
    "    \n",
    "for X, y, lin, label in score_lassos:\n",
    "    print (label)\n",
    "    # this returns R^2 (not accuracy)\n",
    "    print ('{:.4f}'.format(lin.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "{'multi__estimator__alpha': 0.1, 'multi__estimator__fit_intercept': True, 'multi__estimator__positive': True}\n",
      "Data Standard\n",
      "{'multi__estimator__alpha': 0.1, 'multi__estimator__fit_intercept': True, 'multi__estimator__positive': True}\n",
      "Data Invert Min Max\n",
      "{'multi__estimator__alpha': 0.1, 'multi__estimator__fit_intercept': False, 'multi__estimator__positive': True}\n",
      "Data Invert Standard\n",
      "{'multi__estimator__alpha': 0.1, 'multi__estimator__fit_intercept': True, 'multi__estimator__positive': True}\n"
     ]
    }
   ],
   "source": [
    "for _, _, lin, label in score_lassos:\n",
    "    print (label)\n",
    "    print (lin.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "0.3259\n",
      "Data Standard\n",
      "0.4517\n",
      "Data Invert Min Max\n",
      "0.3417\n",
      "Data Invert Standard\n",
      "0.4275\n",
      "CPU times: user 21.2 s, sys: 12.4 s, total: 33.6 s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Random Forest\n",
    "param_space_score_ridge = {\n",
    "    'multi__estimator__alpha' : [10**i for i in range(-5, 5) if i != 0],\n",
    "    'multi__estimator__fit_intercept' : [True, False]\n",
    "}\n",
    "\n",
    "score_ridges = []\n",
    "for poss in sklearn_score_space:\n",
    "    ridge_min_max = Ridge()\n",
    "    multi_output_min_max = MultiOutputRegressor(ridge_min_max, n_jobs=-1)\n",
    "    \n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()),\n",
    "                           ('multi', multi_output_min_max)])\n",
    "    \n",
    "    grid_search_ridge = GridSearchCV(estimator=pipe, \n",
    "                                      param_grid=param_space_score_ridge, \n",
    "                                      cv=10)\n",
    "    \n",
    "    ridge_standard = Ridge()\n",
    "    multi_output_standard = MultiOutputRegressor(ridge_standard, n_jobs=-1)\n",
    "    \n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()),\n",
    "                                    ('multi', multi_output_standard)])\n",
    "    \n",
    "    grid_search_ridge_standard = GridSearchCV(estimator=pipe_standard, \n",
    "                                               param_grid=param_space_score_ridge, \n",
    "                                               cv=10)\n",
    "    \n",
    "    score_ridges.append((poss[0], poss[1], grid_search_ridge, poss[2] + ' Min Max'))\n",
    "    score_ridges.append((poss[0], poss[1], grid_search_ridge_standard, poss[2] + ' Standard'))\n",
    "\n",
    "for X, y, lin, _ in score_ridges:\n",
    "    lin.fit(X, y)\n",
    "    \n",
    "for X, y, lin, label in score_ridges:\n",
    "    print (label)\n",
    "    # this returns R^2 (not accuracy)\n",
    "    print ('{:.4f}'.format(lin.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "{'multi__estimator__alpha': 100, 'multi__estimator__fit_intercept': True}\n",
      "Data Standard\n",
      "{'multi__estimator__alpha': 1000, 'multi__estimator__fit_intercept': True}\n",
      "Data Invert Min Max\n",
      "{'multi__estimator__alpha': 100, 'multi__estimator__fit_intercept': True}\n",
      "Data Invert Standard\n",
      "{'multi__estimator__alpha': 1000, 'multi__estimator__fit_intercept': True}\n"
     ]
    }
   ],
   "source": [
    "for _, _, lin, label in score_ridges:\n",
    "    print (label)\n",
    "    print (lin.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "0.2820\n",
      "Data Standard\n",
      "0.2200\n",
      "Data Invert Min Max\n",
      "0.2961\n",
      "Data Invert Standard\n",
      "0.4257\n",
      "CPU times: user 1min, sys: 42.4 s, total: 1min 43s\n",
      "Wall time: 33min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Random Forest\n",
    "param_space_score_elastic = {\n",
    "    'multi__estimator__alpha' : [10**i for i in range(-5, 5) if i != 0],\n",
    "    'multi__estimator__l1_ratio' : np.linspace(0, 1, num=5)\n",
    "}\n",
    "\n",
    "score_elastic_nets = []\n",
    "for poss in sklearn_score_space:\n",
    "    elastic_min_max = ElasticNet()\n",
    "    multi_output_min_max = MultiOutputRegressor(elastic_min_max, n_jobs=-1)\n",
    "    \n",
    "    pipe = Pipeline(steps=[('scale', MinMaxScaler()),\n",
    "                           ('multi', multi_output_min_max)])\n",
    "    \n",
    "    grid_search_elastic = GridSearchCV(estimator=pipe, \n",
    "                                      param_grid=param_space_score_elastic, \n",
    "                                      cv=10)\n",
    "    \n",
    "    elastic_standard = ElasticNet()\n",
    "    multi_output_standard = MultiOutputRegressor(elastic_standard, n_jobs=-1)\n",
    "    \n",
    "    pipe_standard = Pipeline(steps=[('scale', StandardScaler()),\n",
    "                                    ('multi', multi_output_standard)])\n",
    "    \n",
    "    grid_search_elastic_standard = GridSearchCV(estimator=pipe_standard, \n",
    "                                               param_grid=param_space_score_elastic, \n",
    "                                               cv=10)\n",
    "    \n",
    "    score_elastic_nets.append((poss[0], poss[1], grid_search_elastic, poss[2] + ' Min Max'))\n",
    "    score_elastic_nets.append((poss[0], poss[1], grid_search_elastic_standard, poss[2] + ' Standard'))\n",
    "\n",
    "for X, y, lin, _ in score_elastic_nets:\n",
    "    lin.fit(X, y)\n",
    "    \n",
    "for X, y, lin, label in score_elastic_nets:\n",
    "    print (label)\n",
    "    # this returns R^2 (not accuracy)\n",
    "    print ('{:.4f}'.format(lin.score(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Min Max\n",
      "{'multi__estimator__alpha': 0.1}\n",
      "Data Standard\n",
      "{'multi__estimator__alpha': 10}\n",
      "Data Invert Min Max\n",
      "{'multi__estimator__alpha': 0.1}\n",
      "Data Invert Standard\n",
      "{'multi__estimator__alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "for _, _, lin, label in score_elastic_nets:\n",
    "    print (label)\n",
    "    print (lin.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0, 1, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
